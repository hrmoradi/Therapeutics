

@transform_pandas(
    Output(rid="ri.vector.main.execute.12071d5b-30f5-4987-815f-cac669fb33d6"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def CCI_INDEX(derived_cobination):

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    import seaborn as sns
    
    df = derived_cobination
 
    fig = plt.figure(figsize=(15,15)) 
    ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
    ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
    ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)
    # fig, axs = plt.subplots(ncols=3)
    
    Col =  'CCI_INDEX'
    bin_size =20

    # title='Histogram Of Test Scores',
    # rot=45,
    # grid=True,

    # df.CCI_INDEX.value_counts.plot.bar(  ax=ax0) #column=Col,age
    sns.countplot(data=df,x =Col,ax=ax0).set(title=Col+' Distribution')
    
    df_cp =  df[ (df['os_day28']== 1) ]
    # df_cp.CCI_INDEX.value_counts.plot.bar(  ax=ax1)
    sns.countplot(data=df_cp,x =Col,ax=ax1).set(title=Col+' Survived') 

    df_cp =  df[ (df['os_day28']== 11) ]
    # df_cp.CCI_INDEX.value_counts.plot.bar(  ax=ax2)
    sns.countplot(data=df_cp,x =Col,ax=ax2).set(title=Col+' Death') 

    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")

    # plt.xlabel('Test Score')
    # plt.ylabel("Number Of Students");

    plt.xticks(rotation = 90)
    plt.tight_layout()
    plt.show()

    return(pd.DataFrame() )

@transform_pandas(
    Output(rid="ri.vector.main.execute.f829276a-b78f-414d-b8c2-8f1d5a8c365e"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def Comb(derived_cobination):

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    import seaborn as sns

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = derived_cobination

    ###
    days =  ['day'+str(x) for x in range(1,29,1) ]
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    print('comb_func(row)')
    new_Col =['comb_'+day for day in days ]

    treat_unique = np.array([ x for x in np.unique(df[new_Col].values) if x is not ''])

    print('unique():', len(treat_unique),treat_unique)

    treat_np = np.array([   x for x in    df[new_Col].astype(str).values.flatten()   if x is not ''  ])
    print("treat_np",len(treat_np),type(treat_np))

    treat_df = pd.DataFrame(data=treat_np,columns=["treat_np"]) # {'treat_np': ,index=list(range(len(treat_np)))  ,columns=["treat_np"]
    print(treat_df.head(3))
 
    fig = plt.figure(figsize=(15,5)) 

    sns.countplot(data=treat_df,x ='treat_np',order = treat_df['treat_np'].value_counts().index)

    plt.xticks(rotation = 90)
    plt.tight_layout()
    plt.show()

    return(df)  

@transform_pandas(
    Output(rid="ri.vector.main.execute.f1035357-06e7-4215-b207-14abe4893859"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def KEEP_WARM(derived_cobination):
    import time
    while True:
        time.sleep(60*60*60)

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.39546dee-6dc3-41a3-9a17-6f6d8a71a765"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def NN_autoEncoder(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    # import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    model_name = 'ae'
    # model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        # activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,#activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name ae")
    def create_model_ae(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        num_neurons = neuron_comb
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        # activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        
        l = 'En'
        fully = Dense(num_neurons, activation="relu", name="Fully-con-" + str(l))(inputA)
        if batchNorm:fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)
        fully = Dropout(drop_rate, name="Drop-" + str(l))(fully)

        for lay in range(2,layers_comb+1,1):
            l = 'En-'+str(lay)
            fully = Dense(num_neurons//lay, activation="relu", name="Fully-con-" + str(l))(fully)
            if batchNorm:fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)
            fully = Dropout(drop_rate, name="Drop-" + str(l))(fully)

        l = 'mid'
        encode = Dense(mid_dim, activation="relu", name="Fully-con-" + str(l))(fully)
        # fully = Dropout(drop_rate_ae,name="Drop-"+str(l))(fully)
        # for l in range(2,layers_comb+1,1):
        fully = encode

        for lay in range(layers_comb,1, -1):
            l = 'De-' + str(lay)
            fully = Dense(num_neurons // lay, activation="relu", name="Fully-con-" + str(l))(fully)
            if batchNorm:fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)
            fully = Dropout(drop_rate, name="Drop-" + str(l))(fully)

        l = 'De'
        decode = Dense(num_neurons, activation="relu", name="Fully-con-" + str(l))(fully)
        if batchNorm:fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)
        decode = Dropout(drop_rate, name="Drop-" + str(l))(fully)

        l = 'out'
        decode = Dense( demo_shape, activation="sigmoid", name="Fully-con-" + str(l))(decode)
        # decode = Dropout(drop_rate_ae,name="Drop-"+str(l))(fully)

        ae_model = Model(inputs=inputA, outputs=decode)
        encoder = Model(inputs=inputA, outputs=encode)
         

        # loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        # metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        # if model_type == 'binary':
        #     loss = 'binary_crossentropy'
        # if model_type == 'regression':
        loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
        # metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        metric = tf.keras.metrics.MeanSquaredError(name="mse_metric", dtype=None)

        ae_model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        # es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        # mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True) 
        es = EarlyStopping(monitor='val_mse_metric', mode='min', verbose=verbose, patience=patience, min_delta=1)
        mc = ModelCheckpoint('best_model.h5', monitor='val_mse_metric', mode='min', verbose=verbose, save_best_only=True) 

        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == "False": # 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = ae_model.fit(x=x_learn[pts_col].values,  
                                y=x_learn[pts_col].values,
                                validation_data=(x_val[pts_col].values,  x_val[pts_col].values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            ae_model = load_model('best_model.h5')

        pred_prob = ae_model.predict( x_val[pts_col].values)

        encoded_train = encoder.predict(x_learn[pts_col].values)  # .transpose(0,2, 1)
        encoded_val = encoder.predict(x_val[pts_col].values)
        # encoded_test = encoder.predict(glob_x_test)

        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        acc = sk.metrics.mean_squared_error(glob_x_test, pred_ae)
        print("final mse",acc)
      
        ### for ploting send out
        # print('history.history.keys()',history.history.keys())
        # train_acc=[]
        # val_acc=[]
        # if model_type == 'classification' or model_type == 'binary':
        #     train_acc = history.history['auc'] 
        #     val_acc = history.history['val_auc']
        # elif model_type == 'regression':
        #     train_acc = history.history['mape_metric']
        #     val_acc = history.history['val_mape_metric']
        # else:
        #     print("Error model type")
        #     return(0)
        
        train_acc = history.history['mse_metric']
        al_acc = history.history['val_mse_metric']
        train_loss = history.history['mse_loss']
        val_loss = history.history['val_mse_loss']
        epochs = range(len(train_acc))
    
        # if model_type == 'regression' :
        #     y_val = y_val_binary
        #     actual, pred = np.array(y_val), np.array(predictions)
        #     mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
        #     mape_acc = 100 - mape_error 
        #     results = { 'loss': (-mape_acc), 'status': STATUS_OK,
        #                 'accuracy_score_val': mape_acc, 
        #                 'train_acc':train_acc, 'val_acc':val_acc, 
        #                 'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
        #                 'y_val_binary':y_val_binary,'y_val':y_val,
        #                 'pred_prob':pred_prob, 'predictions':predictions,
        #                 'model': model
        #             }
        #     print("TEST__ accuracy_score:mape_acc ", mape_acc)
        #     print("datetime.now()",datetime.now())
        #     return (results)

        # if model_type == 'binary' or model_type =='classification':
        # accuracy_score_val =round(accuracy_score(y_val,predictions),2)
        # f1_score_val = f1_score(y_val, predictions, average=score_average)
        # precision_score_val = precision_score(y_val,predictions, average=score_average)
        # recall_score_val = recall_score(y_val, predictions, average=score_average)
        # Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
        # roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
        # print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
        # confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
        # if model_type == 'binary':
        #     confusion_= confusion_matrix(y_val_binary.values,predictions)
        # for class_ in range(len(confusion_)):
        #     class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
        #     class_str ="class_"+str(class_)+"_accuracy"
        #     print('TEST__ confusion:',class_str,class_accuracy)
 

        results = { 'loss': (acc), 'status': STATUS_OK,
                    # 'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                    # 'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                    # 'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                    'val_acc':val_acc, 'val_loss':val_loss,
                    'train_acc':train_acc, 'train_loss':train_loss,
                    'epochs':epochs,
                    'y_val_binary':y_val_binary,'y_val':y_val,
                    'pred_prob':pred_prob, #'predictions':predictions,
                    'model': model, 'encoded_val':encoded_val, 'y_val':y_val.values
                    }
        print("datetime.now()",datetime.now())
        return (results)
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'ae':
            tpe_best = fmin(fn=create_model_ae, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'ae':
        final_results = create_model_ae(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
        
        return()

        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.ff6f3d92-70dd-499e-96f2-d95d12d0a892")
)
def NN_barplot( NN_new_9_Shap_1_11, NN_new_7_Shap_1_11, NN_new_5_Shap_1_11, NN_new_3_Shap_1_11):
    NN_Before_Max_Shap_1_11 = NN_Before_after_Max_Shap_1_11
    NN_new_9_gradient = NN_new_9_Shap_1_11
    NN_new_7_gradient = NN_new_7_Shap_1_11
    NN_new_5_gradient = NN_new_5_Shap_1_11
    NN_new_3_gradient = NN_new_3_Shap_1_11

    import pandas as pd
    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns
    
    imp_rf = pd.DataFrame()

    new_pd = NN_new_3_gradient.set_index(pd.Index(["os_3"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    print(imp_rf.index)
    print('new_pd: ',new_pd.sum().sum())
    # print(imp_rf.head())
    new_pd = NN_new_5_gradient.set_index(pd.Index(["os_5"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    print(imp_rf.index)
    print('new_pd: ',new_pd.sum().sum())
    new_pd = NN_new_7_gradient.set_index(pd.Index(["os_7"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    print(imp_rf.index)
    print('new_pd: ',new_pd.sum().sum())
    new_pd = NN_new_9_gradient.set_index(pd.Index(["os_9"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    print('new_pd: ',new_pd.sum().sum())
    # imp_rf=imp_rf.fillna(0)
    print(imp_rf.index)

   
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    new_Col = [treatment+str(level)  for level in [3,5,7,9]for treatment in treatments  ]

    most_imp = new_Col
    imp_rf = imp_rf[most_imp]
    print("imp_rf[most_imp]: \n",imp_rf[most_imp])
    sort = sorted(list(imp_rf.columns))
    print("imp_rf.columns ",sort) 
    imp_rf = imp_rf[sort] 
    print(imp_rf.head(2))
    imp_rf = imp_rf.T
    print(imp_rf.head(6))
    # imp_rf = imp_rf[[]]
    fig = plt.figure(figsize=(18,7))
    ## fig, axes = plt.subplots(2, 1,figsize=(10,20)) # axes[0]
    # ax1 = plt.subplot2grid((6,3),(1,0),rowspan = 1,colspan = 3)
    ## ax2 = plt.subplot2grid((6,3),(2,0), rowspan = 2,colspan = 3)
    locs = [plt.subplot2grid((4,3),(x,0), rowspan = 1,colspan = 3) for x in range(4)]

    for idx, loc in zip(imp_rf.columns,locs):
        A_column = imp_rf[[idx]].dropna()
        clr = []
        palette = sns.color_palette("bright", 6)
        for x in A_column.index:

            if 'Bio' in x: clr.append(palette[0])
            if 'Coa' in x: clr.append(palette[1])
            if 'Mis' in x: clr.append(palette[2])
            if 'Mon' in x: clr.append(palette[3])
            if 'Ster' in x: clr.append(palette[4])
            if 'Vir' in x: clr.append(palette[5])
        
        sns.barplot(ax=loc,x=A_column.index,y=idx,   data=A_column,palette=clr) # y=idx, hue="sex",
        # imp_rf.loc[[idx]].T.plot(ax=loc,kind="bar")
            

    locs[0].set_title('Treatment Effect', y=1.05, size=15) # ax.set_title
    
    plt.show()

    imp_rf['index'] = imp_rf.index
    cols = list(imp_rf.columns)
    print(cols)
    cols= cols[-1:] + cols[:-1]
    print(cols)
    imp_rf = imp_rf[cols]
    print(cols)
    # imp_rf.append(pd.DataFrame(imp_rf.columns).T)
    # imp_rf.iloc[5] =  imp_rf.columns
    imp_rf = imp_rf.astype(str)
    imp_rf = imp_rf.columns.to_frame().T.append(imp_rf, ignore_index=True)
    return(imp_rf.T)

    # index = []
    # value = []
    # new_pd = pd.DataFrame(data=value, columns=index).set_index(pd.Index(["os_?"]))
    # imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    # df['index'] = df.index

@transform_pandas(
    Output(rid="ri.vector.main.execute.002f039d-929e-4eff-8929-7c29e8d126d4")
)
def NN_heatmap_merged( NN_new_9_Shap_1_11, NN_new_7_Shap_1_11, NN_new_5_Shap_1_11, NN_new_3_Shap_1_11):
    NN_Before_Max_Shap_1_11 = NN_Before_after_Max_Shap_1_11
    NN_new_9_gradient = NN_new_9_Shap_1_11
    NN_new_7_gradient = NN_new_7_Shap_1_11
    NN_new_5_gradient = NN_new_5_Shap_1_11
    NN_new_3_gradient = NN_new_3_Shap_1_11

    import pandas as pd
    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns
    
    imp_rf = pd.DataFrame()

    new_pd = NN_new_3_gradient.set_index(pd.Index(["os_3"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    print(imp_rf.index)
    # print(imp_rf.head())
    new_pd = NN_new_5_gradient.set_index(pd.Index(["os_5"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    print(imp_rf.index)
    new_pd = NN_new_7_gradient.set_index(pd.Index(["os_7"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    print(imp_rf.index)
    new_pd = NN_new_9_gradient.set_index(pd.Index(["os_9"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True

    imp_rf=imp_rf.fillna(0)
    print(imp_rf.index)

   
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    new_Col = [treatment+str(level)  for level in [3,5,7,9]for treatment in treatments  ]

    most_imp = new_Col
    imp_rf = imp_rf[most_imp]
    print("imp_rf[most_imp]: \n",imp_rf[most_imp])
    
    fig, ax = plt.subplots(figsize=(18, 6)) 

    ################ stak
    # # imp_rf[most_imp].T.plot.bar(stacked=True)
    # ranges = range(len(imp_rf.loc['os_3',most_imp].values))
    # bottom =  [0 for _ in ranges]
    # plt.bar(ranges, imp_rf.loc['os_3',most_imp].values, bottom = bottom)
    # bottom =  np.add(bottom,imp_rf.loc['os_3',most_imp].values)
    # plt.bar(ranges, imp_rf.loc['os_5',most_imp].values, bottom = bottom)
    # bottom =  np.add(bottom,imp_rf.loc['os_5',most_imp].values)
    # plt.bar(ranges, imp_rf.loc['os_7',most_imp].values, bottom = bottom)
    # bottom =  np.add(bottom,imp_rf.loc['os_7',most_imp].values)
    # plt.bar(ranges, imp_rf.loc['os_9',most_imp].values, bottom = bottom)
    # plt.xticks(range(len(most_imp)),most_imp,rotation=45)
    # plt.yticks(np.arange(0, 0.014, 0.002))
    # # plt.yticks(np.arange(0, 0.13, 0.01)) 
    # plt.legend(['Max_os_3','Max_os_5','Max_os_7','Max_os_9'])
    # plt.tight_layout()
    # # ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))
    # off_diag_mask = np.eye(*cf_matrix.shape, dtype=bool)

    ################ heat
    # off_diag_mask = (imp_rf>=0)  
    # vmin = imp_rf.min().min() # [off_diag_mask]
    # vmax = imp_rf.max().max() # [off_diag_mask]
    # print('imp_rf>0',vmin,vmax)
    # sns.heatmap(ax=ax,data=imp_rf , annot=False, mask=off_diag_mask, cmap='Blues', vmin=vmin, vmax=vmax) # ,fmt='2.0f'
    # # off_diag_mask = (imp_rf==0)  
    # # vmin = imp_rf[off_diag_mask].min().min()
    # # vmax = imp_rf[off_diag_mask].max().max()
    # # print('imp_rf==0',vmin,vmax)
    # # sns.heatmap(ax=ax,data=imp_rf , annot=False, mask=off_diag_mask, cmap='Greys', vmin=vmin, vmax=vmax) # ,fmt='2.0f'
    # off_diag_mask = (imp_rf<0)  
    # # vmin = imp_rf[off_diag_mask].min().min()
    # # vmax = imp_rf[off_diag_mask].max().max()
    # print('imp_rf<0',vmin,vmax)
    # sns.heatmap(ax=ax,data=imp_rf , annot=False, mask=off_diag_mask, cmap='OrRd', vmin=vmax, vmax=vmin, cbar_kws=dict(ticks=[])) # OrRd' # ,fmt='2.0f'

    # imp_rf= imp_rf.mul(1000)
    vmin = imp_rf.min().min()
    vmax = imp_rf.max().max()
    mask = (imp_rf!=0)

    cmap = sns.diverging_palette(h_neg=0, h_pos=255, s=100, l=40, sep=1, center='light', as_cmap=True) # 255, 90, 60 # n=10, 
    sns.heatmap(data=imp_rf, center=0.00, annot=False, mask=~mask, # fmt=".2f", 
           linewidths=0, cmap=cmap, vmin=vmin, vmax=vmax, 
           cbar_kws={"shrink": .8}, square=True)

    ax.set_title('Treatment Effect', y=1.05, size=15)
    plt.show()

    imp_rf['index'] = imp_rf.index
    cols = list(imp_rf.columns)
    print(cols)
    cols= cols[-1:] + cols[:-1]
    print(cols)
    imp_rf = imp_rf[cols]
    print(cols)
    # imp_rf.append(pd.DataFrame(imp_rf.columns).T)
    # imp_rf.iloc[5] =  imp_rf.columns
    imp_rf = imp_rf.astype(str)
    imp_rf = imp_rf.columns.to_frame().T.append(imp_rf, ignore_index=True)
    return(imp_rf.T)

    # index = []
    # value = []
    # new_pd = pd.DataFrame(data=value, columns=index).set_index(pd.Index(["os_?"]))
    # imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    # df['index'] = df.index

@transform_pandas(
    Output(rid="ri.vector.main.execute.95c96c72-62c5-4620-baa8-79c804c5d402")
)
def NN_heatmap_separated( NN_new_9_Shap_1_11, NN_new_7_Shap_1_11, NN_new_5_Shap_1_11, NN_new_3_Shap_1_11):
    NN_Before_Max_Shap_1_11 = NN_Before_after_Max_Shap_1_11
    NN_new_9_gradient = NN_new_9_Shap_1_11
    NN_new_7_gradient = NN_new_7_Shap_1_11
    NN_new_5_gradient = NN_new_5_Shap_1_11
    NN_new_3_gradient = NN_new_3_Shap_1_11

    import pandas as pd
    import matplotlib.pyplot as plt
    import numpy as np
    import seaborn as sns
    
    imp_rf = pd.DataFrame()

    new_pd = NN_new_3_gradient.set_index(pd.Index(["os_3"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    print(imp_rf.index)
    # print(imp_rf.head())
    new_pd = NN_new_5_gradient.set_index(pd.Index(["os_5"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    print(imp_rf.index)
    new_pd = NN_new_7_gradient.set_index(pd.Index(["os_7"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    print(imp_rf.index)
    new_pd = NN_new_9_gradient.set_index(pd.Index(["os_9"])).astype(float)
    imp_rf=imp_rf.append(new_pd) # , ignore_index=True

    imp_rf=imp_rf.fillna(0)
    print(imp_rf.index)

   
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    new_Col = [treatment+str(level)  for level in [3,5,7,9]for treatment in treatments  ]

    most_imp = new_Col
    imp_rf = imp_rf[most_imp]
    print("imp_rf[most_imp]: \n",imp_rf[most_imp])
    
    # fig, ax = plt.subplots(figsize=(18, 6)) 

    ################ stak
    # # imp_rf[most_imp].T.plot.bar(stacked=True)
    # ranges = range(len(imp_rf.loc['os_3',most_imp].values))
    # bottom =  [0 for _ in ranges]
    # plt.bar(ranges, imp_rf.loc['os_3',most_imp].values, bottom = bottom)
    # bottom =  np.add(bottom,imp_rf.loc['os_3',most_imp].values)
    # plt.bar(ranges, imp_rf.loc['os_5',most_imp].values, bottom = bottom)
    # bottom =  np.add(bottom,imp_rf.loc['os_5',most_imp].values)
    # plt.bar(ranges, imp_rf.loc['os_7',most_imp].values, bottom = bottom)
    # bottom =  np.add(bottom,imp_rf.loc['os_7',most_imp].values)
    # plt.bar(ranges, imp_rf.loc['os_9',most_imp].values, bottom = bottom)
    # plt.xticks(range(len(most_imp)),most_imp,rotation=45)
    # plt.yticks(np.arange(0, 0.014, 0.002))
    # # plt.yticks(np.arange(0, 0.13, 0.01)) 
    # plt.legend(['Max_os_3','Max_os_5','Max_os_7','Max_os_9'])
    # plt.tight_layout()
    # # ax.yaxis.set_major_formatter(FormatStrFormatter('%.2f'))
    # off_diag_mask = np.eye(*cf_matrix.shape, dtype=bool)

    ################ heat
    # off_diag_mask = (imp_rf>=0)  
    # vmin = imp_rf.min().min() # [off_diag_mask]
    # vmax = imp_rf.max().max() # [off_diag_mask]
    # print('imp_rf>0',vmin,vmax)
    # sns.heatmap(ax=ax,data=imp_rf , annot=False, mask=off_diag_mask, cmap='Blues', vmin=vmin, vmax=vmax) # ,fmt='2.0f'
    # # off_diag_mask = (imp_rf==0)  
    # # vmin = imp_rf[off_diag_mask].min().min()
    # # vmax = imp_rf[off_diag_mask].max().max()
    # # print('imp_rf==0',vmin,vmax)
    # # sns.heatmap(ax=ax,data=imp_rf , annot=False, mask=off_diag_mask, cmap='Greys', vmin=vmin, vmax=vmax) # ,fmt='2.0f'
    # off_diag_mask = (imp_rf<0)  
    # # vmin = imp_rf[off_diag_mask].min().min()
    # # vmax = imp_rf[off_diag_mask].max().max()
    # print('imp_rf<0',vmin,vmax)
    # sns.heatmap(ax=ax,data=imp_rf , annot=False, mask=off_diag_mask, cmap='OrRd', vmin=vmax, vmax=vmin, cbar_kws=dict(ticks=[])) # OrRd' # ,fmt='2.0f'

    # imp_rf= imp_rf.mul(1000)
    vmin = imp_rf.min().min()
    vmax = imp_rf.max().max()
    mask = (imp_rf==0)

    cmap = sns.diverging_palette(h_neg=0, h_pos=255, s=100, l=40, sep=1, center='light', as_cmap=True) # 255, 90, 60 # n=10, 
    
    fig = plt.figure(figsize=(8,7))
    ## fig, axes = plt.subplots(2, 1,figsize=(10,20)) # axes[0]
    # ax1 = plt.subplot2grid((6,3),(1,0),rowspan = 1,colspan = 3)
    ## ax2 = plt.subplot2grid((6,3),(2,0), rowspan = 2,colspan = 3)
    locs = [plt.subplot2grid((4,3),(x,0), rowspan = 1,colspan = 3) for x in range(4)]

    for idx, loc in zip(imp_rf.index,locs):
        # idx = 'os_3'
        vmin = imp_rf.loc[idx].min()
        vmax = imp_rf.loc[idx].max()
        print(vmin,vmax,imp_rf.loc[[idx]],~mask.loc[[idx]])
        sns.heatmap(ax=loc, data=imp_rf.loc[[idx]], center=0.00, annot=False, mask=mask.loc[[idx]], linewidths=0, cmap=cmap, vmin=vmin, vmax=vmax, cbar_kws={"shrink": .8} ,square=True) # fmt=".2f",
            

    locs[0].set_title('Treatment Effect', y=1.05, size=15) # ax.set_title
    
    plt.show()

    imp_rf['index'] = imp_rf.index
    cols = list(imp_rf.columns)
    print(cols)
    cols= cols[-1:] + cols[:-1]
    print(cols)
    imp_rf = imp_rf[cols]
    print(cols)
    # imp_rf.append(pd.DataFrame(imp_rf.columns).T)
    # imp_rf.iloc[5] =  imp_rf.columns
    imp_rf = imp_rf.astype(str)
    imp_rf = imp_rf.columns.to_frame().T.append(imp_rf, ignore_index=True)
    return(imp_rf.T)

    # index = []
    # value = []
    # new_pd = pd.DataFrame(data=value, columns=index).set_index(pd.Index(["os_?"]))
    # imp_rf=imp_rf.append(new_pd) # , ignore_index=True
    # df['index'] = df.index

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.6c0967bf-e860-4d73-a8d4-326b8a75ae5f"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def ROC_CURVE(derived_cobination):

    # https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html

    
    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.b2693687-0755-4d28-819a-fd89d6e7d3a9"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def Test_all_treat_1(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.c468f416-4076-42eb-b219-9791da24043a"),
    half_or_5=Input(rid="ri.foundry.main.dataset.e4ed812c-cbb4-4a9a-b482-5367e0b2bec4")
)
def XG_DrugComb_NoMax_Shap1_11_matching_overal_1_1_1_copied(half_or_5):
    XG_DrugComb_NoMax_Shap1_11_matching_overal_1_1 = XG_DrugComb_NoMax_Shap1_11_matching_overal_1_1_copied
    derived_cobination = half_or_5

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.83171646-2d30-4105-856c-157395f19084"),
    half_or_3=Input(rid="ri.foundry.main.dataset.8d4c9605-5416-47f3-a175-04dac3ccf57d")
)
def XG_DrugComb_NoMax_Shap1_11_matching_overal_1_1_1_copied_copied(half_or_3):
    derived_cobination = half_or_3

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.b58dca87-4c74-4484-ad27-eda05b349e11"),
    half_or_5=Input(rid="ri.foundry.main.dataset.e4ed812c-cbb4-4a9a-b482-5367e0b2bec4")
)
def XG_DrugComb_NoMax_Shap1_11_matching_overal_1_1_copied(half_or_5):
    derived_cobination = half_or_5

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.df9fcbf3-97d2-4e30-9e89-3ce36c1e07b6"),
    half_or_5=Input(rid="ri.foundry.main.dataset.e4ed812c-cbb4-4a9a-b482-5367e0b2bec4")
)
def XG_DrugComb_NoMax_Shap1_11_matching_overal_1_copied(half_or_5):
    derived_cobination = half_or_5

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                'os_day1', 
                # 'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.833b5567-d4f3-40a3-8385-63df25c943e9"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def age(derived_cobination):

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    import seaborn as sns
    
    df = derived_cobination
 
    fig = plt.figure(figsize=(15,15)) 
    ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
    ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
    ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)
    
    Col =  'age'
    bin_size =40

    # title='Histogram Of Test Scores',
    # rot=45,
    # grid=True,

    df.age.plot.hist( bins=bin_size, ax=ax0) #column=Col,age
    # sns.displot(data=df,x =Col,ax=ax0).set(title=Col+' Distribution')
    
    df_cp =  df[ (df['os_day28']== 1) ]
    df_cp.age.plot.hist( bins=bin_size, ax=ax1)
    # sns.displot(data=df_cp,x =Col,ax=ax1).set(title=Col+' Survived') 

    df_cp =  df[ (df['os_day28']== 11) ]
    df_cp.age.plot.hist( bins=bin_size, ax=ax2)
    # sns.barplot(data=df_cp,x =Col,ax=ax2).set(title=Col+' Death') 

    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")

    # plt.xlabel('Test Score')
    # plt.ylabel("Number Of Students");

    plt.xticks(rotation = 90)
    plt.tight_layout()
    plt.show()

    return(pd.DataFrame() )

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.2499b206-c71b-4399-a4ad-03f2199047ba"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def all_treat(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.77480141-1a3a-45d2-beb1-1bb6bcfb51ed"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def all_treat_NN(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    model_name = 'full'
    # model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64"),
    derived_set=Input(rid="ri.foundry.main.dataset.1be6df15-36dd-41af-a16f-e5794835b64b")
)
def derived_cobination( derived_set):

    import pandas as pd
    import numpy as np
    from datetime import datetime

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = derived_set
    # df = df.head(2000)
    
    pts_col = [  'age', 'bmi', 'CCI_INDEX', 'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', # 'date_of_earliest_covid_diagnosis',
     'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
     'gender_female', 'gender_male', 'gender_other', 'race_asian', 'race_black_or_african_american', 'race_native_hawaiian_or_other_pacific_islander', 'race_other', 'race_white',
      'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other','os_day1',
       '2021Q3', '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', '2019Q4','2019Q3', '2019Q2', '2019Q1', '2018Q4', '2018Q3', '2018Q2', '2018Q1']
    # drug_col = ['Ster_day1',     'Coag_day1',     'Vira_day1',      'Biot_day1',      'Misc_day1',     'Mono_day1', ] os_col = [ 'os_day1',]
    # df['los']
    # df['time_to_outcome']      
    # df['max_level'] 
    # df['days_to_max_level']
    # df['days_max_to_outcome']       = df['time_to_outcome'] -  df['days_to_max_level']
    # df['deter_speed']               = (df['max_level']-df['os_day1'])/(df['days_to_max_level'])
    # df['enhanc_speed']              = (df['max_level']-df['os_day28'])/(df['days_max_to_outcome'])
    # for treat in treatments:
        # new_Col =[treat +'before_max', treat+'after_max']

    ###
    days =  ['day'+str(x) for x in range(1,29,1) ]
    os_days = ['os_'+x for x in days ]
    days_reverse =  ['day'+str(x) for x in range(28,0,-1) ]
    os_days_reverse = ['os_'+x for x in days_reverse ]
    pressor_days = [ 'pressor_' +x for x in days ]
    
    ### 'lenght of stay '
    print('pressor_days')
    def pressor_days_func(row):
        days_in = 0
        for day in pressor_days:
            if row[day] == 1:
                days_in +=1
        return(days_in/int(row['time_to_outcome']))
    df['pressor_days'] = df.apply(lambda row : pressor_days_func(row), axis=1)

    ### 'comb_func' #################
    # treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    treatments = ['BiotMQ_','Coag_','ViraUnp_','Ster_','Misc_','MonoSP_','MonoI_','ViraTrgt_']
    
    print('comb_func(row)')
    def comb_func(row):
        output = pd.Series()
        for day in days:
            comb_day = ""
            for treat in treatments:
                if row[treat+day]==1:
                    comb_day+=treat
            output['comb_'+day]  = comb_day
        return(output)
    new_Col =['comb_'+day for day in days ]
    print('new_Col',new_Col)
    df[new_Col] = df.apply(lambda row : comb_func(row), axis=1) 
    
    
    los = 0
    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_' ...
    unique_comb =np.unique(df[new_Col].values)
    def days_on_treat_func(row,uc,los):
        count= 0
        for c_day in new_Col:
            if row[c_day] == uc: 
                count+=1
        if row['los']==0: 
            los+=1
            return(0)
        return(count/row['los']) #

    for u_c in unique_comb:
        print('u_c',u_c)
        df[u_c] = df.apply(lambda row : days_on_treat_func(row,u_c,los), axis=1) 
        print("datetime.now()",datetime.now())

    print('los = 0 ',los)
    return(df)  

    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_'
    #  'Coag_Biot_' 'Coag_Biot_Misc_' 'Coag_Biot_Misc_Mono_' 'Coag_Biot_Mono_'
    #  'Coag_Misc_' 'Coag_Misc_Mono_' 'Coag_Mono_' 'Coag_Vira_'
    #  'Coag_Vira_Biot_' 'Coag_Vira_Biot_Misc_' 'Coag_Vira_Biot_Misc_Mono_'
    #  'Coag_Vira_Biot_Mono_' 'Coag_Vira_Misc_' 'Coag_Vira_Misc_Mono_'
    #  'Coag_Vira_Mono_' 'Misc_' 'Misc_Mono_' 'Mono_' 'Ster_' 'Ster_Biot_'
    #  'Ster_Biot_Misc_' 'Ster_Biot_Misc_Mono_' 'Ster_Biot_Mono_' 'Ster_Coag_'
    #  'Ster_Coag_Biot_' 'Ster_Coag_Biot_Misc_' 'Ster_Coag_Biot_Misc_Mono_'
    #  'Ster_Coag_Biot_Mono_' 'Ster_Coag_Misc_' 'Ster_Coag_Misc_Mono_'
    #  'Ster_Coag_Mono_' 'Ster_Coag_Vira_' 'Ster_Coag_Vira_Biot_'
    #  'Ster_Coag_Vira_Biot_Misc_' 'Ster_Coag_Vira_Biot_Misc_Mono_'
    #  'Ster_Coag_Vira_Biot_Mono_' 'Ster_Coag_Vira_Misc_'
    #  'Ster_Coag_Vira_Misc_Mono_' 'Ster_Coag_Vira_Mono_' 'Ster_Misc_'
    #  'Ster_Misc_Mono_' 'Ster_Mono_' 'Ster_Vira_' 'Ster_Vira_Biot_'
    #  'Ster_Vira_Biot_Misc_' 'Ster_Vira_Biot_Misc_Mono_' 'Ster_Vira_Biot_Mono_'
    #  'Ster_Vira_Misc_' 'Ster_Vira_Misc_Mono_' 'Ster_Vira_Mono_' 'Vira_'
    #  'Vira_Biot_' 'Vira_Biot_Misc_' 'Vira_Biot_Misc_Mono_' 'Vira_Biot_Mono_'
    #  'Vira_Misc_' 'Vira_Misc_Mono_' 'Vira_Mono_']

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.da5426ba-b1d1-4855-a959-61cf28a90f49"),
    derived_set=Input(rid="ri.foundry.main.dataset.1be6df15-36dd-41af-a16f-e5794835b64b")
)
def derived_cobination_daycount( derived_set):

    import pandas as pd
    import numpy as np
    from datetime import datetime

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = derived_set
    # df = df.head(2000)
    
    pts_col = [  'age', 'bmi', 'CCI_INDEX', 'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', # 'date_of_earliest_covid_diagnosis',
     'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
     'gender_female', 'gender_male', 'gender_other', 'race_asian', 'race_black_or_african_american', 'race_native_hawaiian_or_other_pacific_islander', 'race_other', 'race_white',
      'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other','os_day1',
       '2021Q3', '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', '2019Q4','2019Q3', '2019Q2', '2019Q1', '2018Q4', '2018Q3', '2018Q2', '2018Q1']
    # drug_col = ['Ster_day1',     'Coag_day1',     'Vira_day1',      'Biot_day1',      'Misc_day1',     'Mono_day1', ] os_col = [ 'os_day1',]
    # df['los']
    # df['time_to_outcome']      
    # df['max_level'] 
    # df['days_to_max_level']
    # df['days_max_to_outcome']       = df['time_to_outcome'] -  df['days_to_max_level']
    # df['deter_speed']               = (df['max_level']-df['os_day1'])/(df['days_to_max_level'])
    # df['enhanc_speed']              = (df['max_level']-df['os_day28'])/(df['days_max_to_outcome'])
    # for treat in treatments:
        # new_Col =[treat +'before_max', treat+'after_max']

    ###
    days =  ['day'+str(x) for x in range(1,29,1) ]
    os_days = ['os_'+x for x in days ]
    days_reverse =  ['day'+str(x) for x in range(28,0,-1) ]
    os_days_reverse = ['os_'+x for x in days_reverse ]

    ### 'comb_func' #################
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    print('comb_func(row)')
    def comb_func(row):
        output = pd.Series()
        for day in days:
            comb_day = ""
            for treat in treatments:
                if row[treat+day]==1:
                    comb_day+=treat
            output['comb_'+day]  = comb_day
        return(output)
    new_Col =['comb_'+day for day in days ]
    print('new_Col',new_Col)
    df[new_Col] = df.apply(lambda row : comb_func(row), axis=1) 
    
    
    los = 0
    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_' ...
    unique_comb =np.unique(df[new_Col].values)
    def days_on_treat_func(row,uc,los):
        count= 0
        for c_day in new_Col:
            if row[c_day] == uc: 
                count+=1
        if row['los']==0: 
            los+=1
            return(0)
        return(count)#/row['los']) #

    for u_c in unique_comb:
        print('u_c',u_c)
        df[u_c] = df.apply(lambda row : days_on_treat_func(row,u_c,los), axis=1) 
        print("datetime.now()",datetime.now())

    print('los = 0 ',los)
    return(df)  

    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_'
    #  'Coag_Biot_' 'Coag_Biot_Misc_' 'Coag_Biot_Misc_Mono_' 'Coag_Biot_Mono_'
    #  'Coag_Misc_' 'Coag_Misc_Mono_' 'Coag_Mono_' 'Coag_Vira_'
    #  'Coag_Vira_Biot_' 'Coag_Vira_Biot_Misc_' 'Coag_Vira_Biot_Misc_Mono_'
    #  'Coag_Vira_Biot_Mono_' 'Coag_Vira_Misc_' 'Coag_Vira_Misc_Mono_'
    #  'Coag_Vira_Mono_' 'Misc_' 'Misc_Mono_' 'Mono_' 'Ster_' 'Ster_Biot_'
    #  'Ster_Biot_Misc_' 'Ster_Biot_Misc_Mono_' 'Ster_Biot_Mono_' 'Ster_Coag_'
    #  'Ster_Coag_Biot_' 'Ster_Coag_Biot_Misc_' 'Ster_Coag_Biot_Misc_Mono_'
    #  'Ster_Coag_Biot_Mono_' 'Ster_Coag_Misc_' 'Ster_Coag_Misc_Mono_'
    #  'Ster_Coag_Mono_' 'Ster_Coag_Vira_' 'Ster_Coag_Vira_Biot_'
    #  'Ster_Coag_Vira_Biot_Misc_' 'Ster_Coag_Vira_Biot_Misc_Mono_'
    #  'Ster_Coag_Vira_Biot_Mono_' 'Ster_Coag_Vira_Misc_'
    #  'Ster_Coag_Vira_Misc_Mono_' 'Ster_Coag_Vira_Mono_' 'Ster_Misc_'
    #  'Ster_Misc_Mono_' 'Ster_Mono_' 'Ster_Vira_' 'Ster_Vira_Biot_'
    #  'Ster_Vira_Biot_Misc_' 'Ster_Vira_Biot_Misc_Mono_' 'Ster_Vira_Biot_Mono_'
    #  'Ster_Vira_Misc_' 'Ster_Vira_Misc_Mono_' 'Ster_Vira_Mono_' 'Vira_'
    #  'Vira_Biot_' 'Vira_Biot_Misc_' 'Vira_Biot_Misc_Mono_' 'Vira_Biot_Mono_'
    #  'Vira_Misc_' 'Vira_Misc_Mono_' 'Vira_Mono_']

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.1be6df15-36dd-41af-a16f-e5794835b64b"),
    final_dataset=Input(rid="ri.foundry.main.dataset.3edf50ba-bd93-4890-b061-3c28aa71577e")
)
def derived_set(final_dataset):

    import pandas as pd
    import numpy as np
    from datetime import datetime

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = final_dataset
    # df = df.head(2000)
    
    pts_col = [  'age', 'bmi', 'CCI_INDEX', 'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', # 'date_of_earliest_covid_diagnosis',
     'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
     'gender_female', 'gender_male', 'gender_other', 'race_asian', 'race_black_or_african_american', 'race_native_hawaiian_or_other_pacific_islander', 'race_other', 'race_white',
      'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other','os_day1',
       '2021Q3', '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', '2019Q4','2019Q3', '2019Q2', '2019Q1', '2018Q4', '2018Q3', '2018Q2', '2018Q1']
    # drug_col = ['Ster_day1',     'Coag_day1',     'Vira_day1',      'Biot_day1',      'Misc_day1',     'Mono_day1', ] os_col = [ 'os_day1',]

    ###

    days =  ['day'+str(x) for x in range(1,29,1) ]
    os_days = ['os_'+x for x in days ]
    days_reverse =  ['day'+str(x) for x in range(28,0,-1) ]
    os_days_reverse = ['os_'+x for x in days_reverse ]
    
    ### 'lenght of stay '
    print('los_func(row)')
    def los_func(row):
        days_in = 0
        for day in os_days:
            if row[day] in [3,5,7,9]:
                days_in +=1
        return(days_in)
    df['los'] = df.apply(lambda row : los_func(row), axis=1)
    ### 'time_to_outcome'
    print('time_to_outcome_func(row)')
    def time_to_outcome_func(row):
        outcome_os=int(row['os_day28'])
        last_change = np.nan
        for day in os_days_reverse:
            if row[day]!=outcome_os:
                last_change = int(day.split('day')[1])+1
                break
        return(last_change)
    df['time_to_outcome'] = df.apply(lambda row : time_to_outcome_func(row), axis=1)

    ### 'max_level'
    print('max_level_func(row)')
    def max_level_func(row):
        last_change=int(row['time_to_outcome'])-1
        change_days = os_days[:last_change]
        max_level = row[change_days].max()
        return(max_level)
    df['max_level'] = df.apply(lambda row : max_level_func(row), axis=1)

    ### 'days_to_max_level'
    print('days_to_max_level_func(row)')
    def days_to_max_level_func(row):
        last_change=int(row['time_to_outcome'])-1
        change_days = os_days[:last_change]
        max_level = row['max_level']
        for day in change_days:
            if row[day]==max_level:
                max_level_day = int(day.split('day')[1])
                break
        return(max_level_day)
    df['days_to_max_level'] = df.apply(lambda row : days_to_max_level_func(row), axis=1)

    print('days_max_to_outcome, deter_speed, enhanc_speed')
    df['days_max_to_outcome']= df['time_to_outcome'] -  df['days_to_max_level']
    df['deter_speed'] = (df['max_level']-df['os_day1'])/(df['days_to_max_level'])
    df['enhanc_speed'] = (df['max_level']-df['os_day28'])/(df['days_max_to_outcome'])

    ### 'days_on_treat'
    print('\ndays_on_treat_func(row,level)')
    # treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    treatments = ['BiotMQ_','Coag_','ViraUnp_','Ster_','Misc_','MonoSP_','MonoI_','ViraTrgt_','pressor_']
    def days_on_treat_func(row,treat):
        output = pd.Series()
        
        days_to_max_level = days[:row['days_to_max_level']-1]
        days_max_to_outcome = days[row['days_to_max_level']-1:row['time_to_outcome']-1]

        treat_before_max = [treat+d for d in days_to_max_level]
        treat_after_max =  [treat+d for d in days_max_to_outcome]

        if (len(days_to_max_level)) !=0:
            output[treat+str('before_max')] = row[treat_before_max].sum()/len(days_to_max_level)
        else:
            output[treat+str('before_max')]  = 0
        
        if (len(days_max_to_outcome)) !=0:
            output[treat+str('after_max')] = row[treat_after_max].sum()/len(days_max_to_outcome)
        else:
            output[treat+str('after_max')]  = 0

        return(output)

    for treat in treatments:
        print('\ntreatment: ',treat)
        new_Col =[treat +'before_max', treat+'after_max']
        print('new_Col',new_Col)
        df[new_Col] = df.apply(lambda row : days_on_treat_func(row,treat), axis=1) # [new_Col]
        print("datetime.now()",datetime.now())

    return(df)  

    # ### 'days_on_treat_level'
    # print('\ndays_on_treat_level_func(row,level)')
    # treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    # def days_on_treat_level_func(row,level):
    #     output = pd.Series() 
    #     target_os_days = (row[os_days]==level)
    #     target_os_days_sum =  float(sum(target_os_days.astype(int)))
    #     output['days_on_'+str(level)] = target_os_days_sum
    #     target_os_days_string = np.array(os_days)[target_os_days]
    #     target_os_days_string = [ x.split('_')[1] for x in target_os_days_string]
    #     target_os_days_string_p_treat = [x+y for x in treatments for y in target_os_days_string]
    #     all_treatments_sum = float(sum(  (row[target_os_days_string_p_treat]).astype(int)   ))
    #     output['days_on_'+str(level)+'_with_drug'] = all_treatments_sum
    #     for treat in treatments:
    #         if (target_os_days_sum != 0) and (all_treatments_sum != 0):
    #             target_treat_and_os_days = [treat+y for y in target_os_days_string]
    #             on_treat_in_os = row[target_treat_and_os_days] # .multiply(target_os_days)
    #             output[treat+str(level)] = on_treat_in_os.sum() /float(target_os_days_sum) #/float(all_treatments_sum)
    #             # if on_treat_in_os.sum() != 0:
    #             #     print(row[target_treat_days])
    #         else:
    #             output[treat+str(level)] = 0
    #     return(output)
    # for level in [3,5,7,9]: #
    #     print('\nlevel: ',level)
    #     new_Col =['days_on_'+str(level),'days_on_'+str(level)+'_with_treatment']
    #     new_Col = new_Col+ [treatment+str(level) for treatment in treatments]
    #     print('new_Col',new_Col)
    #     df[new_Col] = df.apply(lambda row : days_on_treat_level_func(row,level), axis=1) # [new_Col]
    #     print("datetime.now()",datetime.now())

    # [os_days]

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.9dab361d-59b0-4abb-8e80-3faf5eaddd9f"),
    with_a_drug=Input(rid="ri.foundry.main.dataset.59b93e13-e5e0-47c3-9327-ff96e5bc7ce5")
)
def drugs_final(with_a_drug):
    return(with_a_drug.fillna(0))
    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.3edf50ba-bd93-4890-b061-3c28aa71577e"),
    drugs_final=Input(rid="ri.foundry.main.dataset.9dab361d-59b0-4abb-8e80-3faf5eaddd9f"),
    levels_by_day=Input(rid="ri.foundry.main.dataset.178867ea-a908-4a22-88fc-d3363a4a6dbf"),
    pts_info=Input(rid="ri.foundry.main.dataset.d875ccf4-8b17-498c-b3c7-18c3c1de97e1")
)
def final_dataset(levels_by_day, drugs_final, pts_info):

    import pandas as pd
    
    df = pts_info
    for table in [levels_by_day, drugs_final ]:
        df = pd.merge(df,table,on=['person_id','date_of_earliest_covid_diagnosis'], how='inner')

    return(df)

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.bc736f5c-2c4e-4cfe-b117-57d49c818d7e"),
    fmk_modified=Input(rid="ri.foundry.main.dataset.068cf2f6-24a5-4a71-9f71-ae6f109c15b3")
)
def flattened_drugs( fmk_modified):
    tim_pts_drugs_by_day = fmk_modified
    
    import pandas as pd
    import numpy as np
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)

    tim = tim_pts_drugs_by_day
    df = tim[['person_id','date_of_earliest_covid_diagnosis']].drop_duplicates()
    drug_class = 'drug_group'

    for drug, b_name in zip(['Antibiotics MQ','Anticoagulants','Antivirals Unproven','Steroid Preparations',                                    
                            'Miscellaneous','Monoclonal Antibodies SP','Monoclonal Antibodies I','Antivirals Targeted'],
                            ['BiotMQ','Coag','ViraUnp','Ster','Misc','MonoSP','MonoI','ViraTrgt']):
        print(drug)  # for each drug
        filtered = tim[tim[drug_class].isin([drug])] # get only the drug of intrest
        f3 = [ x for x in filtered.columns[:3] ] # day 1 to 28 column names
        Lx = [(b_name.replace(' ','_') +'_'+ x) for x in filtered.columns[3:] ] # drugX_day1, ...
        newNames= np.concatenate((f3, Lx))
        filtered = filtered.rename(columns=dict(zip(filtered.columns,newNames)))
        filtered = filtered.drop(columns=[drug_class])
        df = pd.merge(df,filtered,on=['person_id','date_of_earliest_covid_diagnosis'], how='left')
        # print(df[df.notna().all(axis=1)].head(2) )
    return(df)

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.068cf2f6-24a5-4a71-9f71-ae6f109c15b3"),
    FMK_daily_meds_May=Input(rid="ri.foundry.main.dataset.20d1d65b-0404-46ab-979d-566cf90eb53f")
)
def fmk_modified( FMK_daily_meds_May):
    import pandas as pd
    
    print(FMK_daily_meds_May.drug_group.unique())
    
    dic_col =  {}
    for col in FMK_daily_meds_May.columns:
        print(col)
        if "med_day" in col:
            modified_col = col.replace("med_","")
            dic_col[col]=modified_col

    FMK_daily_meds_May.rename(dic_col, axis=1, inplace=True)
    FMK_daily_meds_May.drop(columns=['seq2','V27','V28'],inplace=True)
    print(FMK_daily_meds_May.columns)
    
    return(FMK_daily_meds_May)

#  'Antibiotics MQ' 'Anticoagulants' 'Antivirals Unproven'
#  'Steroid Preparations' 'Miscellaneous' 'Monoclonal Antibodies SP'
#  'Monoclonal Antibodies I' 'Antivirals Targeted'

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.8d4c9605-5416-47f3-a175-04dac3ccf57d"),
    derived_set=Input(rid="ri.foundry.main.dataset.1be6df15-36dd-41af-a16f-e5794835b64b")
)
def half_or_3( derived_set):

    import pandas as pd
    import numpy as np
    from datetime import datetime

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = derived_set
    # df = df.head(2000)
    
    pts_col = [  'age', 'bmi', 'CCI_INDEX', 'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', # 'date_of_earliest_covid_diagnosis',
     'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
     'gender_female', 'gender_male', 'gender_other', 'race_asian', 'race_black_or_african_american', 'race_native_hawaiian_or_other_pacific_islander', 'race_other', 'race_white',
      'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other','os_day1',
       '2021Q3', '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', '2019Q4','2019Q3', '2019Q2', '2019Q1', '2018Q4', '2018Q3', '2018Q2', '2018Q1']
    # drug_col = ['Ster_day1',     'Coag_day1',     'Vira_day1',      'Biot_day1',      'Misc_day1',     'Mono_day1', ] os_col = [ 'os_day1',]
    # df['los']
    # df['time_to_outcome']      
    # df['max_level'] 
    # df['days_to_max_level']
    # df['days_max_to_outcome']       = df['time_to_outcome'] -  df['days_to_max_level']
    # df['deter_speed']               = (df['max_level']-df['os_day1'])/(df['days_to_max_level'])
    # df['enhanc_speed']              = (df['max_level']-df['os_day28'])/(df['days_max_to_outcome'])
    # for treat in treatments:
        # new_Col =[treat +'before_max', treat+'after_max']

    ###
    days =  ['day'+str(x) for x in range(1,29,1) ]
    os_days = ['os_'+x for x in days ]
    days_reverse =  ['day'+str(x) for x in range(28,0,-1) ]
    os_days_reverse = ['os_'+x for x in days_reverse ]

    ### 'comb_func' #################
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    print('comb_func(row)')
    def comb_func(row):
        output = pd.Series()
        for day in days:
            comb_day = ""
            for treat in treatments:
                if row[treat+day]==1:
                    comb_day+=treat
            output['comb_'+day]  = comb_day
        return(output)
    new_Col =['comb_'+day for day in days ]
    print('new_Col',new_Col)
    df[new_Col] = df.apply(lambda row : comb_func(row), axis=1) 
    
    
    los = 0
    los_half= 0 
    min_stay = 3
    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_' ...
    unique_comb =np.unique(df[new_Col].values)
    def days_on_treat_func(row,uc,los,los_half):
        count= 0
        half_in_hosp = row['los']//2
        if row['los'] <= min_stay: 
            half_in_hosp=row['los']
        elif half_in_hosp <= min_stay: 
            half_in_hosp=min_stay
        for c_day in new_Col[:half_in_hosp]:
            if row[c_day] == uc: 
                count+=1
        if row['los']==0: 
            los+=1
            return(0)
        if half_in_hosp==0:
            los_half+=1
            return(0)
        return(count/half_in_hosp) #

    for u_c in unique_comb:
        print('u_c',u_c)
        df[u_c] = df.apply(lambda row : days_on_treat_func(row,u_c,los,los_half), axis=1) 
        print("datetime.now()",datetime.now())

    print('los = 0 ',los, ' los_half=0',los_half)
    return(df)  

    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_'
    #  'Coag_Biot_' 'Coag_Biot_Misc_' 'Coag_Biot_Misc_Mono_' 'Coag_Biot_Mono_'
    #  'Coag_Misc_' 'Coag_Misc_Mono_' 'Coag_Mono_' 'Coag_Vira_'
    #  'Coag_Vira_Biot_' 'Coag_Vira_Biot_Misc_' 'Coag_Vira_Biot_Misc_Mono_'
    #  'Coag_Vira_Biot_Mono_' 'Coag_Vira_Misc_' 'Coag_Vira_Misc_Mono_'
    #  'Coag_Vira_Mono_' 'Misc_' 'Misc_Mono_' 'Mono_' 'Ster_' 'Ster_Biot_'
    #  'Ster_Biot_Misc_' 'Ster_Biot_Misc_Mono_' 'Ster_Biot_Mono_' 'Ster_Coag_'
    #  'Ster_Coag_Biot_' 'Ster_Coag_Biot_Misc_' 'Ster_Coag_Biot_Misc_Mono_'
    #  'Ster_Coag_Biot_Mono_' 'Ster_Coag_Misc_' 'Ster_Coag_Misc_Mono_'
    #  'Ster_Coag_Mono_' 'Ster_Coag_Vira_' 'Ster_Coag_Vira_Biot_'
    #  'Ster_Coag_Vira_Biot_Misc_' 'Ster_Coag_Vira_Biot_Misc_Mono_'
    #  'Ster_Coag_Vira_Biot_Mono_' 'Ster_Coag_Vira_Misc_'
    #  'Ster_Coag_Vira_Misc_Mono_' 'Ster_Coag_Vira_Mono_' 'Ster_Misc_'
    #  'Ster_Misc_Mono_' 'Ster_Mono_' 'Ster_Vira_' 'Ster_Vira_Biot_'
    #  'Ster_Vira_Biot_Misc_' 'Ster_Vira_Biot_Misc_Mono_' 'Ster_Vira_Biot_Mono_'
    #  'Ster_Vira_Misc_' 'Ster_Vira_Misc_Mono_' 'Ster_Vira_Mono_' 'Vira_'
    #  'Vira_Biot_' 'Vira_Biot_Misc_' 'Vira_Biot_Misc_Mono_' 'Vira_Biot_Mono_'
    #  'Vira_Misc_' 'Vira_Misc_Mono_' 'Vira_Mono_']

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.e4ed812c-cbb4-4a9a-b482-5367e0b2bec4"),
    derived_set=Input(rid="ri.foundry.main.dataset.1be6df15-36dd-41af-a16f-e5794835b64b")
)
def half_or_5( derived_set):

    import pandas as pd
    import numpy as np
    from datetime import datetime

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = derived_set
    # df = df.head(2000)
    
    pts_col = [  'age', 'bmi', 'CCI_INDEX', 'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', # 'date_of_earliest_covid_diagnosis',
     'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
     'gender_female', 'gender_male', 'gender_other', 'race_asian', 'race_black_or_african_american', 'race_native_hawaiian_or_other_pacific_islander', 'race_other', 'race_white',
      'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other','os_day1',
       '2021Q3', '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', '2019Q4','2019Q3', '2019Q2', '2019Q1', '2018Q4', '2018Q3', '2018Q2', '2018Q1']
    # drug_col = ['Ster_day1',     'Coag_day1',     'Vira_day1',      'Biot_day1',      'Misc_day1',     'Mono_day1', ] os_col = [ 'os_day1',]
    # df['los']
    # df['time_to_outcome']      
    # df['max_level'] 
    # df['days_to_max_level']
    # df['days_max_to_outcome']       = df['time_to_outcome'] -  df['days_to_max_level']
    # df['deter_speed']               = (df['max_level']-df['os_day1'])/(df['days_to_max_level'])
    # df['enhanc_speed']              = (df['max_level']-df['os_day28'])/(df['days_max_to_outcome'])
    # for treat in treatments:
        # new_Col =[treat +'before_max', treat+'after_max']

    ###
    days =  ['day'+str(x) for x in range(1,29,1) ]
    os_days = ['os_'+x for x in days ]
    days_reverse =  ['day'+str(x) for x in range(28,0,-1) ]
    os_days_reverse = ['os_'+x for x in days_reverse ]

    ### 'comb_func' #################
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    print('comb_func(row)')
    def comb_func(row):
        output = pd.Series()
        for day in days:
            comb_day = ""
            for treat in treatments:
                if row[treat+day]==1:
                    comb_day+=treat
            output['comb_'+day]  = comb_day
        return(output)
    new_Col =['comb_'+day for day in days ]
    print('new_Col',new_Col)
    df[new_Col] = df.apply(lambda row : comb_func(row), axis=1) 
    
    
    los = 0
    los_half= 0 
    min_stay = 5
    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_' ...
    unique_comb =np.unique(df[new_Col].values)
    def days_on_treat_func(row,uc,los,los_half):
        count= 0
        half_in_hosp = row['los']//2
        if row['los'] <= min_stay: 
            half_in_hosp=row['los']
        elif half_in_hosp <= min_stay: 
            half_in_hosp=min_stay
        for c_day in new_Col[:half_in_hosp]:
            if row[c_day] == uc: 
                count+=1
        if row['los']==0: 
            los+=1
            return(0)
        if half_in_hosp==0:
            los_half+=1
            return(0)
        return(count/half_in_hosp) #

    for u_c in unique_comb:
        print('u_c',u_c)
        df[u_c] = df.apply(lambda row : days_on_treat_func(row,u_c,los,los_half), axis=1) 
        print("datetime.now()",datetime.now())

    print('los = 0 ',los, ' los_half=0',los_half)
    return(df)  

    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_'
    #  'Coag_Biot_' 'Coag_Biot_Misc_' 'Coag_Biot_Misc_Mono_' 'Coag_Biot_Mono_'
    #  'Coag_Misc_' 'Coag_Misc_Mono_' 'Coag_Mono_' 'Coag_Vira_'
    #  'Coag_Vira_Biot_' 'Coag_Vira_Biot_Misc_' 'Coag_Vira_Biot_Misc_Mono_'
    #  'Coag_Vira_Biot_Mono_' 'Coag_Vira_Misc_' 'Coag_Vira_Misc_Mono_'
    #  'Coag_Vira_Mono_' 'Misc_' 'Misc_Mono_' 'Mono_' 'Ster_' 'Ster_Biot_'
    #  'Ster_Biot_Misc_' 'Ster_Biot_Misc_Mono_' 'Ster_Biot_Mono_' 'Ster_Coag_'
    #  'Ster_Coag_Biot_' 'Ster_Coag_Biot_Misc_' 'Ster_Coag_Biot_Misc_Mono_'
    #  'Ster_Coag_Biot_Mono_' 'Ster_Coag_Misc_' 'Ster_Coag_Misc_Mono_'
    #  'Ster_Coag_Mono_' 'Ster_Coag_Vira_' 'Ster_Coag_Vira_Biot_'
    #  'Ster_Coag_Vira_Biot_Misc_' 'Ster_Coag_Vira_Biot_Misc_Mono_'
    #  'Ster_Coag_Vira_Biot_Mono_' 'Ster_Coag_Vira_Misc_'
    #  'Ster_Coag_Vira_Misc_Mono_' 'Ster_Coag_Vira_Mono_' 'Ster_Misc_'
    #  'Ster_Misc_Mono_' 'Ster_Mono_' 'Ster_Vira_' 'Ster_Vira_Biot_'
    #  'Ster_Vira_Biot_Misc_' 'Ster_Vira_Biot_Misc_Mono_' 'Ster_Vira_Biot_Mono_'
    #  'Ster_Vira_Misc_' 'Ster_Vira_Misc_Mono_' 'Ster_Vira_Mono_' 'Vira_'
    #  'Vira_Biot_' 'Vira_Biot_Misc_' 'Vira_Biot_Misc_Mono_' 'Vira_Biot_Mono_'
    #  'Vira_Misc_' 'Vira_Misc_Mono_' 'Vira_Mono_']

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.178867ea-a908-4a22-88fc-d3363a4a6dbf"),
    FMK_covidpos_new_OS_May=Input(rid="ri.foundry.main.dataset.eaed807a-4ebd-4c84-a302-b6fc22d373c6")
)
def levels_by_day( FMK_covidpos_new_OS_May):
    import pandas as pd
    
    # tim_level_by_day = tim_level_by_day

    days = [ ('day'+str(x)) for x in range(1,29,1)]
    pressor_ = [ ('pressor_'+x) for x in days]
    print("pressor_",pressor_)

    tim_level_by_day_columns= ['person_id', 'date_of_earliest_covid_diagnosis', 'day1', 'day2', 'day3', 'day4', 'day5', 'day6', 'day7', 'day8', 'day9', 'day10', 'day11', 'day12', 'day13', 'day14', 'day15', 'day16', 'day17', 'day18', 'day19', 'day20', 'day21', 'day22', 'day23', 'day24', 'day25', 'day26', 'day27', 'day28']
    print("tim_level_by_day_columns", tim_level_by_day_columns)

    needed = tim_level_by_day_columns + pressor_
    print("needed", needed)
    df = FMK_covidpos_new_OS_May[needed]

    ######################################
    # seaborn: added to env.
    # Keras: added to env.
    # hyperopt: added to env.
    # ADD ? imbalance-learn
    
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    from matplotlib import pyplot as plt
    # import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    # from imblearn.over_sampling import SMOTE ####
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn import model_selection
    from sklearn.utils import shuffle
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.optimizers import RMSprop, SGD
    # # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from keras.wrappers.scikit_learn import KerasClassifier
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = time.time() 
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################

    # no date_of_earliest_covid_diagnosis,
    
    print(df.head(3))

    for i in range(1,13,2):
        print("Before df.shape",i,df.shape)
        
        # changes = ~(df.iloc[:,days] == i).all(axis='columns')
        changes = ~(df[days] == i).all(axis='columns')
        print(changes.head(3))
    
        df =  df[changes]
        print("After df.shape",i,df.shape)
    
        
    # changes = ~(df.iloc[:,2:5] == 1).all(axis='columns')
    # df =  df[changes]
    # print("After 1st week not hospitilized df.shape",df.shape)

    changes = ~((df.iloc[:,2:5] == 11).any(axis='columns'))
    df =  df[changes]
    print("drop dead in the first 3 days df.shape:" ,df.shape)

    print(df.head(10))
 
    # f3 = [ x for x in df.columns[:2] ]
    # Lx = [('os_'+ x) for x in df.columns[days] ]
    Lx = [('os_'+ x) for x in days ]
    # newNames= np.concatenate((f3, Lx))
    print(dict(zip(days,Lx)))
    df = df.rename(columns=dict(zip(days,Lx)))
     
    return(df)

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.3e6b1f11-91d3-4273-bd75-beb37623710b"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def los(derived_cobination):

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    import seaborn as sns
    
    df = derived_cobination
 
    fig = plt.figure(figsize=(15,15)) 
    ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
    ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
    ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)
    
    Col =  'los'
    bin_size =28

    # title='Histogram Of Test Scores',
    # rot=45,
    # grid=True,

    df.los.plot.hist( bins=bin_size, ax=ax0) #column=Col,age
    # sns.displot(data=df,x =Col,ax=ax0).set(title=Col+' Distribution')
    
    df_cp =  df[ (df['os_day28']== 1) ]
    df_cp.los.plot.hist( bins=bin_size, ax=ax1)
    # sns.displot(data=df_cp,x =Col,ax=ax1).set(title=Col+' Survived') 

    df_cp =  df[ (df['os_day28']== 11) ]
    df_cp.los.plot.hist( bins=bin_size, ax=ax2)
    # sns.barplot(data=df_cp,x =Col,ax=ax2).set(title=Col+' Death') 

    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")

    # plt.xlabel('Test Score')
    # plt.ylabel("Number Of Students");

    plt.xticks(rotation = 90)
    plt.tight_layout()
    plt.show()

    return(pd.DataFrame() )

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.2755eaaa-0eb5-44bf-b785-afb350a4910b"),
    derived_set=Input(rid="ri.foundry.main.dataset.1be6df15-36dd-41af-a16f-e5794835b64b")
)
def los_or_1( derived_set):
  

    import pandas as pd
    import numpy as np
    from datetime import datetime

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = derived_set
    # df = df.head(2000)
    
    pts_col = [  'age', 'bmi', 'CCI_INDEX', 'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', # 'date_of_earliest_covid_diagnosis',
     'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
     'gender_female', 'gender_male', 'gender_other', 'race_asian', 'race_black_or_african_american', 'race_native_hawaiian_or_other_pacific_islander', 'race_other', 'race_white',
      'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other','os_day1',
       '2021Q3', '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', '2019Q4','2019Q3', '2019Q2', '2019Q1', '2018Q4', '2018Q3', '2018Q2', '2018Q1']
    # drug_col = ['Ster_day1',     'Coag_day1',     'Vira_day1',      'Biot_day1',      'Misc_day1',     'Mono_day1', ] os_col = [ 'os_day1',]
    # df['los']
    # df['time_to_outcome']      
    # df['max_level'] 
    # df['days_to_max_level']
    # df['days_max_to_outcome']       = df['time_to_outcome'] -  df['days_to_max_level']
    # df['deter_speed']               = (df['max_level']-df['os_day1'])/(df['days_to_max_level'])
    # df['enhanc_speed']              = (df['max_level']-df['os_day28'])/(df['days_max_to_outcome'])
    # for treat in treatments:
        # new_Col =[treat +'before_max', treat+'after_max']

    ###
    days =  ['day'+str(x) for x in range(1,29,1) ]
    os_days = ['os_'+x for x in days ]
    days_reverse =  ['day'+str(x) for x in range(28,0,-1) ]
    os_days_reverse = ['os_'+x for x in days_reverse ]

    ### 'comb_func' #################
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    print('comb_func(row)')
    def comb_func(row):
        output = pd.Series()
        for day in days:
            comb_day = ""
            for treat in treatments:
                if row[treat+day]==1:
                    comb_day+=treat
            output['comb_'+day]  = comb_day
        return(output)
    new_Col =['comb_'+day for day in days ]
    print('new_Col',new_Col)
    df[new_Col] = df.apply(lambda row : comb_func(row), axis=1) 
    
    
    los = 0
    los_half= 0 
    min_stay = 1
    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_' ...
    unique_comb =np.unique(df[new_Col].values)
    def days_on_treat_func(row,uc,los,los_half):
        count= 0
        half_in_hosp = min_stay
        if row['los'] <= min_stay: 
            half_in_hosp=row['los']
        for c_day in new_Col[:half_in_hosp]:
            if row[c_day] == uc: 
                count+=1
        if row['los']==0: 
            los+=1
            return(0)
        if half_in_hosp==0:
            los_half+=1
            return(0)
        return(count/half_in_hosp) #

    for u_c in unique_comb:
        print('u_c',u_c)
        df[u_c] = df.apply(lambda row : days_on_treat_func(row,u_c,los,los_half), axis=1) 
        print("datetime.now()",datetime.now())

    print('los = 0 ',los, ' los_half=0',los_half)
    return(df)  

    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_'
    #  'Coag_Biot_' 'Coag_Biot_Misc_' 'Coag_Biot_Misc_Mono_' 'Coag_Biot_Mono_'
    #  'Coag_Misc_' 'Coag_Misc_Mono_' 'Coag_Mono_' 'Coag_Vira_'
    #  'Coag_Vira_Biot_' 'Coag_Vira_Biot_Misc_' 'Coag_Vira_Biot_Misc_Mono_'
    #  'Coag_Vira_Biot_Mono_' 'Coag_Vira_Misc_' 'Coag_Vira_Misc_Mono_'
    #  'Coag_Vira_Mono_' 'Misc_' 'Misc_Mono_' 'Mono_' 'Ster_' 'Ster_Biot_'
    #  'Ster_Biot_Misc_' 'Ster_Biot_Misc_Mono_' 'Ster_Biot_Mono_' 'Ster_Coag_'
    #  'Ster_Coag_Biot_' 'Ster_Coag_Biot_Misc_' 'Ster_Coag_Biot_Misc_Mono_'
    #  'Ster_Coag_Biot_Mono_' 'Ster_Coag_Misc_' 'Ster_Coag_Misc_Mono_'
    #  'Ster_Coag_Mono_' 'Ster_Coag_Vira_' 'Ster_Coag_Vira_Biot_'
    #  'Ster_Coag_Vira_Biot_Misc_' 'Ster_Coag_Vira_Biot_Misc_Mono_'
    #  'Ster_Coag_Vira_Biot_Mono_' 'Ster_Coag_Vira_Misc_'
    #  'Ster_Coag_Vira_Misc_Mono_' 'Ster_Coag_Vira_Mono_' 'Ster_Misc_'
    #  'Ster_Misc_Mono_' 'Ster_Mono_' 'Ster_Vira_' 'Ster_Vira_Biot_'
    #  'Ster_Vira_Biot_Misc_' 'Ster_Vira_Biot_Misc_Mono_' 'Ster_Vira_Biot_Mono_'
    #  'Ster_Vira_Misc_' 'Ster_Vira_Misc_Mono_' 'Ster_Vira_Mono_' 'Vira_'
    #  'Vira_Biot_' 'Vira_Biot_Misc_' 'Vira_Biot_Misc_Mono_' 'Vira_Biot_Mono_'
    #  'Vira_Misc_' 'Vira_Misc_Mono_' 'Vira_Mono_']

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.fa76dd6c-fd31-45ef-9300-24ce030f6723"),
    los_or_1=Input(rid="ri.foundry.main.dataset.2755eaaa-0eb5-44bf-b785-afb350a4910b")
)
def los_or_1_1and11_(los_or_1):
    derived_cobination = los_or_1

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.c23f2f07-81f0-4289-b935-fc8e6ea6524b"),
    los_or_1=Input(rid="ri.foundry.main.dataset.2755eaaa-0eb5-44bf-b785-afb350a4910b")
)
def los_or_1_1and11__1(los_or_1):
    derived_cobination = los_or_1

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = False
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.0ed927ca-8161-40ab-9fa5-39b96bd26ffa"),
    los_or_1=Input(rid="ri.foundry.main.dataset.2755eaaa-0eb5-44bf-b785-afb350a4910b"),
    los_or_1_1and11_=Input(rid="ri.foundry.main.dataset.fa76dd6c-fd31-45ef-9300-24ce030f6723")
)
def los_or_1_all(los_or_1, los_or_1_1and11_):
    XG_DrugComb_NoMax_Shap1_11_matching_overal_1_1_1_copied_copied_copied_copied_copied_copied = los_or_1_1and11_
    derived_cobination = los_or_1

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.60e4d7a4-a683-47c6-8092-67af90c77c48"),
    derived_set=Input(rid="ri.foundry.main.dataset.1be6df15-36dd-41af-a16f-e5794835b64b")
)
def los_or_2( derived_set):

    import pandas as pd
    import numpy as np
    from datetime import datetime

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = derived_set
    # df = df.head(2000)
    
    pts_col = [  'age', 'bmi', 'CCI_INDEX', 'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', # 'date_of_earliest_covid_diagnosis',
     'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
     'gender_female', 'gender_male', 'gender_other', 'race_asian', 'race_black_or_african_american', 'race_native_hawaiian_or_other_pacific_islander', 'race_other', 'race_white',
      'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other','os_day1',
       '2021Q3', '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', '2019Q4','2019Q3', '2019Q2', '2019Q1', '2018Q4', '2018Q3', '2018Q2', '2018Q1']
    # drug_col = ['Ster_day1',     'Coag_day1',     'Vira_day1',      'Biot_day1',      'Misc_day1',     'Mono_day1', ] os_col = [ 'os_day1',]
    # df['los']
    # df['time_to_outcome']      
    # df['max_level'] 
    # df['days_to_max_level']
    # df['days_max_to_outcome']       = df['time_to_outcome'] -  df['days_to_max_level']
    # df['deter_speed']               = (df['max_level']-df['os_day1'])/(df['days_to_max_level'])
    # df['enhanc_speed']              = (df['max_level']-df['os_day28'])/(df['days_max_to_outcome'])
    # for treat in treatments:
        # new_Col =[treat +'before_max', treat+'after_max']

    ###
    days =  ['day'+str(x) for x in range(1,29,1) ]
    os_days = ['os_'+x for x in days ]
    days_reverse =  ['day'+str(x) for x in range(28,0,-1) ]
    os_days_reverse = ['os_'+x for x in days_reverse ]

    ### 'comb_func' #################
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    print('comb_func(row)')
    def comb_func(row):
        output = pd.Series()
        for day in days:
            comb_day = ""
            for treat in treatments:
                if row[treat+day]==1:
                    comb_day+=treat
            output['comb_'+day]  = comb_day
        return(output)
    new_Col =['comb_'+day for day in days ]
    print('new_Col',new_Col)
    df[new_Col] = df.apply(lambda row : comb_func(row), axis=1) 
    
    
    los = 0
    los_half= 0 
    min_stay = 2
    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_' ...
    unique_comb =np.unique(df[new_Col].values)
    def days_on_treat_func(row,uc,los,los_half):
        count= 0
        half_in_hosp = min_stay
        if row['los'] <= min_stay: 
            half_in_hosp=row['los']
        for c_day in new_Col[:half_in_hosp]:
            if row[c_day] == uc: 
                count+=1
        if row['los']==0: 
            los+=1
            return(0)
        if half_in_hosp==0:
            los_half+=1
            return(0)
        return(count/half_in_hosp) #

    for u_c in unique_comb:
        print('u_c',u_c)
        df[u_c] = df.apply(lambda row : days_on_treat_func(row,u_c,los,los_half), axis=1) 
        print("datetime.now()",datetime.now())

    print('los = 0 ',los, ' los_half=0',los_half)
    return(df)  

    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_'
    #  'Coag_Biot_' 'Coag_Biot_Misc_' 'Coag_Biot_Misc_Mono_' 'Coag_Biot_Mono_'
    #  'Coag_Misc_' 'Coag_Misc_Mono_' 'Coag_Mono_' 'Coag_Vira_'
    #  'Coag_Vira_Biot_' 'Coag_Vira_Biot_Misc_' 'Coag_Vira_Biot_Misc_Mono_'
    #  'Coag_Vira_Biot_Mono_' 'Coag_Vira_Misc_' 'Coag_Vira_Misc_Mono_'
    #  'Coag_Vira_Mono_' 'Misc_' 'Misc_Mono_' 'Mono_' 'Ster_' 'Ster_Biot_'
    #  'Ster_Biot_Misc_' 'Ster_Biot_Misc_Mono_' 'Ster_Biot_Mono_' 'Ster_Coag_'
    #  'Ster_Coag_Biot_' 'Ster_Coag_Biot_Misc_' 'Ster_Coag_Biot_Misc_Mono_'
    #  'Ster_Coag_Biot_Mono_' 'Ster_Coag_Misc_' 'Ster_Coag_Misc_Mono_'
    #  'Ster_Coag_Mono_' 'Ster_Coag_Vira_' 'Ster_Coag_Vira_Biot_'
    #  'Ster_Coag_Vira_Biot_Misc_' 'Ster_Coag_Vira_Biot_Misc_Mono_'
    #  'Ster_Coag_Vira_Biot_Mono_' 'Ster_Coag_Vira_Misc_'
    #  'Ster_Coag_Vira_Misc_Mono_' 'Ster_Coag_Vira_Mono_' 'Ster_Misc_'
    #  'Ster_Misc_Mono_' 'Ster_Mono_' 'Ster_Vira_' 'Ster_Vira_Biot_'
    #  'Ster_Vira_Biot_Misc_' 'Ster_Vira_Biot_Misc_Mono_' 'Ster_Vira_Biot_Mono_'
    #  'Ster_Vira_Misc_' 'Ster_Vira_Misc_Mono_' 'Ster_Vira_Mono_' 'Vira_'
    #  'Vira_Biot_' 'Vira_Biot_Misc_' 'Vira_Biot_Misc_Mono_' 'Vira_Biot_Mono_'
    #  'Vira_Misc_' 'Vira_Misc_Mono_' 'Vira_Mono_']

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.842b2944-d53e-46f6-9206-0e5b4a5fe27c"),
    los_or_2=Input(rid="ri.foundry.main.dataset.60e4d7a4-a683-47c6-8092-67af90c77c48")
)
def los_or_2_1and11_(los_or_2):
    derived_cobination = los_or_2

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.63b71c30-ef62-42f9-8e23-fb29228131fd"),
    los_or_2=Input(rid="ri.foundry.main.dataset.60e4d7a4-a683-47c6-8092-67af90c77c48")
)
def los_or_2_1and11__1(los_or_2):
    derived_cobination = los_or_2

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = False
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.58777154-7e9b-4ebe-8962-528ba156f0b9"),
    los_or_2=Input(rid="ri.foundry.main.dataset.60e4d7a4-a683-47c6-8092-67af90c77c48"),
    los_or_2_1and11_=Input(rid="ri.foundry.main.dataset.842b2944-d53e-46f6-9206-0e5b4a5fe27c")
)
def los_or_2_all(los_or_2, los_or_2_1and11_):
    XG_DrugComb_NoMax_Shap1_11_matching_overal_1_1_1_copied_copied_copied_copied_copied = los_or_2_1and11_
    derived_cobination = los_or_2

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.c208a92a-e905-4ddd-8536-05ed6b7ffae9"),
    derived_set=Input(rid="ri.foundry.main.dataset.1be6df15-36dd-41af-a16f-e5794835b64b")
)
def los_or_3( derived_set):

    import pandas as pd
    import numpy as np
    from datetime import datetime

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = derived_set
    # df = df.head(2000)
    
    pts_col = [  'age', 'bmi', 'CCI_INDEX', 'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', # 'date_of_earliest_covid_diagnosis',
     'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
     'gender_female', 'gender_male', 'gender_other', 'race_asian', 'race_black_or_african_american', 'race_native_hawaiian_or_other_pacific_islander', 'race_other', 'race_white',
      'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other','os_day1',
       '2021Q3', '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', '2019Q4','2019Q3', '2019Q2', '2019Q1', '2018Q4', '2018Q3', '2018Q2', '2018Q1']
    # drug_col = ['Ster_day1',     'Coag_day1',     'Vira_day1',      'Biot_day1',      'Misc_day1',     'Mono_day1', ] os_col = [ 'os_day1',]
    # df['los']
    # df['time_to_outcome']      
    # df['max_level'] 
    # df['days_to_max_level']
    # df['days_max_to_outcome']       = df['time_to_outcome'] -  df['days_to_max_level']
    # df['deter_speed']               = (df['max_level']-df['os_day1'])/(df['days_to_max_level'])
    # df['enhanc_speed']              = (df['max_level']-df['os_day28'])/(df['days_max_to_outcome'])
    # for treat in treatments:
        # new_Col =[treat +'before_max', treat+'after_max']

    ###
    days =  ['day'+str(x) for x in range(1,29,1) ]
    os_days = ['os_'+x for x in days ]
    days_reverse =  ['day'+str(x) for x in range(28,0,-1) ]
    os_days_reverse = ['os_'+x for x in days_reverse ]

    ### 'comb_func' #################
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    print('comb_func(row)')
    def comb_func(row):
        output = pd.Series()
        for day in days:
            comb_day = ""
            for treat in treatments:
                if row[treat+day]==1:
                    comb_day+=treat
            output['comb_'+day]  = comb_day
        return(output)
    new_Col =['comb_'+day for day in days ]
    print('new_Col',new_Col)
    df[new_Col] = df.apply(lambda row : comb_func(row), axis=1) 
    
    
    los = 0
    los_half= 0 
    min_stay = 3
    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_' ...
    unique_comb =np.unique(df[new_Col].values)
    def days_on_treat_func(row,uc,los,los_half):
        count= 0
        half_in_hosp = min_stay
        if row['los'] <= min_stay: 
            half_in_hosp=row['los']
        for c_day in new_Col[:half_in_hosp]:
            if row[c_day] == uc: 
                count+=1
        if row['los']==0: 
            los+=1
            return(0)
        if half_in_hosp==0:
            los_half+=1
            return(0)
        return(count/half_in_hosp) #

    for u_c in unique_comb:
        print('u_c',u_c)
        df[u_c] = df.apply(lambda row : days_on_treat_func(row,u_c,los,los_half), axis=1) 
        print("datetime.now()",datetime.now())

    print('los = 0 ',los, ' los_half=0',los_half)
    return(df)  

    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_'
    #  'Coag_Biot_' 'Coag_Biot_Misc_' 'Coag_Biot_Misc_Mono_' 'Coag_Biot_Mono_'
    #  'Coag_Misc_' 'Coag_Misc_Mono_' 'Coag_Mono_' 'Coag_Vira_'
    #  'Coag_Vira_Biot_' 'Coag_Vira_Biot_Misc_' 'Coag_Vira_Biot_Misc_Mono_'
    #  'Coag_Vira_Biot_Mono_' 'Coag_Vira_Misc_' 'Coag_Vira_Misc_Mono_'
    #  'Coag_Vira_Mono_' 'Misc_' 'Misc_Mono_' 'Mono_' 'Ster_' 'Ster_Biot_'
    #  'Ster_Biot_Misc_' 'Ster_Biot_Misc_Mono_' 'Ster_Biot_Mono_' 'Ster_Coag_'
    #  'Ster_Coag_Biot_' 'Ster_Coag_Biot_Misc_' 'Ster_Coag_Biot_Misc_Mono_'
    #  'Ster_Coag_Biot_Mono_' 'Ster_Coag_Misc_' 'Ster_Coag_Misc_Mono_'
    #  'Ster_Coag_Mono_' 'Ster_Coag_Vira_' 'Ster_Coag_Vira_Biot_'
    #  'Ster_Coag_Vira_Biot_Misc_' 'Ster_Coag_Vira_Biot_Misc_Mono_'
    #  'Ster_Coag_Vira_Biot_Mono_' 'Ster_Coag_Vira_Misc_'
    #  'Ster_Coag_Vira_Misc_Mono_' 'Ster_Coag_Vira_Mono_' 'Ster_Misc_'
    #  'Ster_Misc_Mono_' 'Ster_Mono_' 'Ster_Vira_' 'Ster_Vira_Biot_'
    #  'Ster_Vira_Biot_Misc_' 'Ster_Vira_Biot_Misc_Mono_' 'Ster_Vira_Biot_Mono_'
    #  'Ster_Vira_Misc_' 'Ster_Vira_Misc_Mono_' 'Ster_Vira_Mono_' 'Vira_'
    #  'Vira_Biot_' 'Vira_Biot_Misc_' 'Vira_Biot_Misc_Mono_' 'Vira_Biot_Mono_'
    #  'Vira_Misc_' 'Vira_Misc_Mono_' 'Vira_Mono_']

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.cc5a34cf-f636-411b-ada4-c55f4ff68cdf"),
    los_or_3=Input(rid="ri.foundry.main.dataset.c208a92a-e905-4ddd-8536-05ed6b7ffae9")
)
def los_or_3_1and11(los_or_3):
    derived_cobination = los_or_3

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.608c607a-5d83-4153-8b48-c907540d7b65"),
    los_or_3=Input(rid="ri.foundry.main.dataset.c208a92a-e905-4ddd-8536-05ed6b7ffae9")
)
def los_or_3_1and11_1(los_or_3):
    derived_cobination = los_or_3

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = False
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.fbbab98e-2ab3-4cf5-b717-6132cd2d1492"),
    los_or_3=Input(rid="ri.foundry.main.dataset.c208a92a-e905-4ddd-8536-05ed6b7ffae9"),
    los_or_3_1and11=Input(rid="ri.foundry.main.dataset.cc5a34cf-f636-411b-ada4-c55f4ff68cdf")
)
def los_or_3_all(los_or_3, los_or_3_1and11):
    XG_DrugComb_NoMax_Shap1_11_matching_overal_1_1_1_copied_copied_copied_copied = los_or_3_1and11
    derived_cobination = los_or_3

    ######F
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    # pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '2_code'] = 1 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '2_code'] =  0   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '2_code'] = 0  # deteriorated
    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.09eb2381-45dd-470a-ab37-512f083d25ac"),
    derived_set=Input(rid="ri.foundry.main.dataset.1be6df15-36dd-41af-a16f-e5794835b64b")
)
def los_or_5( derived_set):

    import pandas as pd
    import numpy as np
    from datetime import datetime

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = derived_set
    # df = df.head(2000)
    
    pts_col = [  'age', 'bmi', 'CCI_INDEX', 'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', # 'date_of_earliest_covid_diagnosis',
     'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
     'gender_female', 'gender_male', 'gender_other', 'race_asian', 'race_black_or_african_american', 'race_native_hawaiian_or_other_pacific_islander', 'race_other', 'race_white',
      'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other','os_day1',
       '2021Q3', '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', '2019Q4','2019Q3', '2019Q2', '2019Q1', '2018Q4', '2018Q3', '2018Q2', '2018Q1']
    # drug_col = ['Ster_day1',     'Coag_day1',     'Vira_day1',      'Biot_day1',      'Misc_day1',     'Mono_day1', ] os_col = [ 'os_day1',]
    # df['los']
    # df['time_to_outcome']      
    # df['max_level'] 
    # df['days_to_max_level']
    # df['days_max_to_outcome']       = df['time_to_outcome'] -  df['days_to_max_level']
    # df['deter_speed']               = (df['max_level']-df['os_day1'])/(df['days_to_max_level'])
    # df['enhanc_speed']              = (df['max_level']-df['os_day28'])/(df['days_max_to_outcome'])
    # for treat in treatments:
        # new_Col =[treat +'before_max', treat+'after_max']

    ###
    days =  ['day'+str(x) for x in range(1,29,1) ]
    os_days = ['os_'+x for x in days ]
    days_reverse =  ['day'+str(x) for x in range(28,0,-1) ]
    os_days_reverse = ['os_'+x for x in days_reverse ]

    ### 'comb_func' #################
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    print('comb_func(row)')
    def comb_func(row):
        output = pd.Series()
        for day in days:
            comb_day = ""
            for treat in treatments:
                if row[treat+day]==1:
                    comb_day+=treat
            output['comb_'+day]  = comb_day
        return(output)
    new_Col =['comb_'+day for day in days ]
    print('new_Col',new_Col)
    df[new_Col] = df.apply(lambda row : comb_func(row), axis=1) 
    
    
    los = 0
    los_half= 0 
    min_stay = 5
    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_' ...
    unique_comb =np.unique(df[new_Col].values)
    def days_on_treat_func(row,uc,los,los_half):
        count= 0
        half_in_hosp = min_stay
        if row['los'] <= min_stay: 
            half_in_hosp=row['los']
        for c_day in new_Col[:half_in_hosp]:
            if row[c_day] == uc: 
                count+=1
        if row['los']==0: 
            los+=1
            return(0)
        if half_in_hosp==0:
            los_half+=1
            return(0)
        return(count/half_in_hosp) #

    for u_c in unique_comb:
        print('u_c',u_c)
        df[u_c] = df.apply(lambda row : days_on_treat_func(row,u_c,los,los_half), axis=1) 
        print("datetime.now()",datetime.now())

    print('los = 0 ',los, ' los_half=0',los_half)
    return(df)  

    #  unique_comb = ['' 'Biot_' 'Biot_Misc_' 'Biot_Misc_Mono_' 'Biot_Mono_' 'Coag_'
    #  'Coag_Biot_' 'Coag_Biot_Misc_' 'Coag_Biot_Misc_Mono_' 'Coag_Biot_Mono_'
    #  'Coag_Misc_' 'Coag_Misc_Mono_' 'Coag_Mono_' 'Coag_Vira_'
    #  'Coag_Vira_Biot_' 'Coag_Vira_Biot_Misc_' 'Coag_Vira_Biot_Misc_Mono_'
    #  'Coag_Vira_Biot_Mono_' 'Coag_Vira_Misc_' 'Coag_Vira_Misc_Mono_'
    #  'Coag_Vira_Mono_' 'Misc_' 'Misc_Mono_' 'Mono_' 'Ster_' 'Ster_Biot_'
    #  'Ster_Biot_Misc_' 'Ster_Biot_Misc_Mono_' 'Ster_Biot_Mono_' 'Ster_Coag_'
    #  'Ster_Coag_Biot_' 'Ster_Coag_Biot_Misc_' 'Ster_Coag_Biot_Misc_Mono_'
    #  'Ster_Coag_Biot_Mono_' 'Ster_Coag_Misc_' 'Ster_Coag_Misc_Mono_'
    #  'Ster_Coag_Mono_' 'Ster_Coag_Vira_' 'Ster_Coag_Vira_Biot_'
    #  'Ster_Coag_Vira_Biot_Misc_' 'Ster_Coag_Vira_Biot_Misc_Mono_'
    #  'Ster_Coag_Vira_Biot_Mono_' 'Ster_Coag_Vira_Misc_'
    #  'Ster_Coag_Vira_Misc_Mono_' 'Ster_Coag_Vira_Mono_' 'Ster_Misc_'
    #  'Ster_Misc_Mono_' 'Ster_Mono_' 'Ster_Vira_' 'Ster_Vira_Biot_'
    #  'Ster_Vira_Biot_Misc_' 'Ster_Vira_Biot_Misc_Mono_' 'Ster_Vira_Biot_Mono_'
    #  'Ster_Vira_Misc_' 'Ster_Vira_Misc_Mono_' 'Ster_Vira_Mono_' 'Vira_'
    #  'Vira_Biot_' 'Vira_Biot_Misc_' 'Vira_Biot_Misc_Mono_' 'Vira_Biot_Mono_'
    #  'Vira_Misc_' 'Vira_Misc_Mono_' 'Vira_Mono_']

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.8b407c64-76ee-49c4-8280-4f80f3533dda"),
    los_or_5=Input(rid="ri.foundry.main.dataset.09eb2381-45dd-470a-ab37-512f083d25ac")
)
def los_or_5_1and11(los_or_5):
    derived_cobination = los_or_5

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.57584df3-63f6-440c-8776-7f2f65b406f5"),
    los_or_5=Input(rid="ri.foundry.main.dataset.09eb2381-45dd-470a-ab37-512f083d25ac"),
    los_or_5_1and11=Input(rid="ri.foundry.main.dataset.8b407c64-76ee-49c4-8280-4f80f3533dda")
)
def los_or_5_all(los_or_5, los_or_5_1and11):
    XG_DrugComb_NoMax_Shap1_11_matching_overal_1_1_1_copied_copied_copied = los_or_5_1and11
    derived_cobination = los_or_5

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier()
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.b7ceea1e-d261-488d-93e1-4499d8eebe91"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def medication_ROC(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    from sklearn.model_selection import StratifiedKFold
    from sklearn.model_selection import KFold
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col_no_treat = ['2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0']
    pts_col =  pts_col_no_treat +  treat_unique #
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    counter_fold = 0
    kf = KFold(n_splits=10)
    skf = StratifiedKFold(n_splits=10)
    # skf.split(X, y):
    for train_index, test_index in skf.split(pts[pts_col ],pts[outcome]):
        counter_fold+=1
        print("\n\n\n#####################################\n FOLD ",counter_fold,"\n#################################\n")
        # x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
        x_train= pts.iloc[train_index][pts_col ]
        x_test= pts.iloc[test_index][pts_col ]
        y_train= pts.iloc[train_index][outcome ]
        y_test= pts.iloc[test_index][outcome ]
        
        # y_train = pd.DataFrame(data=y_train,columns=outcome)
        # x_train = pd.DataFrame(data=x_train,columns=pts_col)

        print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
        print('onehot y_train')
        print(onehot.sum(axis=0))
    
        ### Class weight
        print("\nClass weight")
        num_classes = len(pts[outcome[0]].unique())
        if model_type == 'binary' or model_type == 'regression':
            num_classes = 1
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
        class_counts = onehot.sum(axis=0).values
        total_count = sum(class_counts)
        class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
        class_weights = dict(enumerate(class_rate))
        print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

        print("\nscale")
        scale=   treat_unique # [ 'bmi'] + + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
        #  'age',
        print(scale) 
        scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
        x_train.loc[:,scale] = scale_x.transform(x_train[scale])
        x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
        
        original_x_train = x_train.copy()
        original_y_train = y_train.copy()
        original_x_test = x_test.copy()
        original_y_test = y_test.copy()
        ### SMOTE
        if resample:
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7) 
            x_train, y_train = sm.fit_sample(x_train, y_train)
            print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
            x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
            y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
            for c in range(num_classes):
                class_weights[c]=1
            print('class weights reseted: ',class_weights)
        
        ### split learn/val
        print('\nsplit learn/val')
        x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
        print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

        print('\nonehot train/test/learn/val')
        if model_type == 'classification' :
            y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_train.sum(axis=0))
            y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_test.sum(axis=0))
            y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_learn.sum(axis=0))
            y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_val.sum(axis=0))

        ### Hyper Param 
        ######################################
        param_grid = []
        if optimize == True:
            neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
            layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
            drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
            batchNorm = hp.choice( 'batchNorm',[True, False] )
            activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
            learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
            epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
            batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

            param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                    drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                    learning_ratio=learning_ratio, # compile
                    epochs=epochs, batch_size=batch_size ) # fit           

            if model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
    
        ### Functions ### 
        ######################################
        print("Model_name full")
        def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            print('TEST__ param_grid:', param_grid)

            # Model
            neuron_comb =  param_grid['neuron_comb']
            layers_comb = param_grid['layers_comb']
            drop_rate =  param_grid['drop_rate'] 
            batchNorm = param_grid['batchNorm'] 
            activation_out = param_grid['activation_out']
            # compile
            learning_ratio = param_grid['learning_ratio']
            # fit
            epochs = param_grid['epochs']
            batch_size= param_grid['batch_size']
        
            ## model__fn
            inputA = Input(shape=(demo_shape,))
            if final_model:
                print('\nFinal_model inputA.shape',inputA.shape)
    
            attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
            inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
            
            fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
            fully = Dropout(drop_rate,name="Drop-1")(fully)
            if batchNorm:
                fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

            for l in range(2,layers_comb+1,1):
                    fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                    fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                    if batchNorm:
                        fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

            out= []
            if model_type != 'regression':
                out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
                # sigmoid softmax tanh relu PReLU Leaky ReLU 
            else:
                out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
                # linear' bias_initializer="zeros",
                # kernel_regularizer=None,
                # bias_regularizer=None,
                # activity_regularizer=None,
            model = Model(inputs=inputA, outputs=out)

            if final_model:
                model.summary()

            loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
            metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
            if model_type == 'binary':
                loss = 'binary_crossentropy'
            if model_type == 'regression':
                loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
                metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
            model.compile(loss = loss,
                        metrics=metrics,
                        optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

            es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
            if model_type == 'regression':
                es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
                mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

            history = []
            if model_type == 'classification':
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values), 
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc],
                                    class_weight=class_weights
                                    )
                model = load_model('best_model.h5')
            else:
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values),
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc]
                                )
                model = load_model('best_model.h5')

            pred_prob = model.predict( x_val[pts_col].values)
            if np.isnan(pred_prob).any():
                print("ERROR np.nan_to_num(pred_prob)")
                pred_prob = np.nan_to_num(pred_prob)
            predictions = pred_prob.argmax(axis=1)
            if model_type == 'binary':
                fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
                gmeans = np.sqrt(tpr * (1-fpr))
                ix = np.argmax(gmeans)
                print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
                predictions = (pred_prob >thresholds[ix]).astype(np.int) 
            if model_type == 'regression':
                predictions = model.predict( x_val[pts_col].values)
            y_val_binary = y_val.copy()
            if model_type == 'classification':
                y_val = y_val.values.argmax(axis=1)
        
            ### for ploting send out
            print('history.history.keys()',history.history.keys())
            train_acc=[]
            val_acc=[]
            if model_type == 'classification' or model_type == 'binary':
                train_acc = history.history['auc'] 
                val_acc = history.history['val_auc']
            elif model_type == 'regression':
                train_acc = history.history['mape_metric']
                val_acc = history.history['val_mape_metric']
            else:
                print("Error model type")
                return(0)
            train_loss = history.history['loss']
            val_loss = history.history['val_loss']
            epochs = range(len(train_acc))
        
            if model_type == 'regression' :
                y_val = y_val_binary
                actual, pred = np.array(y_val), np.array(predictions)
                mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
                mape_acc = 100 - mape_error 
                results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                            'accuracy_score_val': mape_acc, 
                            'train_acc':train_acc, 'val_acc':val_acc, 
                            'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                        }
                print("TEST__ accuracy_score:mape_acc ", mape_acc)
                print("datetime.now()",datetime.now())
                return (results)

            if model_type == 'binary' or model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)
                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
            
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    print('TEST__ confusion:',class_str,class_accuracy)
    

                results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
        ######################################
        print("Model_name rf")
        def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            
            ## model__rf
            # model = RandomForestClassifier(random_state=7)
            model = sk.ensemble.GradientBoostingClassifier(n_estimators=100) #1000
            # model = sk.neighbors.KNeighborsClassifier()
            # model = sk.svm.SVC()

            model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
            predictions = model.predict(x_val[pts_col].values)
            pred_prob = model.predict_proba(x_val[pts_col].values)
            y_val_binary = y_val
            y_val = y_val.values.argmax(axis=1)
            
            ### for ploting send out
            train_acc=0
            val_acc=0
            train_loss = 0
            val_loss = 0
            epochs = 0
        
            if model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)
                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
            
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    print('TEST__ confusion:',class_str,class_accuracy)
    

                results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
            else:
                print("\nwrong model type")
                return(0)
        ######################################
        print("Results #############################")
        ### trials 
        tpe_trials = Trials()
        tpe_best = []
        if optimize:
            if model_name == 'full':
                tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
            elif model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
            else:
                print("Error optimize: model not found")
                return(0)
            param_grid= space_eval(param_grid, tpe_best)
            print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
            print("datetime.now(): ", datetime.now())
        else: # __param
            if model_name == 'full':
                param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
                if max_level == 3:
                    param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
                elif max_level == 5:
                    param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
                elif max_level == 5:
                    param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
                elif max_level == 9:
                    param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

                if model_type == 'binary':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
                if model_type == 'regression':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif model_name == 'rf':
                print("rf not optimize")
            else:
                print("Error optimize: model not found")
                return(0)

        print("\nFinal Model")
        final_results = []
        final_model =  True
        # __call
        if model_name == 'full':
            final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
        if model_name == 'rf':
            final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
        
        ## __plot
        if True: 
            # fig = plt.figure(figsize=(15,15)) 
            # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
            # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
            # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

            # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
            # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
            # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
            # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
            # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
            # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

            fpr, tpr, threshold = final_results['roc_curve_val']
            roc_auc = auc(fpr, tpr)
            # ax1.set_title('Receiver Operating Characteristic')
            # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
            # ax1.legend(loc = 'lower right')
            # ax1.plot([0, 1], [0, 1],'r--')
            # ax1.set_xlim([0, 1])
            # ax1.set_ylim([0, 1])
            # ax1.set_ylabel('True Positive Rate')
            # ax1.set_xlabel('False Positive Rate')

            interp_tpr = np.interp(mean_fpr, fpr, tpr)
            interp_tpr[0] = 0.0
            tprs.append(interp_tpr)
            aucs.append(roc_auc)

    # __ft
    ax.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)

    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs)
    ax.plot(
        mean_fpr,
        mean_tpr,
        color="b",
        label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
        lw=2,
        alpha=0.8,
    )

    std_tpr = np.std(tprs, axis=0)
    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
    ax.fill_between(
        mean_fpr,
        tprs_lower,
        tprs_upper,
        color="grey",
        alpha=0.2,
        label=r"$\pm$ 1 std. dev.",
    )

    ax.set(
        xlim=[-0.05, 1.05],
        ylim=[-0.05, 1.05],
        title="Receiver Operating Characteristic (ROC)",
    )
    ax.legend(loc="lower right")
    plt.show()

    print("after KFold for : datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 
    
    if False: 
        if False:
            print('\nInterpret model')
            ## __shap __importance __inter
            if model_importance and False: 

                x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
                y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
                xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
                print('y_overal',y_overal.sum(axis=0))
                print('xy_overal.shape: ',xy_overal.shape)
                print("\nMissigness")
                for col in xy_overal.columns:
                    missigness = xy_overal[col].isna().astype(int).sum() 
                    if missigness/float(xy_overal.shape[0])!=0:
                        print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
                print("Before Dropna pts:",xy_overal.shape[0])
                xy_overal.dropna(inplace=True) # 
                print("After Dropna pts:",xy_overal.shape[0],"\n")
                # return(xy_overal)
        
                propensity = LogisticRegression()
                propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
                pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
                print ('pscore[:5]',pscore[:5])
                xy_overal['Propensity'] = pscore

                print("\nMatchFrame")
                g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
                matched_frame = pd.DataFrame()
                counter =0
                print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
                for index, row in g_death.iterrows():
                    g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                    closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                    matched_frame = matched_frame.append(row, ignore_index = True)
                    matched_frame = matched_frame.append(closest, ignore_index = True)
                    g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                    # if counter ==1000:
                    #     print(matched_frame[['Propensity','prop_diff']].head(10))
                    #     break
                    # counter +=1
                print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
                print('matched_frame.shape',matched_frame.shape)
                print(matched_frame.head(4))

                # x_overal = original_x_train[pts_col]
                # y_overal = original_y_train[outcome]
                # xy_overal = pd.concat([ x_overal , y_overal  ],
                print('\nSMOTE')
                sm = RandomOverSampler(random_state = 7)
                print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
                x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
                print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
                if model_type == 'classification' :
                    y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                    print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

                if model_name == 'full':
                    final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
                if model_name == 'rf':
                    final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
                model = final_results['model']

                print("\nKmean start datetime.now(): ", datetime.now())
                kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
                kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
                print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
                print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
                centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
                print('centroids',centroids.shape)
                
                training = centroids           
                testing = matched_frame[pts_col] 
                
                print("\nShap Start datetime.now(): ", datetime.now())
                explainer = []
                if model_name == 'rf':
                    explainer = shap.KernelExplainer(model.predict_proba, training)
                else:
                    explainer = shap.KernelExplainer(model.predict, training)
                shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
                print("Shap datetime.now(): ", datetime.now())
                    
                print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
                print('type shap: ',type(shap_values))
                print('type shap class: ', type(shap_values[0]))

                num_class_samples_columns =len(shap_values[target_shap_class][0])
                all_variables = np.array([0 for _ in range(num_class_samples_columns)])
                for x in range(shap_values[target_shap_class].shape[0]) :
                    all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
                all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
                print('all_variables avegrafe Shap - shape',all_variables.shape)

                overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
                overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
                overall_importance_df = overall_importance_df.iloc[0]

                overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
                ax2.set_title("Feature importances ")
                ax2.set_ylabel("Shap value")
                fig.tight_layout()
                plt.tight_layout()
                plt.show()
                print("show")
    
                print("overall_importance_df Shap")
                imp_rf = pd.DataFrame() 
                imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
                # return( imp_rf ) 

                print("Detail Shap")
                detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
                return(detailed_shap_df)
                
            print('\nFinal accuracy Table')
            param_grid_ = []
            if model_name != 'rf':
                param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

            accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
            f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
            precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
            recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
            roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
            finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
            
            confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
                finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
            finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
            print("datetime.now(): ", datetime.now())
            plt.tight_layout()
            plt.show()
            print("show")
            return(finalResults)
        
        print("after KFold for : datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.c594fc54-f935-4567-97a2-82fc91649289"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def medication_only(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col_no_treat = ['2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0']
    pts_col =  pts_col_no_treat +  treat_unique #
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale=   treat_unique # [ 'bmi'] + + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge.loc[:,'prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.dee09859-fe23-4bd3-8d46-c1c8cc7f38b8"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def new_shap(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            # kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            # kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col]) 'os_day1_1.0'
            kmeans_1 = sk.cluster.KMeans(n_clusters=20, random_state=0).fit(x_overal.loc[x_overal['os_day1_1.0']==1,pts_col])
            kmeans_3 = sk.cluster.KMeans(n_clusters=20, random_state=0).fit(x_overal.loc[x_overal['os_day1_3.0']==1,pts_col])
            kmeans_5 = sk.cluster.KMeans(n_clusters=20, random_state=0).fit(x_overal.loc[x_overal['os_day1_5.0']==1,pts_col])
            kmeans_7 = sk.cluster.KMeans(n_clusters=20, random_state=0).fit(x_overal.loc[x_overal['os_day1_7.0']==1,pts_col])
            kmeans_9 = sk.cluster.KMeans(n_clusters=20, random_state=0).fit(x_overal.loc[x_overal['os_day1_9.0']==1,pts_col])

            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_3.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_3.cluster_centers_, axis=0)
            centroids= np.append(centroids, kmeans_5.cluster_centers_, axis=0)
            centroids= np.append(centroids, kmeans_7.cluster_centers_, axis=0)
            centroids= np.append(centroids, kmeans_9.cluster_centers_, axis=0)

            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.d59ecd3c-d198-41aa-873d-6579d0454d37"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def no_treat(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat# +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi']# +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.cc2bc92b-5e59-46ee-b953-6d536abe4fff"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def no_treat_ROC(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    from sklearn.model_selection import StratifiedKFold
    from sklearn.model_selection import KFold
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat# +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    counter_fold = 0
    kf = KFold(n_splits=10)
    skf = StratifiedKFold(n_splits=10)
    # skf.split(X, y):
    for train_index, test_index in skf.split(pts[pts_col ],pts[outcome]):
        counter_fold+=1
        print("\n\n\n#####################################\n FOLD ",counter_fold,"\n#################################\n")
        # x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
        x_train= pts.iloc[train_index][pts_col ]
        x_test= pts.iloc[test_index][pts_col ]
        y_train= pts.iloc[train_index][outcome ]
        y_test= pts.iloc[test_index][outcome ]
        
        # y_train = pd.DataFrame(data=y_train,columns=outcome)
        # x_train = pd.DataFrame(data=x_train,columns=pts_col)

        print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
        print('onehot y_train')
        print(onehot.sum(axis=0))
    
        ### Class weight
        print("\nClass weight")
        num_classes = len(pts[outcome[0]].unique())
        if model_type == 'binary' or model_type == 'regression':
            num_classes = 1
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
        class_counts = onehot.sum(axis=0).values
        total_count = sum(class_counts)
        class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
        class_weights = dict(enumerate(class_rate))
        print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

        print("\nscale")
        scale= [ 'bmi']# +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
        #  'age',
        print(scale) 
        scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
        x_train.loc[:,scale] = scale_x.transform(x_train[scale])
        x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
        
        original_x_train = x_train.copy()
        original_y_train = y_train.copy()
        original_x_test = x_test.copy()
        original_y_test = y_test.copy()
        ### SMOTE
        if resample:
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7) 
            x_train, y_train = sm.fit_sample(x_train, y_train)
            print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
            x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
            y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
            for c in range(num_classes):
                class_weights[c]=1
            print('class weights reseted: ',class_weights)
        
        ### split learn/val
        print('\nsplit learn/val')
        x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
        print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

        print('\nonehot train/test/learn/val')
        if model_type == 'classification' :
            y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_train.sum(axis=0))
            y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_test.sum(axis=0))
            y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_learn.sum(axis=0))
            y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_val.sum(axis=0))

        ### Hyper Param 
        ######################################
        param_grid = []
        if optimize == True:
            neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
            layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
            drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
            batchNorm = hp.choice( 'batchNorm',[True, False] )
            activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
            learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
            epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
            batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

            param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                    drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                    learning_ratio=learning_ratio, # compile
                    epochs=epochs, batch_size=batch_size ) # fit           

            if model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
    
        ### Functions ### 
        ######################################
        print("Model_name full")
        def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            print('TEST__ param_grid:', param_grid)

            # Model
            neuron_comb =  param_grid['neuron_comb']
            layers_comb = param_grid['layers_comb']
            drop_rate =  param_grid['drop_rate'] 
            batchNorm = param_grid['batchNorm'] 
            activation_out = param_grid['activation_out']
            # compile
            learning_ratio = param_grid['learning_ratio']
            # fit
            epochs = param_grid['epochs']
            batch_size= param_grid['batch_size']
        
            ## model__fn
            inputA = Input(shape=(demo_shape,))
            if final_model:
                print('\nFinal_model inputA.shape',inputA.shape)
    
            attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
            inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
            
            fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
            fully = Dropout(drop_rate,name="Drop-1")(fully)
            if batchNorm:
                fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

            for l in range(2,layers_comb+1,1):
                    fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                    fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                    if batchNorm:
                        fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

            out= []
            if model_type != 'regression':
                out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
                # sigmoid softmax tanh relu PReLU Leaky ReLU 
            else:
                out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
                # linear' bias_initializer="zeros",
                # kernel_regularizer=None,
                # bias_regularizer=None,
                # activity_regularizer=None,
            model = Model(inputs=inputA, outputs=out)

            if final_model:
                model.summary()

            loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
            metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
            if model_type == 'binary':
                loss = 'binary_crossentropy'
            if model_type == 'regression':
                loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
                metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
            model.compile(loss = loss,
                        metrics=metrics,
                        optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

            es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
            if model_type == 'regression':
                es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
                mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

            history = []
            if model_type == 'classification':
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values), 
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc],
                                    class_weight=class_weights
                                    )
                model = load_model('best_model.h5')
            else:
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values),
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc]
                                )
                model = load_model('best_model.h5')

            pred_prob = model.predict( x_val[pts_col].values)
            if np.isnan(pred_prob).any():
                print("ERROR np.nan_to_num(pred_prob)")
                pred_prob = np.nan_to_num(pred_prob)
            predictions = pred_prob.argmax(axis=1)
            if model_type == 'binary':
                fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
                gmeans = np.sqrt(tpr * (1-fpr))
                ix = np.argmax(gmeans)
                print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
                predictions = (pred_prob >thresholds[ix]).astype(np.int) 
            if model_type == 'regression':
                predictions = model.predict( x_val[pts_col].values)
            y_val_binary = y_val.copy()
            if model_type == 'classification':
                y_val = y_val.values.argmax(axis=1)
        
            ### for ploting send out
            print('history.history.keys()',history.history.keys())
            train_acc=[]
            val_acc=[]
            if model_type == 'classification' or model_type == 'binary':
                train_acc = history.history['auc'] 
                val_acc = history.history['val_auc']
            elif model_type == 'regression':
                train_acc = history.history['mape_metric']
                val_acc = history.history['val_mape_metric']
            else:
                print("Error model type")
                return(0)
            train_loss = history.history['loss']
            val_loss = history.history['val_loss']
            epochs = range(len(train_acc))
        
            if model_type == 'regression' :
                y_val = y_val_binary
                actual, pred = np.array(y_val), np.array(predictions)
                mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
                mape_acc = 100 - mape_error 
                results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                            'accuracy_score_val': mape_acc, 
                            'train_acc':train_acc, 'val_acc':val_acc, 
                            'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                        }
                print("TEST__ accuracy_score:mape_acc ", mape_acc)
                print("datetime.now()",datetime.now())
                return (results)

            if model_type == 'binary' or model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)
                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
            
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    print('TEST__ confusion:',class_str,class_accuracy)
    

                results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
        ######################################
        print("Model_name rf")
        def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            
            ## model__rf
            # model = RandomForestClassifier(random_state=7)
            model = sk.ensemble.GradientBoostingClassifier(n_estimators=100) #1000
            # model = sk.neighbors.KNeighborsClassifier()
            # model = sk.svm.SVC()

            model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
            predictions = model.predict(x_val[pts_col].values)
            pred_prob = model.predict_proba(x_val[pts_col].values)
            y_val_binary = y_val
            y_val = y_val.values.argmax(axis=1)
            
            ### for ploting send out
            train_acc=0
            val_acc=0
            train_loss = 0
            val_loss = 0
            epochs = 0
        
            if model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)
                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
            
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    print('TEST__ confusion:',class_str,class_accuracy)
    

                results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
            else:
                print("\nwrong model type")
                return(0)
        ######################################
        print("Results #############################")
        ### trials 
        tpe_trials = Trials()
        tpe_best = []
        if optimize:
            if model_name == 'full':
                tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
            elif model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
            else:
                print("Error optimize: model not found")
                return(0)
            param_grid= space_eval(param_grid, tpe_best)
            print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
            print("datetime.now(): ", datetime.now())
        else: # __param
            if model_name == 'full':
                if max_level == 3:
                    param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
                elif max_level == 5:
                    param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
                elif max_level == 5:
                    param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
                elif max_level == 9:
                    param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

                if model_type == 'binary':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
                if model_type == 'regression':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif model_name == 'rf':
                print("rf not optimize")
            else:
                print("Error optimize: model not found")
                return(0)

        print("\nFinal Model")
        final_results = []
        final_model =  True
        # __call
        if model_name == 'full':
            final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
        if model_name == 'rf':
            final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
        
        ## __plot
        if True: 
            # fig = plt.figure(figsize=(15,15)) 
            # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
            # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
            # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

            # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
            # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
            # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
            # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
            # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
            # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

            fpr, tpr, threshold = final_results['roc_curve_val']
            roc_auc = auc(fpr, tpr)
            # ax1.set_title('Receiver Operating Characteristic')
            # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
            # ax1.legend(loc = 'lower right')
            # ax1.plot([0, 1], [0, 1],'r--')
            # ax1.set_xlim([0, 1])
            # ax1.set_ylim([0, 1])
            # ax1.set_ylabel('True Positive Rate')
            # ax1.set_xlabel('False Positive Rate')

            interp_tpr = np.interp(mean_fpr, fpr, tpr)
            interp_tpr[0] = 0.0
            tprs.append(interp_tpr)
            aucs.append(roc_auc)

    # __ft
    ax.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)

    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs)
    ax.plot(
        mean_fpr,
        mean_tpr,
        color="b",
        label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc),
        lw=2,
        alpha=0.8,
    )

    std_tpr = np.std(tprs, axis=0)
    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr, 0)
    ax.fill_between(
        mean_fpr,
        tprs_lower,
        tprs_upper,
        color="grey",
        alpha=0.2,
        label=r"$\pm$ 1 std. dev.",
    )

    ax.set(
        xlim=[-0.05, 1.05],
        ylim=[-0.05, 1.05],
        title="Receiver Operating Characteristic (ROC)",
    )
    ax.legend(loc="lower right")
    plt.show()

    print("after KFold for : datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 
    
    if False: 
        if False:
            print('\nInterpret model')
            ## __shap __importance __inter
            if model_importance and False: 

                x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
                y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
                xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
                print('y_overal',y_overal.sum(axis=0))
                print('xy_overal.shape: ',xy_overal.shape)
                print("\nMissigness")
                for col in xy_overal.columns:
                    missigness = xy_overal[col].isna().astype(int).sum() 
                    if missigness/float(xy_overal.shape[0])!=0:
                        print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
                print("Before Dropna pts:",xy_overal.shape[0])
                xy_overal.dropna(inplace=True) # 
                print("After Dropna pts:",xy_overal.shape[0],"\n")
                # return(xy_overal)
        
                propensity = LogisticRegression()
                propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
                pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
                print ('pscore[:5]',pscore[:5])
                xy_overal['Propensity'] = pscore

                print("\nMatchFrame")
                g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
                matched_frame = pd.DataFrame()
                counter =0
                print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
                for index, row in g_death.iterrows():
                    g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                    closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                    matched_frame = matched_frame.append(row, ignore_index = True)
                    matched_frame = matched_frame.append(closest, ignore_index = True)
                    g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                    # if counter ==1000:
                    #     print(matched_frame[['Propensity','prop_diff']].head(10))
                    #     break
                    # counter +=1
                print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
                print('matched_frame.shape',matched_frame.shape)
                print(matched_frame.head(4))

                # x_overal = original_x_train[pts_col]
                # y_overal = original_y_train[outcome]
                # xy_overal = pd.concat([ x_overal , y_overal  ],
                print('\nSMOTE')
                sm = RandomOverSampler(random_state = 7)
                print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
                x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
                print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
                if model_type == 'classification' :
                    y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                    print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

                if model_name == 'full':
                    final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
                if model_name == 'rf':
                    final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
                model = final_results['model']

                print("\nKmean start datetime.now(): ", datetime.now())
                kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
                kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
                print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
                print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
                centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
                print('centroids',centroids.shape)
                
                training = centroids           
                testing = matched_frame[pts_col] 
                
                print("\nShap Start datetime.now(): ", datetime.now())
                explainer = []
                if model_name == 'rf':
                    explainer = shap.KernelExplainer(model.predict_proba, training)
                else:
                    explainer = shap.KernelExplainer(model.predict, training)
                shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
                print("Shap datetime.now(): ", datetime.now())
                    
                print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
                print('type shap: ',type(shap_values))
                print('type shap class: ', type(shap_values[0]))

                num_class_samples_columns =len(shap_values[target_shap_class][0])
                all_variables = np.array([0 for _ in range(num_class_samples_columns)])
                for x in range(shap_values[target_shap_class].shape[0]) :
                    all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
                all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
                print('all_variables avegrafe Shap - shape',all_variables.shape)

                overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
                overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
                overall_importance_df = overall_importance_df.iloc[0]

                overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
                ax2.set_title("Feature importances ")
                ax2.set_ylabel("Shap value")
                fig.tight_layout()
                plt.tight_layout()
                plt.show()
                print("show")
    
                print("overall_importance_df Shap")
                imp_rf = pd.DataFrame() 
                imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
                # return( imp_rf ) 

                print("Detail Shap")
                detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
                return(detailed_shap_df)
                
            print('\nFinal accuracy Table')
            param_grid_ = []
            if model_name != 'rf':
                param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

            accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
            f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
            precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
            recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
            roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
            finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
            
            confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
                finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
            finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
            print("datetime.now(): ", datetime.now())
            plt.tight_layout()
            plt.show()
            print("show")
            return(finalResults)
        
        print("after KFold for : datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.d1feb7cf-e460-4681-9989-3496dce354fa"),
    los_or_5=Input(rid="ri.foundry.main.dataset.09eb2381-45dd-470a-ab37-512f083d25ac")
)
def only_demographics_1and11(los_or_5):
    derived_cobination = los_or_5

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = False
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat # +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] # +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=100)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.fec9d4b4-9c4c-4e10-8579-e42feeca980d"),
    los_or_5=Input(rid="ri.foundry.main.dataset.09eb2381-45dd-470a-ab37-512f083d25ac")
)
def only_demographics_1and11_1(los_or_5):
    derived_cobination = los_or_5

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = False
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat  +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi']  +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.1ade4208-272c-45cc-9f22-499ec86e8e05"),
    los_or_5=Input(rid="ri.foundry.main.dataset.09eb2381-45dd-470a-ab37-512f083d25ac")
)
def only_demographics_all(los_or_5):
    derived_cobination = los_or_5

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = False
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    # pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat # +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '2_code'] = 1 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '2_code'] =  0   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '2_code'] = 0  # deteriorated
    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] # +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.faaf122f-88c2-4f80-a660-251c6571a0ad"),
    los_or_5=Input(rid="ri.foundry.main.dataset.09eb2381-45dd-470a-ab37-512f083d25ac")
)
def only_demographics_all_1(los_or_5):
    derived_cobination = los_or_5

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = False
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    # pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat  +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '2_code'] = 1 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '2_code'] =  0   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '2_code'] = 0  # deteriorated
    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi']  +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.d21114b2-5c4a-4658-9889-8d7955023c80"),
    all_treat=Input(rid="ri.foundry.main.dataset.2499b206-c71b-4399-a4ad-03f2199047ba")
)
def shap_graph(all_treat):
    all_treat = all_treat
    import pandas as pd

    
    target_shap_class = 1 
    shap_values = all_treat

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]

    overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
    ax2.set_title("Feature importances ")
    ax2.set_ylabel("Shap value")
    fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.ca1be836-e641-450a-832d-971d3f4ecda7"),
    derived_set=Input(rid="ri.foundry.main.dataset.1be6df15-36dd-41af-a16f-e5794835b64b")
)
def test_x_3( derived_set):
    ######
    input_table = derived_set 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    max_level = 3 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    model_name = 'full'
    # model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =True
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age))
    pts['age60p']= pts['age'].apply(lambda x:1 if x>=60  else 0)
    pts['age60m']= pts['age'].apply(lambda x:1 if x<60  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col = [  'bmi', 'CCI_INDEX', 'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD',
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', 'gender_other', 
                'race_asian', 'race_black_or_african_american', 'race_native_hawaiian_or_other_pacific_islander', 'race_other', 'race_white',
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                'os_day1', 
                # 'age', # age_bucket
                'age60p', 'age60m',
                'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    print(pts_col)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    

    return( x_test) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.e5625c27-f829-4015-a6d3-14d7ba040f12"),
    final_dataset=Input(rid="ri.foundry.main.dataset.3edf50ba-bd93-4890-b061-3c28aa71577e")
)
def unnamed(final_dataset):
    import pandas as pd
    import numpy as np 
    import matplotlib.pyplot as plt
    
    days = [ 'day'+str(x) for x in range(1,29,1) ]
    
    df = final_dataset

    print('\ntry to filter')
    df =df.loc[:,df.isna().sum(axis=0) > 0]
    df =df.loc[:,df.isna().sum(axis=0) < 3000]
    print('filtered for na', df.shape)
    df = df.isna().sum(axis=0)
    print('filtered for na', df.shape)
    
    # fig = plt.figure(figsize=(7,20)) 
    df.reset_index(name="count").plot.bar(x='index', y='count', rot=45,figsize=(20,7)) # df[df[columns].sum(axis=1) > 0]
    plt.tight_layout()
    plt.show()
    

    return(    df.to_frame()     )

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.b32d0f37-aa07-4ab1-861e-427a3e2af278"),
    FMK_covidpos_new_OS_May=Input(rid="ri.foundry.main.dataset.eaed807a-4ebd-4c84-a302-b6fc22d373c6")
)
def unnamed_1( FMK_covidpos_new_OS_May):
    import pandas as pd
    import numpy as np 
    import matplotlib.pyplot as plt
    
    days = [ 'day'+str(x) for x in range(1,29,1) ]

    
    df = FMK_covidpos_new_OS_May

    print('\ntry to filter')
    df =df.loc[:,df.isna().sum(axis=0) > 0]
    # df =df.loc[:,df.isna().sum(axis=0) < 3000]
    print('filtered for na', df.shape)
    df = df.isna().sum(axis=0)
    print('filtered for na', df.shape)
    
    # fig = plt.figure(figsize=(7,20)) 
    df.reset_index(name="count").plot.bar(x='index', y='count', rot=45,figsize=(20,7)) # df[df[columns].sum(axis=1) > 0]
    plt.tight_layout()
    plt.show()
    

    return(    df.to_frame()     )

@transform_pandas(
    Output(rid="ri.vector.main.execute.fe9faa46-73ab-4906-9fff-970005207ea2"),
    FMK_control_covidpos_new_OS_May=Input(rid="ri.foundry.main.dataset.04bd788a-ed1b-4aa1-8a80-36c6a1f1af93")
)
def unnamed_3(FMK_control_covidpos_new_OS_May):
    import pandas as pd
    import numpy as np 
    import matplotlib.pyplot as plt
    
    days = [ 'day'+str(x) for x in range(1,29,1) ]
       
    df = FMK_control_covidpos_new_OS_May

    print('\ntry to filter')
    df =df.loc[:,df.isna().sum(axis=0) > 0]
    # df =df.loc[:,df.isna().sum(axis=0) < 3000]
    print('filtered for na', df.shape)
    df = df.isna().sum(axis=0)
    print('filtered for na', df.shape)
    
    fig = plt.figure(figsize=(7,15)) 
    df.reset_index(name="count").plot.bar(x='index', y='count', rot=45,figsize=(20,7)) # df[df[columns].sum(axis=1) > 0]
    plt.tight_layout()
    plt.show()
    

    return(    df.to_frame()     )

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.59b93e13-e5e0-47c3-9327-ff96e5bc7ce5"),
    drugs_by_day=Input(rid="ri.foundry.main.dataset.856f0c0e-c701-4d06-868b-5374a4e5af46")
)
def with_a_drug(drugs_by_day):

    drugs_by_day=drugs_by_day[drugs_by_day.iloc[:,2:].notna().any(axis=1)]

    return(drugs_by_day)
    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.90ffcea9-44e5-47f4-8fa2-a01d327751b2"),
    derived_set=Input(rid="ri.foundry.main.dataset.1be6df15-36dd-41af-a16f-e5794835b64b")
)
def y_test_3( derived_set):
    ######
    input_table = derived_set 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    max_level = 3 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    model_name = 'full'
    # model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =True
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age))
    pts['age60p']= pts['age'].apply(lambda x:1 if x>=60  else 0)
    pts['age60m']= pts['age'].apply(lambda x:1 if x<60  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col = [  'bmi', 'CCI_INDEX', 'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD',
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', 'gender_other', 
                'race_asian', 'race_black_or_african_american', 'race_native_hawaiian_or_other_pacific_islander', 'race_other', 'race_white',
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                'os_day1', 
                # 'age', # age_bucket
                'age60p', 'age60m',
                'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    print(pts_col)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    

    return( y_test) 

    

