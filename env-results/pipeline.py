

@transform_pandas(
    Output(rid="ri.vector.main.execute.98e5c3da-00ac-41d9-a51f-6d1cf53b5ead"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def CCI_INDEX(derived_cobination):
    derived_cobination = derived_cobination

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    import seaborn as sns
    
    df = derived_cobination
 
    fig = plt.figure(figsize=(15,15)) 
    ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
    ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
    ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)
    # fig, axs = plt.subplots(ncols=3)
    
    Col =  'CCI_INDEX'
    bin_size =20

    # title='Histogram Of Test Scores',
    # rot=45,
    # grid=True,

    # df.CCI_INDEX.value_counts.plot.bar(  ax=ax0) #column=Col,age
    sns.countplot(data=df,x =Col,ax=ax0).set(title=Col+' Distribution')
    
    df_cp =  df[ (df['os_day28']== 1) ]
    # df_cp.CCI_INDEX.value_counts.plot.bar(  ax=ax1)
    sns.countplot(data=df_cp,x =Col,ax=ax1).set(title=Col+' Survived') 

    df_cp =  df[ (df['os_day28']== 11) ]
    # df_cp.CCI_INDEX.value_counts.plot.bar(  ax=ax2)
    sns.countplot(data=df_cp,x =Col,ax=ax2).set(title=Col+' Death') 

    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")

    # plt.xlabel('Test Score')
    # plt.ylabel("Number Of Students");

    plt.xticks(rotation = 90)
    plt.tight_layout()
    plt.show()

    return(pd.DataFrame() )

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.3b64bbf1-f867-4300-96df-67c39d0dce46"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def Describe_data(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    # from numpy import expand_dims
    # # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    # from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    from sklearn.model_selection import StratifiedKFold
    from sklearn.model_selection import KFold
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown',]) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        # pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', 'gender_other', 
                'race_black_or_african_american', 'race_white',  'race_asian', 'race_other', 'race_native_hawaiian_or_other_pacific_islander',
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 1 ), '2_code_1'] = 1  # discharged
    pts.loc[(pts['os_day28'] != 1 ), '2_code_1'] = 0  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code_0'] = 1  # death
    pts.loc[(pts['os_day28'] != 11), '2_code_0'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    

    table = pd.DataFrame()
    # pts_col_no_treat 
    print("\n\n\n ####Generate Teble #### \n\n\n")
    counter = 0
    for x in ['age','bmi','time_to_outcome']:
        counter+=1
        print(x,end=' ,')
        print("mean, ",pts[x].mean(),end=" ,")
        print("std, ",pts[x].std(),end=" ,")
        if x=='time_to_outcome':
            # '2_code_1','2_code_0' 
            print("median time_to_outcome 2_code_1, ",pts[pts['2_code_1']==1][x].median())
            print("median time_to_outcome 2_code_0, ",pts[pts['2_code_0']==1][x].median())

        table.loc[counter, 'name'] = x
        table.loc[counter, 'mean-std'] = str( str(format(pts[x].mean() , ".1f"))+" "+str("("+str(format( pts[x].std() , ".1f"))+")")   ) # pts[x].mean()
        # table.loc[counter, 'std'] = pts[x].std()

        print()
        # df['condition']. value_counts()
        # print("# event:")
        # print(jhs_analysis.loc[jhs_analysis['event']==1,x].describe())
        # print("median", jhs_analysis.loc[jhs_analysis['event']==1,x].median())
        # print("sum", jhs_analysis.loc[jhs_analysis['event']==1,x].sum())
        # print("# not event:")
        # print(jhs_analysis.loc[jhs_analysis['event'] != 1, x].describe())
        # print("median", jhs_analysis.loc[jhs_analysis['event'] != 1, x].median())
        # print("sum", jhs_analysis.loc[jhs_analysis['event'] != 1, x].sum())
        # print('# cat')
        # print(jhs_analysis[x].value_counts())
        # print("# event:")
        # print(jhs_analysis.loc[jhs_analysis['event'] == 1,x].value_counts())
        # print("# not event:")
        # print(jhs_analysis.loc[jhs_analysis['event'] != 1,x].value_counts())
        
    for x in [ 'gender_female', 'gender_male', 'gender_other', 

                'race_asian','race_black_or_african_american',  'race_native_hawaiian_or_other_pacific_islander','race_white',   'race_other',

                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino', 'ethnicity_other',                 

                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 

                'hypertension', 'diabetes', 'MI', 'CHF','PVD','stroke','dementia', 'pulmonary',   'rheumatic',  'liver_mild', 'liversevere','upper_gi_bleed','renal',  
                 'PUD','paralysis','cancer','dmcx','mets' , 'hiv',    

                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 

                '2_code_1','2_code_0'    ]:
        counter+=1
        print(x,end=' ,')
        # print(pts[x].value_counts())
        table.loc[counter, 'name'] = x

        for index, value in pts[x].value_counts().iteritems():
            tot = pts[x].value_counts().sum()
            print(str(index),", ", str(int(value)),end=", ")
            table.loc[counter, str(int(index))] = str(int(value))+" "+str("("+str(format( ((int(value)/int(tot))*100) , ".1f"))+")")
            # table.loc[counter, str(int(index))+" %"] = format( ((int(value)/int(tot))*100) , ".1f")
        print()

        # print("mean, ",pts[x].mean(),end=" ,")
        # print("std, ",pts[x].std(),end=" ,")

        # table.loc[counter, 'name'] = x
        # table.loc[counter, 'mean'] = pts[x].mean()
        # table.loc[counter, 'std'] = pts[x].std()     

        # print("\n\n####### ", x)
        # print('# cat')
        # print(jhs_analysis[x].value_counts())
        # print("# event:")
        # print(jhs_analysis.loc[jhs_analysis['event'] == 1, x].value_counts())
        # print("# not event:")
        # print(jhs_analysis.loc[jhs_analysis['event'] != 1, x].value_counts())

    return(table)
    # print(pts[['2_code_1','2_code_0']+pts_col ].describe())
    # print(pts[['2_code_1','2_code_0']+pts_col ].sum().to_frame().T)
    # return(pts[['2_code_1','2_code_0']+pts_col])

@transform_pandas(
    Output(rid="ri.vector.main.execute.c7a225d6-c76a-4734-812d-714f9f7af4c9")
)

@transform_pandas(
    Output(rid="ri.vector.main.execute.ab2efe37-cbf7-4aa9-9de3-3e3f4b120083"),
    all_treat_new_l1_10_copied_1=Input(rid="ri.vector.main.execute.ae5119fd-18af-42f3-a787-5aed7adc5187"),
    all_treat_new_l3_10_copied_1=Input(rid="ri.vector.main.execute.ba419135-815d-4e22-a1c9-c02c53ecbd94"),
    all_treat_new_l5_10_copied_1=Input(rid="ri.vector.main.execute.9cc7bf8f-0325-4444-b0c8-eb8097984e18"),
    all_treat_new_l7_10_copied_1=Input(rid="ri.vector.main.execute.722be80e-8c67-4255-ba7a-78f7641eb76f"),
    all_treat_new_l9_10_copied_1=Input(rid="ri.vector.main.execute.c1e00dee-e5f0-4247-a688-00a78adec7d2"),
    shap_graph_top20_med=Input(rid="ri.vector.main.execute.e9050241-1998-4d48-a2b1-f597cd7c9796")
)
@output_image_type('svg')
def Join_all_1_copied_copied(all_treat_new_l5_10_copied_1, all_treat_new_l9_10_copied_1, all_treat_new_l7_10_copied_1, all_treat_new_l3_10_copied_1, all_treat_new_l1_10_copied_1, shap_graph_top20_med):
    all_treat_new_l3_10 = all_treat_new_l3_10_copied_1
    all_treat_new_l1_10 = all_treat_new_l1_10_copied_1
    all_treat_new_l7_10 = all_treat_new_l7_10_copied_1
    all_treat_new_l9_10 = all_treat_new_l9_10_copied_1
    all_treat_new_l5_10 = all_treat_new_l5_10_copied_1
    
    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    shap_graph_top20_med.index = ['shap_graph_top20_med']

    all_treat_new_l1_10.index = ['all_treat_new_l1_10']
    all_treat_new_l3_10.index = ['all_treat_new_l3_10']
    all_treat_new_l5_10.index = ['all_treat_new_l5_10']
    all_treat_new_l7_10.index = ['all_treat_new_l7_10']
    all_treat_new_l9_10.index = ['all_treat_new_l9_10']
    
    

    joined_all = pd.concat([shap_graph_top20_med, all_treat_new_l1_10, all_treat_new_l3_10,all_treat_new_l5_10,all_treat_new_l7_10,all_treat_new_l9_10  ], axis=0,sort=False)
    print(joined_all.shape)
    print(joined_all)
    top = 4

    # fig = plt.figure(figsize=(25,7)) # 18 or 28
    fig = plt.figure(figsize=(10,3)) # 18 or 28
    # ax1 = plt.subplot2grid((2,6),(0,0), rowspan = 2,colspan = 1)
    # ax2 = plt.subplot2grid((2,6),(0,1), rowspan = 2,colspan = 1)
    # ax3 = plt.subplot2grid((2,6),(0,2), rowspan = 2,colspan = 1)
    # ax4 = plt.subplot2grid((2,6),(0,3), rowspan = 2,colspan = 1)
    # ax5 = plt.subplot2grid((2,6),(0,4), rowspan = 2,colspan = 1)
    # ax6 = plt.subplot2grid((2,6),(0,5), rowspan = 2,colspan = 1)
    ax1 = plt.subplot2grid((4,3),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((4,3),(0,1), rowspan = 2,colspan = 1)
    ax3 = plt.subplot2grid((4,3),(0,2), rowspan = 2,colspan = 1)
    ax4 = plt.subplot2grid((4,3),(2,0), rowspan = 2,colspan = 1)
    ax5 = plt.subplot2grid((4,3),(2,1), rowspan = 2,colspan = 1)
    ax6 = plt.subplot2grid((4,3),(2,2), rowspan = 2,colspan = 1)
    

    positives = joined_all[joined_all>0].dropna(axis=1, how='all').sort_values(by = 'shap_graph_top20_med', axis = 1, ascending=True,na_position='first')# .sort_values(ascending=True)
    print(positives.shape)
    print(positives.select_dtypes(np.number).gt(0).sum(axis=1))
    
    negatives = joined_all[joined_all<=0].dropna(axis=1, how='all').sort_values(by = 'shap_graph_top20_med', axis = 1, ascending=False,na_position='first') # .sort_values(ascending=False)
    print(negatives.shape)
    print(list(negatives.columns))
    print(negatives.select_dtypes(np.number).le(0).sum(axis=1))

    # positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')

    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, 10 )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, 10 )) # .9 positives.size
    # negatives = negatives.drop('Coag & ViraUnp & Ster & MonoI', axis=1)
    negatives=negatives.iloc[: , -top:]
    df_ax1 = negatives.loc['shap_graph_top20_med',:].plot.barh( ax=ax1,color=[(color_n[int(10-c)]) if  c > 0 else (color_n[9]) for c in negatives.loc['shap_graph_top20_med',:].rank()] ) # yerr=std[:24], ######[new_Col] [new_Col]
    ax1.set_title("Overall")
    df_ax2 = negatives.loc['all_treat_new_l1_10',:].plot.barh( ax=ax2,color= [(color_n[int(10-c)]) if  c > 0 else (color_n[9]) for c in negatives.loc['all_treat_new_l1_10',:].rank()] )
    ax2.set_title("OS level 1")
    df_ax3 = negatives.loc['all_treat_new_l3_10',:].plot.barh( ax=ax3,color=[(color_n[int(10-c)]) if  c > 0 else (color_n[9]) for c in negatives.loc['all_treat_new_l3_10',:].rank()])
    ax3.set_title("OS level 3")
    df_ax4 = negatives.loc['all_treat_new_l5_10',:].plot.barh( ax=ax4,color=[(color_n[int(10-c)]) if  c > 0 else (color_n[9]) for c in negatives.loc['all_treat_new_l5_10',:].rank()])
    ax4.set_title("OS level 5")
    df_ax5 = negatives.loc['all_treat_new_l7_10',:].plot.barh( ax=ax5,color=[(color_n[int(10-c)]) if  c > 0 else (color_n[9]) for c in negatives.loc['all_treat_new_l7_10',:].rank()])
    ax5.set_title("OS level 7")
    print('all_treat_new_l7_10',negatives.loc['all_treat_new_l7_10',:].rank().T.values)
    df_ax6 = negatives.loc['all_treat_new_l9_10',:].plot.barh( ax=ax6,color=[(color_n[int(10-c)]) if  c > 0 else (color_n[9]) for c in negatives.loc['all_treat_new_l9_10',:].rank()])
    ax6.set_title("OS level 9")
    # df_ax6.axes.yaxis.set_ticklabels([]) #.set(yticklabels=[]) worked # 
    # negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]
    # print(" negative.dtype",type(negatives[-top:]))
    axes = [ax1,ax2,ax3,ax4,ax5,ax6]
    df_axes = [df_ax1,df_ax2,df_ax3,df_ax4,df_ax5,df_ax6]
    for x in df_axes:
        y= 0
        # x.axes.yaxis.set_ticklabels([])
        x.set_xlim(0 ,-0.05)
        # x.set_title("shap_graph_top20_med")
        # fig.set_xlabel("Mean Shap value (impact on model output)")
        # fig.set_ylabel("Features")
    
    # plt.xlabel("Mean Shap value (impact on model output)",horizontalalignment='center')
    # plt.ylabel("Features",horizontalalignment='center')
    # fig.tight_layout()

    # ax1.set_xlabel("Mean Shap value (impact on model output)")
    # ax1.set_ylabel("Features")
    fig.text(0.5, 0.01, 'Mean Shap value (impact on model output)', ha='center', fontsize=10)
    fig.text(0.01, 0.5, 'Features', va='center', rotation='vertical', fontsize=10)
    # plt.xlabel("Mean Shap value (impact on model output)")
    # plt.ylabel("Features")
    
    SMALL_SIZE = 8
    MEDIUM_SIZE = 10
    BIGGER_SIZE = 12
    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

    plt.tight_layout()
    plt.show()
    print("show")

    return(joined_all)
    

@transform_pandas(
    Output(rid="ri.vector.main.execute.1de9f28a-7b2f-40c2-8872-8c4464cbb07e"),
    all_treat_new_l1_10_copied_1=Input(rid="ri.vector.main.execute.ae5119fd-18af-42f3-a787-5aed7adc5187"),
    all_treat_new_l3_10_copied_1=Input(rid="ri.vector.main.execute.ba419135-815d-4e22-a1c9-c02c53ecbd94"),
    all_treat_new_l5_10_copied_1=Input(rid="ri.vector.main.execute.9cc7bf8f-0325-4444-b0c8-eb8097984e18"),
    all_treat_new_l7_10_copied_1=Input(rid="ri.vector.main.execute.722be80e-8c67-4255-ba7a-78f7641eb76f"),
    all_treat_new_l9_10_copied_1=Input(rid="ri.vector.main.execute.c1e00dee-e5f0-4247-a688-00a78adec7d2"),
    shap_graph_top20_med=Input(rid="ri.vector.main.execute.e9050241-1998-4d48-a2b1-f597cd7c9796")
)
@output_image_type('svg')
def Join_all_copied_2_copied(all_treat_new_l5_10_copied_1, all_treat_new_l9_10_copied_1, all_treat_new_l7_10_copied_1, all_treat_new_l3_10_copied_1, all_treat_new_l1_10_copied_1, shap_graph_top20_med):
    all_treat_new_l3_10 = all_treat_new_l3_10_copied_1
    all_treat_new_l1_10 = all_treat_new_l1_10_copied_1
    all_treat_new_l7_10 = all_treat_new_l7_10_copied_1
    all_treat_new_l9_10 = all_treat_new_l9_10_copied_1
    all_treat_new_l5_10 = all_treat_new_l5_10_copied_1
    
    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    shap_graph_top20_med.index = ['shap_graph_top20_med']

    all_treat_new_l1_10.index = ['all_treat_new_l1_10']
    all_treat_new_l3_10.index = ['all_treat_new_l3_10']
    all_treat_new_l5_10.index = ['all_treat_new_l5_10']
    all_treat_new_l7_10.index = ['all_treat_new_l7_10']
    all_treat_new_l9_10.index = ['all_treat_new_l9_10']
    
    

    joined_all = pd.concat([shap_graph_top20_med, all_treat_new_l1_10, all_treat_new_l3_10,all_treat_new_l5_10,all_treat_new_l7_10,all_treat_new_l9_10  ], axis=0,sort=False)
    print(joined_all.shape)
    print(joined_all)
    top = 4

    # fig = plt.figure(figsize=(25,7)) # 18 or 28
    fig = plt.figure(figsize=(10,3)) # 18 or 28
    # ax1 = plt.subplot2grid((2,6),(0,0), rowspan = 2,colspan = 1)
    # ax2 = plt.subplot2grid((2,6),(0,1), rowspan = 2,colspan = 1)
    # ax3 = plt.subplot2grid((2,6),(0,2), rowspan = 2,colspan = 1)
    # ax4 = plt.subplot2grid((2,6),(0,3), rowspan = 2,colspan = 1)
    # ax5 = plt.subplot2grid((2,6),(0,4), rowspan = 2,colspan = 1)
    # ax6 = plt.subplot2grid((2,6),(0,5), rowspan = 2,colspan = 1)
    ax1 = plt.subplot2grid((4,3),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((4,3),(0,1), rowspan = 2,colspan = 1)
    ax3 = plt.subplot2grid((4,3),(0,2), rowspan = 2,colspan = 1)
    ax4 = plt.subplot2grid((4,3),(2,0), rowspan = 2,colspan = 1)
    ax5 = plt.subplot2grid((4,3),(2,1), rowspan = 2,colspan = 1)
    ax6 = plt.subplot2grid((4,3),(2,2), rowspan = 2,colspan = 1)
    

    positives = joined_all[joined_all>0].dropna(axis=1, how='all').sort_values(by = 'shap_graph_top20_med', axis = 1, ascending=True,na_position='first')# .sort_values(ascending=True)
    print(positives.shape)
    print(positives.select_dtypes(np.number).gt(0).sum(axis=1))
    
    negatives = joined_all[joined_all<=0].dropna(axis=1, how='all').sort_values(by = 'shap_graph_top20_med', axis = 1, ascending=False,na_position='last') # .sort_values(ascending=False)
    print(negatives.shape)
    print(negatives.select_dtypes(np.number).le(0).sum(axis=1))

    # positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')

    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, 10 )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, 10 )) # .9 positives.size
    print("print(positives)")
    print(positives)
    
    positives=positives.iloc[: , -top:]
    print("rank")
    print(positives.loc['shap_graph_top20_med',:].rank())

    df_ax1 = positives.loc['shap_graph_top20_med',:].plot.barh( ax=ax1,color=[(color_p[int(c+5)]) if  c > 0 else (color_p[9]) for c in positives.loc['shap_graph_top20_med',:].rank()] ) # yerr=std[:24], ######[new_Col] [new_Col] ########### c-1
    ax1.set_title("Overall") 
    df_ax2 = positives.loc['all_treat_new_l1_10',:].plot.barh( ax=ax2,color= [(color_p[int(c+5)]) if  c > 0 else (color_p[9]) for c in positives.loc['all_treat_new_l1_10',:].rank()] )
    ax2.set_title("OS level 1")
    df_ax3 = positives.loc['all_treat_new_l3_10',:].plot.barh( ax=ax3,color=[(color_p[int(c+5)]) if  c > 0 else (color_p[9]) for c in positives.loc['all_treat_new_l3_10',:].rank()])
    ax3.set_title("OS level 3")
    df_ax4 = positives.loc['all_treat_new_l5_10',:].plot.barh( ax=ax4,color=[(color_p[int(c+5)]) if  c > 0 else (color_p[9]) for c in positives.loc['all_treat_new_l5_10',:].rank()])
    ax4.set_title("OS level 5")
    df_ax5 = positives.loc['all_treat_new_l7_10',:].plot.barh( ax=ax5,color=[(color_p[int(c+5)]) if  c > 0 else (color_p[9]) for c in positives.loc['all_treat_new_l7_10',:].rank()])
    ax5.set_title("OS level 7")
    print('all_treat_new_l7_10',positives.loc['all_treat_new_l7_10',:].rank().T.values)
    df_ax6 = positives.loc['all_treat_new_l9_10',:].plot.barh( ax=ax6,color=[(color_p[int(c+5)]) if  c > 0 else (color_p[9]) for c in positives.loc['all_treat_new_l9_10',:].rank()])
    ax6.set_title("OS level 9")
    # df_ax6.axes.yaxis.set_ticklabels([]) #.set(yticklabels=[]) worked # 
    # negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]
    # print(" negative.dtype",type(negatives[-top:]))
    axes = [ax1,ax2,ax3,ax4,ax5,ax6]
    df_axes = [df_ax1,df_ax2,df_ax3,df_ax4,df_ax5,df_ax6]
    for x in df_axes[1:]:
        y= 0
        # x.axes.yaxis.set_ticklabels([])
        # x.set_xlim(positives.loc['shap_graph_top20_med',:].min()* 1.2,positives.loc['shap_graph_top20_med',:].max()* 1.10 )
        # x.set_title("shap_graph_top20_med")
        # fig.set_xlabel("Mean Shap value (impact on model output)")
        # fig.set_ylabel("Features")
    
    # plt.xlabel("Mean Shap value (impact on model output)",horizontalalignment='center')
    # plt.ylabel("Features",horizontalalignment='center')
    # fig.tight_layout()

    # ax1.set_xlabel("Mean Shap value (impact on model output)")
    # ax1.set_ylabel("Features")
    fig.text(0.5, 0.01, 'Mean Shap value (impact on model output)', ha='center', fontsize=10)
    fig.text(0.01, 0.5, 'Features', va='center', rotation='vertical', fontsize=10)
    # plt.xlabel("Mean Shap value (impact on model output)")
    # plt.ylabel("Features")
    
    SMALL_SIZE = 8
    MEDIUM_SIZE = 10
    BIGGER_SIZE = 12
    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

    plt.tight_layout()
    plt.show()
    print("show")

    return(joined_all)
    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.9d0c0395-cfa5-4b15-9d02-7bd9d9ddf0c5"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
@output_image_type('svg')
def ROC_CURVE_10fold(derived_cobination):
    derived_cobination = derived_cobination

    # https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html

    
    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    # from numpy import expand_dims
    # # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    # from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    from sklearn.model_selection import StratifiedKFold
    from sklearn.model_selection import KFold
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                'pressor_days',
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    tprs = []
    aucs = []
    f1= []
    rec= []
    per=[]
    c1=[]
    c0=[] #
    f101=[] #.append(final_results['f101'])
    rec01=[] #.append(final_results['rec01'])
    per01=[] #.append(final_results['pre01'])
    mean_fpr = np.linspace(0, 1, 100)
    fig, ax = plt.subplots(figsize=(5,5), dpi=600)
    # fig = plt.figure(figsize=(5,5))
    ### SPLIT
    print('\nsplit')
    counter_fold = 0
    # kf = KFold(n_splits=20)
    pts=pts.sample(frac=1)

    skf = StratifiedKFold(n_splits=5, random_state=9) #1000 #, random_state=7,
    # skf.split(X, y):
    for train_index, test_index in skf.split(pts[pts_col ],pts[outcome]):
        counter_fold+=1
        print("\n\n\n#####################################\n FOLD ",counter_fold,"\n#################################\n")
        # x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
        x_train= pts.iloc[train_index][pts_col ]
        x_test= pts.iloc[test_index][pts_col ]
        y_train= pts.iloc[train_index][outcome ]
        y_test= pts.iloc[test_index][outcome ]
        
        # y_train = pd.DataFrame(data=y_train,columns=outcome)
        # x_train = pd.DataFrame(data=x_train,columns=pts_col)

        print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
        print('onehot y_train')
        print(onehot.sum(axis=0))
    
        ### Class weight
        print("\nClass weight")
        num_classes = len(pts[outcome[0]].unique())
        if model_type == 'binary' or model_type == 'regression':
            num_classes = 1
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
        class_counts = onehot.sum(axis=0).values
        total_count = sum(class_counts)
        class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
        class_weights = dict(enumerate(class_rate))
        print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

        print("\nscale")
        scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
        #  'age',
        print(scale) 
        scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
        x_train.loc[:,scale] = scale_x.transform(x_train[scale])
        x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
        
        original_x_train = x_train.copy()
        original_y_train = y_train.copy()
        original_x_test = x_test.copy()
        original_y_test = y_test.copy()
        ### SMOTE
        if resample:
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7) 
            x_train, y_train = sm.fit_sample(x_train, y_train)
            print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
            x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
            y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
            for c in range(num_classes):
                class_weights[c]=1
            print('class weights reseted: ',class_weights)
        
        ### split learn/val
        print('\nsplit learn/val')
        x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
        print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

        print('\nonehot train/test/learn/val')
        if model_type == 'classification' :
            y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_train.sum(axis=0))
            y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_test.sum(axis=0))
            y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_learn.sum(axis=0))
            y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_val.sum(axis=0))

        ### Hyper Param 
        ######################################
        param_grid = []
        if optimize == True:
            neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
            layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
            drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
            batchNorm = hp.choice( 'batchNorm',[True, False] )
            activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
            learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
            epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
            batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

            param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                    drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                    learning_ratio=learning_ratio, # compile
                    epochs=epochs, batch_size=batch_size ) # fit           

            if model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
    
        ### Functions ### 
        ######################################
        print("Model_name full")
        def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            print('TEST__ param_grid:', param_grid)

            # Model
            neuron_comb =  param_grid['neuron_comb']
            layers_comb = param_grid['layers_comb']
            drop_rate =  param_grid['drop_rate'] 
            batchNorm = param_grid['batchNorm'] 
            activation_out = param_grid['activation_out']
            # compile
            learning_ratio = param_grid['learning_ratio']
            # fit
            epochs = param_grid['epochs']
            batch_size= param_grid['batch_size']
        
            ## model__fn
            inputA = Input(shape=(demo_shape,))
            if final_model:
                print('\nFinal_model inputA.shape',inputA.shape)
    
            attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
            inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
            
            fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
            fully = Dropout(drop_rate,name="Drop-1")(fully)
            if batchNorm:
                fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

            for l in range(2,layers_comb+1,1):
                    fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                    fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                    if batchNorm:
                        fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

            out= []
            if model_type != 'regression':
                out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
                # sigmoid softmax tanh relu PReLU Leaky ReLU 
            else:
                out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
                # linear' bias_initializer="zeros",
                # kernel_regularizer=None,
                # bias_regularizer=None,
                # activity_regularizer=None,
            model = Model(inputs=inputA, outputs=out)

            if final_model:
                model.summary()

            loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
            metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
            if model_type == 'binary':
                loss = 'binary_crossentropy'
            if model_type == 'regression':
                loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
                metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
            model.compile(loss = loss,
                        metrics=metrics,
                        optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

            es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
            if model_type == 'regression':
                es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
                mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

            history = []
            if model_type == 'classification':
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values), 
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc],
                                    class_weight=class_weights
                                    )
                model = load_model('best_model.h5')
            else:
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values),
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc]
                                )
                model = load_model('best_model.h5')

            pred_prob = model.predict( x_val[pts_col].values)
            if np.isnan(pred_prob).any():
                print("ERROR np.nan_to_num(pred_prob)")
                pred_prob = np.nan_to_num(pred_prob)
            predictions = pred_prob.argmax(axis=1)
            if model_type == 'binary':
                fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
                gmeans = np.sqrt(tpr * (1-fpr))
                ix = np.argmax(gmeans)
                print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
                predictions = (pred_prob >thresholds[ix]).astype(np.int) 
            if model_type == 'regression':
                predictions = model.predict( x_val[pts_col].values)
            y_val_binary = y_val.copy()
            if model_type == 'classification':
                y_val = y_val.values.argmax(axis=1)
        
            ### for ploting send out
            print('history.history.keys()',history.history.keys())
            train_acc=[]
            val_acc=[]
            if model_type == 'classification' or model_type == 'binary':
                train_acc = history.history['auc'] 
                val_acc = history.history['val_auc']
            elif model_type == 'regression':
                train_acc = history.history['mape_metric']
                val_acc = history.history['val_mape_metric']
            else:
                print("Error model type")
                return(0)
            train_loss = history.history['loss']
            val_loss = history.history['val_loss']
            epochs = range(len(train_acc))
        
            if model_type == 'regression' :
                y_val = y_val_binary
                actual, pred = np.array(y_val), np.array(predictions)
                mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
                mape_acc = 100 - mape_error 
                results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                            'accuracy_score_val': mape_acc, 
                            'train_acc':train_acc, 'val_acc':val_acc, 
                            'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                        }
                print("TEST__ accuracy_score:mape_acc ", mape_acc)
                print("datetime.now()",datetime.now())
                return (results)

            if model_type == 'binary' or model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)
                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
            
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    print('TEST__ confusion:',class_str,class_accuracy)
    

                results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
        ######################################
        print("Model_name rf")
        def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            
            ## model__rf
            # model = RandomForestClassifier(random_state=7)
            model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000) #1000 __trees __ft
            # model = sk.neighbors.KNeighborsClassifier()
            # model = sk.svm.SVC()

            model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
            predictions = model.predict(x_val[pts_col].values)
            pred_prob = model.predict_proba(x_val[pts_col].values)
            y_val_binary = y_val
            y_val = y_val.values.argmax(axis=1)
            
            ### for ploting send out
            train_acc=0
            val_acc=0
            train_loss = 0
            val_loss = 0
            epochs = 0
        
            if model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)

                f101 = f1_score(y_val, predictions, average=None)
                pre01 = precision_score(y_val,predictions, average=None)
                rec01 = recall_score(y_val, predictions, average=None)

                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
                class_1= 0
                class_0 = 0
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    if class_ == 0:  class_0 = class_accuracy
                    if class_ == 1:  class_1 = class_accuracy
                    print('TEST__ confusion:',class_str,class_accuracy)
    

                results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val,  'class_0':class_0, "class_1":class_1, "f101":f101, "rec01":rec01, "pre01":pre01,
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
            else:
                print("\nwrong model type")
                return(0)
        ######################################
        print("Results #############################")
        ### trials 
        # tpe_trials = Trials()
        tpe_best = []
        if optimize:
            if model_name == 'full':
                tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
            elif model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
            else:
                print("Error optimize: model not found")
                return(0)
            param_grid= space_eval(param_grid, tpe_best)
            print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
            print("datetime.now(): ", datetime.now())
        else: # __param
            if model_name == 'full':
                param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
                if max_level == 3:
                    param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
                elif max_level == 5:
                    param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
                elif max_level == 5:
                    param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
                elif max_level == 9:
                    param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

                if model_type == 'binary':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
                if model_type == 'regression':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif model_name == 'rf':
                print("rf not optimize")
            else:
                print("Error optimize: model not found")
                return(0)

        print("\nFinal Model")
        final_results = []
        final_model =  True
        # __call
        if model_name == 'full':
            final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
        if model_name == 'rf':
            final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
        
        ## __plot
        if True: 
            # fig = plt.figure(figsize=(5,5)) 
            # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
            # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
            # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

            # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
            # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
            # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
            # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
            # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
            # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

            #  f1= []
            # rec= []
            # per=[]
            # 'f1_score_val': f1_score_val, 
            # 'precision_score_val': precision_score_val, 
            # 'recall_score_val': recall_score_val,

            f101.append(final_results['f101'])
            rec01.append(final_results['rec01'])
            per01.append(final_results['pre01'])
            
            f1.append(final_results['f1_score_val'])
            rec.append(final_results['recall_score_val'])
            per.append(final_results['precision_score_val'])
            c1.append(final_results['class_1'])
            c0.append(final_results['class_0'])

            fpr, tpr, threshold = final_results['roc_curve_val']
            roc_auc = auc(fpr, tpr)
            # ax1.set_title('Receiver Operating Characteristic')
            # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
            # ax1.legend(loc = 'lower right')
            # ax1.plot([0, 1], [0, 1],'r--')
            # ax1.set_xlim([0, 1])
            # ax1.set_ylim([0, 1])
            # ax1.set_ylabel('True Positive Rate')
            # ax1.set_xlabel('False Positive Rate')

            interp_tpr = np.interp(mean_fpr, fpr, tpr)
            interp_tpr[0] = 0.0
            tprs.append(interp_tpr)
            aucs.append(roc_auc)

        break
    # __ft
    
    ax.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)

    print("f1:",np.mean(f1))
    print("per:",np.mean(per))
    print("rec:",np.mean(rec))

    print("f101:",np.mean(f101, axis=0))
    print("per01:",np.mean(per01, axis=0))
    print("rec01:",np.mean(rec01, axis=0))

    print("class_1",np.mean(c1))
    print("class_0",np.mean(c0))
    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs)
    print("std_auc: ",std_auc)
    ax.plot( #1000
        mean_fpr,
        mean_tpr,
        color="b",
        label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, 0.01), # std_auc 
        # label=r"Mean ROC (AUC = %0.2f)" % (mean_auc),

        lw=2,
        alpha=0.8,
    )
    count_ratio = 0
    colors_sns = sns.color_palette("tab10")
    for t in tprs:
        count_ratio+=1
        ax.plot(
        mean_fpr,
        t,
        color=colors_sns[count_ratio-1],
        # label=r"Fold %f (AUC = %0.2f $\pm$ %0.2f)" % (count_ratio,mean_auc, std_auc),
        # label=r"Fold %f (AUC = %0.2f $\pm$ %0.2f)" % (count_ratio,mean_auc, std_auc),
        lw=1,
        alpha=0.2,
        )

    std_tpr = np.std(tprs, axis=0)
    tprs_upper = np.minimum(mean_tpr + std_tpr*1.5, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr*1.5, 0)
    ax.fill_between(
        mean_fpr,
        tprs_lower,
        tprs_upper,
        color="grey",
        alpha=0.4,
        label=r"$\pm$ 1 std. dev.",
    )

    ax.set(
        xlim=[-0.05, 1.05],
        ylim=[-0.05, 1.05],
        title="Receiver Operating Characteristic (ROC)",
    )
    ax.legend(loc="lower right")
    ax.set(xlabel='False Positive Rate')
    ax.set(ylabel='True Positive Rate')

    SMALL_SIZE = 8
    MEDIUM_SIZE = 10
    BIGGER_SIZE = 12

    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

    plt.show()

    print("after KFold for : datetime.now(): ", datetime.now())
    # return( pd.DataFrame( {'return' : [0]} ) ) 
    return(pd.DataFrame({'mean_tpr': mean_tpr, 'mean_fpr': mean_fpr}) )

@transform_pandas(
    Output(rid="ri.vector.main.execute.efea65c8-a928-414e-a0e2-875be917f6bd"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
@output_image_type('svg')
def ROC_CURVE_10fold_1(derived_cobination):
    derived_cobination = derived_cobination

    # https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html

    
    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    # from numpy import expand_dims
    # # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    # from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    from sklearn.model_selection import StratifiedKFold
    from sklearn.model_selection import KFold
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                'pressor_days',
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    tprs = []
    aucs = []
    f1= []
    rec= []
    per=[]
    c1=[]
    c0=[] #
    f101=[] #.append(final_results['f101'])
    rec01=[] #.append(final_results['rec01'])
    per01=[] #.append(final_results['pre01'])
    mean_fpr = np.linspace(0, 1, 100)
    fig, ax = plt.subplots(figsize=(5,5), dpi=600)
    # fig = plt.figure(figsize=(5,5))
    ### SPLIT
    print('\nsplit')
    counter_fold = 0
    # kf = KFold(n_splits=20)
    pts=pts.sample(frac=1)

    skf = StratifiedKFold(n_splits=5, random_state=9) #1000 #, random_state=7,
    # skf.split(X, y):
    for train_index, test_index in skf.split(pts[pts_col ],pts[outcome]):
        counter_fold+=1
        print("\n\n\n#####################################\n FOLD ",counter_fold,"\n#################################\n")
        # x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
        x_train= pts.iloc[train_index][pts_col ]
        x_test= pts.iloc[test_index][pts_col ]
        y_train= pts.iloc[train_index][outcome ]
        y_test= pts.iloc[test_index][outcome ]
        
        # y_train = pd.DataFrame(data=y_train,columns=outcome)
        # x_train = pd.DataFrame(data=x_train,columns=pts_col)

        print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
        print('onehot y_train')
        print(onehot.sum(axis=0))
    
        ### Class weight
        print("\nClass weight")
        num_classes = len(pts[outcome[0]].unique())
        if model_type == 'binary' or model_type == 'regression':
            num_classes = 1
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
        class_counts = onehot.sum(axis=0).values
        total_count = sum(class_counts)
        class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
        class_weights = dict(enumerate(class_rate))
        print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

        print("\nscale")
        scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
        #  'age',
        print(scale) 
        scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
        x_train.loc[:,scale] = scale_x.transform(x_train[scale])
        x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
        
        original_x_train = x_train.copy()
        original_y_train = y_train.copy()
        original_x_test = x_test.copy()
        original_y_test = y_test.copy()
        ### SMOTE
        if resample:
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7) 
            x_train, y_train = sm.fit_sample(x_train, y_train)
            print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
            x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
            y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
            for c in range(num_classes):
                class_weights[c]=1
            print('class weights reseted: ',class_weights)
        
        ### split learn/val
        print('\nsplit learn/val')
        x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
        print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

        print('\nonehot train/test/learn/val')
        if model_type == 'classification' :
            y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_train.sum(axis=0))
            y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_test.sum(axis=0))
            y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_learn.sum(axis=0))
            y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_val.sum(axis=0))

        ### Hyper Param 
        ######################################
        param_grid = []
        if optimize == True:
            neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
            layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
            drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
            batchNorm = hp.choice( 'batchNorm',[True, False] )
            activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
            learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
            epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
            batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

            param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                    drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                    learning_ratio=learning_ratio, # compile
                    epochs=epochs, batch_size=batch_size ) # fit           

            if model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
    
        ### Functions ### 
        ######################################
        print("Model_name full")
        def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            print('TEST__ param_grid:', param_grid)

            # Model
            neuron_comb =  param_grid['neuron_comb']
            layers_comb = param_grid['layers_comb']
            drop_rate =  param_grid['drop_rate'] 
            batchNorm = param_grid['batchNorm'] 
            activation_out = param_grid['activation_out']
            # compile
            learning_ratio = param_grid['learning_ratio']
            # fit
            epochs = param_grid['epochs']
            batch_size= param_grid['batch_size']
        
            ## model__fn
            inputA = Input(shape=(demo_shape,))
            if final_model:
                print('\nFinal_model inputA.shape',inputA.shape)
    
            attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
            inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
            
            fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
            fully = Dropout(drop_rate,name="Drop-1")(fully)
            if batchNorm:
                fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

            for l in range(2,layers_comb+1,1):
                    fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                    fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                    if batchNorm:
                        fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

            out= []
            if model_type != 'regression':
                out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
                # sigmoid softmax tanh relu PReLU Leaky ReLU 
            else:
                out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
                # linear' bias_initializer="zeros",
                # kernel_regularizer=None,
                # bias_regularizer=None,
                # activity_regularizer=None,
            model = Model(inputs=inputA, outputs=out)

            if final_model:
                model.summary()

            loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
            metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
            if model_type == 'binary':
                loss = 'binary_crossentropy'
            if model_type == 'regression':
                loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
                metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
            model.compile(loss = loss,
                        metrics=metrics,
                        optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

            es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
            if model_type == 'regression':
                es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
                mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

            history = []
            if model_type == 'classification':
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values), 
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc],
                                    class_weight=class_weights
                                    )
                model = load_model('best_model.h5')
            else:
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values),
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc]
                                )
                model = load_model('best_model.h5')

            pred_prob = model.predict( x_val[pts_col].values)
            if np.isnan(pred_prob).any():
                print("ERROR np.nan_to_num(pred_prob)")
                pred_prob = np.nan_to_num(pred_prob)
            predictions = pred_prob.argmax(axis=1)
            if model_type == 'binary':
                fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
                gmeans = np.sqrt(tpr * (1-fpr))
                ix = np.argmax(gmeans)
                print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
                predictions = (pred_prob >thresholds[ix]).astype(np.int) 
            if model_type == 'regression':
                predictions = model.predict( x_val[pts_col].values)
            y_val_binary = y_val.copy()
            if model_type == 'classification':
                y_val = y_val.values.argmax(axis=1)
        
            ### for ploting send out
            print('history.history.keys()',history.history.keys())
            train_acc=[]
            val_acc=[]
            if model_type == 'classification' or model_type == 'binary':
                train_acc = history.history['auc'] 
                val_acc = history.history['val_auc']
            elif model_type == 'regression':
                train_acc = history.history['mape_metric']
                val_acc = history.history['val_mape_metric']
            else:
                print("Error model type")
                return(0)
            train_loss = history.history['loss']
            val_loss = history.history['val_loss']
            epochs = range(len(train_acc))
        
            if model_type == 'regression' :
                y_val = y_val_binary
                actual, pred = np.array(y_val), np.array(predictions)
                mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
                mape_acc = 100 - mape_error 
                results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                            'accuracy_score_val': mape_acc, 
                            'train_acc':train_acc, 'val_acc':val_acc, 
                            'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                        }
                print("TEST__ accuracy_score:mape_acc ", mape_acc)
                print("datetime.now()",datetime.now())
                return (results)

            if model_type == 'binary' or model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)
                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
            
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    print('TEST__ confusion:',class_str,class_accuracy)
    

                results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
        ######################################
        print("Model_name rf")
        def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            
            ## model__rf
            # model = RandomForestClassifier(random_state=7)
            model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000) #1000 __trees __ft
            # model = sk.neighbors.KNeighborsClassifier()
            # model = sk.svm.SVC()

            model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
            predictions = model.predict(x_val[pts_col].values)
            pred_prob = model.predict_proba(x_val[pts_col].values)
            y_val_binary = y_val
            y_val = y_val.values.argmax(axis=1)
            
            ### for ploting send out
            train_acc=0
            val_acc=0
            train_loss = 0
            val_loss = 0
            epochs = 0
        
            if model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)

                f101 = f1_score(y_val, predictions, average=None)
                pre01 = precision_score(y_val,predictions, average=None)
                rec01 = recall_score(y_val, predictions, average=None)

                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
                class_1= 0
                class_0 = 0
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    if class_ == 0:  class_0 = class_accuracy
                    if class_ == 1:  class_1 = class_accuracy
                    print('TEST__ confusion:',class_str,class_accuracy)
    

                results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val,  'class_0':class_0, "class_1":class_1, "f101":f101, "rec01":rec01, "pre01":pre01,
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
            else:
                print("\nwrong model type")
                return(0)
        ######################################
        print("Results #############################")
        ### trials 
        # tpe_trials = Trials()
        tpe_best = []
        if optimize:
            if model_name == 'full':
                tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
            elif model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
            else:
                print("Error optimize: model not found")
                return(0)
            param_grid= space_eval(param_grid, tpe_best)
            print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
            print("datetime.now(): ", datetime.now())
        else: # __param
            if model_name == 'full':
                param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
                if max_level == 3:
                    param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
                elif max_level == 5:
                    param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
                elif max_level == 5:
                    param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
                elif max_level == 9:
                    param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

                if model_type == 'binary':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
                if model_type == 'regression':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif model_name == 'rf':
                print("rf not optimize")
            else:
                print("Error optimize: model not found")
                return(0)

        print("\nFinal Model")
        final_results = []
        final_model =  True
        # __call
        if model_name == 'full':
            final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
        if model_name == 'rf':
            final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
        
        ## __plot
        if True: 
            # fig = plt.figure(figsize=(5,5)) 
            # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
            # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
            # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

            # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
            # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
            # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
            # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
            # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
            # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

            #  f1= []
            # rec= []
            # per=[]
            # 'f1_score_val': f1_score_val, 
            # 'precision_score_val': precision_score_val, 
            # 'recall_score_val': recall_score_val,

            f101.append(final_results['f101'])
            rec01.append(final_results['rec01'])
            per01.append(final_results['pre01'])
            
            f1.append(final_results['f1_score_val'])
            rec.append(final_results['recall_score_val'])
            per.append(final_results['precision_score_val'])
            c1.append(final_results['class_1'])
            c0.append(final_results['class_0'])

            fpr, tpr, threshold = final_results['roc_curve_val']
            roc_auc = auc(fpr, tpr)
            # ax1.set_title('Receiver Operating Characteristic')
            # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
            # ax1.legend(loc = 'lower right')
            # ax1.plot([0, 1], [0, 1],'r--')
            # ax1.set_xlim([0, 1])
            # ax1.set_ylim([0, 1])
            # ax1.set_ylabel('True Positive Rate')
            # ax1.set_xlabel('False Positive Rate')

            interp_tpr = np.interp(mean_fpr, fpr, tpr)
            interp_tpr[0] = 0.0
            tprs.append(interp_tpr)
            aucs.append(roc_auc)

        break
    # __ft
    
    ax.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)

    print("f1:",np.mean(f1))
    print("per:",np.mean(per))
    print("rec:",np.mean(rec))

    print("f101:",np.mean(f101, axis=0))
    print("per01:",np.mean(per01, axis=0))
    print("rec01:",np.mean(rec01, axis=0))

    print("class_1",np.mean(c1))
    print("class_0",np.mean(c0))
    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs)
    print("std_auc: ",std_auc)
    ax.plot( #1000
        mean_fpr,
        mean_tpr,
        color="b",
        label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, 0.01), # std_auc 
        # label=r"Mean ROC (AUC = %0.2f)" % (mean_auc),

        lw=2,
        alpha=0.8,
    )
    count_ratio = 0
    colors_sns = sns.color_palette("tab10")
    for t in tprs:
        count_ratio+=1
        ax.plot(
        mean_fpr,
        t,
        color=colors_sns[count_ratio-1],
        # label=r"Fold %f (AUC = %0.2f $\pm$ %0.2f)" % (count_ratio,mean_auc, std_auc),
        # label=r"Fold %f (AUC = %0.2f $\pm$ %0.2f)" % (count_ratio,mean_auc, std_auc),
        lw=1,
        alpha=0.2,
        )

    std_tpr = np.std(tprs, axis=0)
    tprs_upper = np.minimum(mean_tpr + std_tpr*1.5, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr*1.5, 0)
    ax.fill_between(
        mean_fpr,
        tprs_lower,
        tprs_upper,
        color="grey",
        alpha=0.4,
        label=r"$\pm$ 1 std. dev.",
    )

    ax.set(
        xlim=[-0.05, 1.05],
        ylim=[-0.05, 1.05],
        title="Receiver Operating Characteristic (ROC)",
    )
    ax.legend(loc="lower right")
    ax.set(xlabel='False Positive Rate')
    ax.set(ylabel='True Positive Rate')

    SMALL_SIZE = 8
    MEDIUM_SIZE = 10
    BIGGER_SIZE = 12

    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

    plt.show()

    print("after KFold for : datetime.now(): ", datetime.now())
    # return( pd.DataFrame( {'return' : [0]} ) ) 
    return(pd.DataFrame({'mean_tpr': mean_tpr, 'mean_fpr': mean_fpr}) )

@transform_pandas(
    Output(rid="ri.vector.main.execute.698e89e3-6eda-46b9-b9ce-52662c9ec538"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def age(derived_cobination):
    derived_cobination = derived_cobination

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    import seaborn as sns
    
    df = derived_cobination
 
    fig = plt.figure(figsize=(15,15)) 
    ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
    ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
    ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)
    
    Col =  'age'
    bin_size =40

    # title='Histogram Of Test Scores',
    # rot=45,
    # grid=True,

    df.age.plot.hist( bins=bin_size, ax=ax0) #column=Col,age
    # sns.displot(data=df,x =Col,ax=ax0).set(title=Col+' Distribution')
    
    df_cp =  df[ (df['os_day28']== 1) ]
    df_cp.age.plot.hist( bins=bin_size, ax=ax1)
    # sns.displot(data=df_cp,x =Col,ax=ax1).set(title=Col+' Survived') 

    df_cp =  df[ (df['os_day28']== 11) ]
    df_cp.age.plot.hist( bins=bin_size, ax=ax2)
    # sns.barplot(data=df_cp,x =Col,ax=ax2).set(title=Col+' Death') 

    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")

    # plt.xlabel('Test Score')
    # plt.ylabel("Number Of Students");

    plt.xticks(rotation = 90)
    plt.tight_layout()
    plt.show()

    return(pd.DataFrame() )

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def all_treat_new(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                'pressor_days',
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.989eaefc-0cfb-4d56-9d98-32ecb6c76f05"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def all_treat_new_1(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]

    ########################################################d

    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                'pressor_days',
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    

    #######################################################d
    ### 'time_to_first_day'

    def time_to_outcome_func(row):
        
        last_change = np.nan
        for day in os_col:
            if row[day]!=1:
                last_change = int(day.split('day')[1])+1
                break
        return(last_change)
    pts['time_to_first_day'] = pts.apply(lambda row : time_to_outcome_func(row), axis=1)

    print(".median(): ",pts['time_to_first_day'].median())

    return(pd.DataFrame({0:"returned"}))
    ########################################################

    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.d46b1d80-5515-4f36-92a2-a60ba3a06e98"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def all_treat_new_CNN(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    # # hyperopt: added to env.
    # # Shap
    # # Lime
    # # seaborn
    # # ADD ? 
    # # imbalance-learn
    # # seaborn: added to env.
    # # Keras: added to env.
    # # sklearn-pandas
    # # keras, keras-vis removed
    # # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    # import time
    # import numpy as np
    # import pandas as pd
    # import matplotlib as mpl
    # import matplotlib.cm as cm
    # from matplotlib import pyplot as plt
    # import shap
    # # from vis.visualization import visualize_cam, overlay
    # # from vis.utils import utils
    # # import torch
    # # from tab_transformer_pytorch import TabTransformer
    # import seaborn as sns
    # from os import path
    # import sklearn as sk
    # from sklearn.model_selection import train_test_split
    # from sklearn.linear_model import LogisticRegression
    # from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    # from sklearn.svm import SVC
    # from sklearn.naive_bayes import GaussianNB
    # from sklearn.neighbors import KNeighborsClassifier
    # from sklearn.ensemble import RandomForestClassifier
    # from sklearn.neural_network import MLPClassifier
    # from sklearn.metrics import roc_auc_score, roc_curve
    # from sklearn.metrics import f1_score
    # from sklearn.metrics import precision_score
    # from sklearn.metrics import recall_score
    # from sklearn.metrics import mean_squared_error
    # from sklearn.metrics import mean_absolute_error
    # # from sklearn.metrics import mean_absolute_percentage_error
    # from sklearn.utils import class_weight
    # import imblearn 
    # from imblearn.over_sampling import SMOTE ####
    # from imblearn.under_sampling import RandomUnderSampler
    # from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    # from sklearn.inspection import permutation_importance
    # from sklearn.feature_selection import RFE
    # from sklearn.neighbors import LocalOutlierFactor
    # from sklearn import preprocessing
    # from sklearn.preprocessing import StandardScaler, MinMaxScaler
    # from sklearn import model_selection
    # from sklearn.utils import shuffle
    # # from sklearn_pandas import DataFrameMapper
    # import random 
    # # import tensorflow as tf
    # # from tensorflow import keras
    # # from tensorflow.keras import backend as K
    # # print(K.image_data_format())
    # # from tensorflow.keras import models
    # # from tensorflow.keras import layers
    # # from tensorflow.keras import activations
    # # from tensorflow.keras.models import Model
    # # from tensorflow.keras.models import Sequential
    # # from tensorflow.keras.layers import BatchNormalization
    # # from tensorflow.keras.layers import AveragePooling2D
    # # from tensorflow.keras.layers import MaxPooling2D
    # # from tensorflow.keras.layers import AveragePooling1D
    # # from tensorflow.keras.layers import MaxPooling1D
    # # from tensorflow.keras.layers import Conv2D
    # # from tensorflow.keras.layers import Conv1D
    # # from tensorflow.keras.layers import Activation
    # # from tensorflow.keras.layers import Dropout
    # # from tensorflow.keras.layers import Flatten
    # # from tensorflow.keras.layers import Input
    # # from tensorflow.keras.layers import Dense
    # # from tensorflow.keras.layers import concatenate
    # # from tensorflow.keras.layers import Attention
    # # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # # from keras.models import load_model
    # # from tensorflow.keras.models import load_model
    # from numpy import expand_dims
    # # import tensorflow_datasets as tensorflow_datasets
    # # from tensorflow.keras.preprocessing import image
    # # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # # from __future__ import absolute_import, division, print_function, unicode_literals ###
    # import matplotlib.image as mpimg
    # from scipy import misc 
    # from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    # np.random.seed(7)
    # # tf.random.set_seed(12) 
    # from datetime import datetime
    # start = datetime.now()
    # seconds_in_day = 24 * 60 * 60
    # print("datetime.now()",datetime.now())
    # print("Numpy ",np.version.version)
    # print("Sklearn ",sk.__version__)
    # # print("Keras ", keras.__version__)
    # # print("TF ",tf.__version__)
    # # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # # else: print('    CPU version')
    # pd.set_option('display.max_columns', None)
    # pd.set_option('display.width', None)
    # from sklearn.exceptions import ConvergenceWarning
    # import warnings
    # warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # # warnings.filterwarnings('ignore')
    # print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    model_name = 'full'
    # model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = False
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize = False
    max_eval = 30
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    pts =  pts[   (pts['os_day1']== 1)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
    print('columns:',str(list(pts.columns)))
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        try:
            pts = pts.drop([col+'_other'], axis=1)
        except:
            print("except no ",col,"with ---others---" )
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                # 'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full cnn")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        print("Conv1D")
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,1))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape,demo_shape)
        print('\nFinal_model inputA.shape',inputA.shape,demo_shape)
 
        ############################################################################################
        num_filters = 128
        conv_num = "1x7-base"
        conv = Conv1D(num_filters , kernel_size=7, strides=1, padding="same", name=("Conv-" + conv_num))(inputA)  # valid # orig (1,3) # larger (5,5) (3,3)
        if batchNorm: conv = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
        act = Activation("relu", name=("Act-" + conv_num))(conv)
        base = MaxPooling1D(pool_size=2, name=("Pooling-" + conv_num))(act)  # MaxPooling1D

        conv_num = "1x3-1-64"
        conv = Conv1D(num_filters, kernel_size=3, strides=1, padding="same", name=("Conv-" + conv_num))(base)
        if batchNorm: conv = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
        act = Activation("relu", name=("Act-" + conv_num))(conv)

        conv_num = "1x3-2-64"
        conv = Conv1D(num_filters, kernel_size=3, strides=1, padding="same", name=("Conv-" + conv_num))(act)
        if batchNorm: conv = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
        act = Activation("relu", name=("Act-" + conv_num))(conv)
        block = keras.layers.Add(name="add-"+conv_num)([base,act])

        conv_num = "64to128"
        feature_increase = Conv1D(num_filters * 2, kernel_size=1, strides=2, padding="same", name=("Conv-" + conv_num))(block)

        conv_num = "1x3-1-128"
        conv = Conv1D(num_filters*2, kernel_size=3, strides=2, padding="same", name=("Conv-" + conv_num))(block)
        if batchNorm: conv  = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
        act = Activation("relu", name=("Act-" + conv_num))(conv)

        conv_num = "1x3-2-128"
        conv = Conv1D(num_filters*2, kernel_size=3, strides=1, padding="same", name=("Conv-" + conv_num))(act)
        if batchNorm: conv  = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
        act = Activation("relu", name=("Act-" + conv_num))(conv)
        block = keras.layers.Add(name="add-"+conv_num)([feature_increase, act])

        conv_num = "128to256"
        feature_increase = Conv1D(num_filters * 2*2, kernel_size=1, strides=2, padding="same", name=("Conv-" + conv_num))(block)

        conv_num = "3x3-1-256"
        conv = Conv1D(num_filters * 2* 2, kernel_size=3, strides=2, padding="same", name=("Conv-" + conv_num))(block)
        if batchNorm: conv  = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
        act = Activation("relu", name=("Act-" + conv_num))(conv)

        conv_num = "3x3-2-256"
        conv = Conv1D(num_filters * 2* 2, kernel_size=3, strides=1, padding="same", name=("Conv-" + conv_num))(act)
        if batchNorm: conv  = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
        act = Activation("relu", name=("Act-" + conv_num))(conv)
        block = keras.layers.Add(name="add-"+conv_num)([feature_increase, act])

        flat9 = Flatten()(block)

        # combined = concatenate([inputAA, flat9 ]) # flat1, flat2, flat3, flat4, flat5, flat6
        # combined outputs
        fully = Dense(neuron_comb, activation="relu", name="Fully-con-1")(flat9)
        fully = Dropout(drop_rate, name="DropF-1")(fully)
        for l in range(2, layers_comb + 1, 1):
            fully = Dense( (neuron_comb/(2**(l-1))), activation="relu", name="Fully-con-" + str(l))(fully)
            fully = Dropout(drop_rate, name="DropF-" + str(l))(fully)

        # initializer = []
        # if activation_out == 'softmax' or activation_out == 'sigmoid': initializer = tf.keras.initializers.HeNormal() # or uniform
        # else: initializer = tf.keras.initializers.GlorotNormal()

        out = Dense(2, activation=activation_out, name="active-Out-sig/soft")(fully)
        # sigmoid (original) # tanh (-1 1 binary binary) # softmax (cat-cross) prob-distribution # relu #? PReLU - Leaky ReLU -
        ############################################################################################
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.default_rng(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 64, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 800}
            # {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')

        plt.tight_layout()
        plt.show()
        print("show")
        return(pd.DataFrame({'mean_tpr': tpr, 'mean_fpr': fpr}) )

        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        
        
        
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        print(final_results['accuracy_score_val'])
        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        

        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        
        
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)

        
        return(finalResults)
        # return( pd.DataFrame( {'return' : ['exit()']} ) )

        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        
        return(finalResults)
        return( pd.DataFrame( {'return' : ['exit()']} ) )
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.8c1b5c71-5722-4a0e-a25b-61778aee1d65"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def all_treat_new_NN(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    model_name = 'full'
    # model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = False
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        try:
            pts = pts.drop([col+'_other'], axis=1)
        except:
            print("except no ",col,"with ---others---" )
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                'pressor_days',
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.533e5b12-9b39-4359-a3b4-050558d8e5f8"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def all_treat_new_copied_20q1q2(derived_cobination):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    
  

    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)

    # '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1'
    # pts =  pts[   (pts['2020Q2']== 1) | (pts['2020Q1']== 11)  ]
    pts =  pts[   (pts['date_of_earliest_covid_diagnosis']== '2020Q1') | (pts['date_of_earliest_covid_diagnosis']== '2020Q2')  ]
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                 '2020Q2', '2020Q1', #'2021Q2', '2021Q1','2020Q4','2020Q3',
                'pressor_days',
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.75025029-953e-45ee-bb89-f75addb1eb0c"),
    all_treat_new_copied_20q1q2=Input(rid="ri.foundry.main.dataset.533e5b12-9b39-4359-a3b4-050558d8e5f8"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def all_treat_new_copied_20q3q4(derived_cobination, all_treat_new_copied_20q1q2):
    all_treat_new_copied = all_treat_new_copied_20q1q2

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    
  

    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)

    # '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1'
    # pts =  pts[   (pts['2020Q2']== 1) | (pts['2020Q1']== 11)  ]
    pts =  pts[   (pts['date_of_earliest_covid_diagnosis']== '2020Q3') | (pts['date_of_earliest_covid_diagnosis']== '2020Q4')  ]
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                 '2020Q4', '2020Q3', #'2021Q2', '2021Q1','2020Q4','2020Q3',
                'pressor_days',
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.d3e0335b-c044-40a1-8f33-bbd2b1ebab3b"),
    all_treat_new_copied_20q3q4=Input(rid="ri.foundry.main.dataset.75025029-953e-45ee-bb89-f75addb1eb0c"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def all_treat_new_copied_21q1q2(derived_cobination, all_treat_new_copied_20q3q4):
    all_treat_new_copied_q3q4 = all_treat_new_copied_20q3q4

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    
  

    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)

    # '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1'
    # pts =  pts[   (pts['2020Q2']== 1) | (pts['2020Q1']== 11)  ]
    pts =  pts[   (pts['date_of_earliest_covid_diagnosis']== '2021Q1') | (pts['date_of_earliest_covid_diagnosis']== '2021Q2')  ]
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                 '2021Q2', '2021Q1', #'2021Q2', '2021Q1','2020Q4','2020Q3',
                'pressor_days',
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.b26bd452-dea3-438d-9e7c-437d8e6b5b79"),
    derived_cobination_delta_2_216=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def all_treat_new_copied_21q1q2_1(derived_cobination_delta_2_216):
    # all_treat_new_copied_q3q4 = all_treat_new_copied_20q3q4

    ######
    input_table = derived_cobination_delta_2_216 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    
  

    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)

    # '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1'
    # pts =  pts[   (pts['2020Q2']== 1) | (pts['2020Q1']== 11)  ]
    pts =  pts[   (pts['date_of_earliest_covid_diagnosis']== '2021Q3') | (pts['date_of_earliest_covid_diagnosis']== '2021Q4')  ]
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                 '2021Q3', '2021Q4', #'2021Q2', '2021Q1','2020Q4','2020Q3',
                'pressor_days',
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            print("Shap datetime.now(): ", datetime.now())
                
            print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
            print('type shap: ',type(shap_values))
            print('type shap class: ', type(shap_values[0]))

            num_class_samples_columns =len(shap_values[target_shap_class][0])
            all_variables = np.array([0 for _ in range(num_class_samples_columns)])
            for x in range(shap_values[target_shap_class].shape[0]) :
                all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
            all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
            print('all_variables avegrafe Shap - shape',all_variables.shape)

            overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
            overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
            overall_importance_df = overall_importance_df.iloc[0]

            overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
            ax2.set_title("Feature importances ")
            ax2.set_ylabel("Shap value")
            fig.tight_layout()
            plt.tight_layout()
            plt.show()
            print("show")
   
            print("overall_importance_df Shap")
            imp_rf = pd.DataFrame() 
            imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
            # return( imp_rf ) 

            print("Detail Shap")
            detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
            return(detailed_shap_df)
            
        print('\nFinal accuracy Table')
        param_grid_ = []
        if model_name != 'rf':
            param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

        accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
        f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
        precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
        recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
        roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
        finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
        confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
        for class_ in range(len(confusion_)):
            class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
            class_str ="class_"+str(class_)+"_accuracy"
            class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
            finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
        finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
        print("datetime.now(): ", datetime.now())
        plt.tight_layout()
        plt.show()
        print("show")
        return(finalResults)
    
    print("Error shoud not get here: datetime.now(): ", datetime.now())
    return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.ae5119fd-18af-42f3-a787-5aed7adc5187"),
    shap_get_it_1=Input(rid="ri.foundry.main.dataset.827c3607-b5ca-430e-b48d-32144df3a08e")
)
@output_image_type('svg')
def all_treat_new_l1_10_copied_1(shap_get_it_1):
    import numpy as np
    top=10

    treatments = ['BiotMQ_', 'BiotMQ_Coag_', 'BiotMQ_Coag_Misc_', 'BiotMQ_Coag_Misc_MonoI_', 'BiotMQ_Coag_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Misc_MonoSP_', 'BiotMQ_Coag_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Misc_ViraTrgt_', 'BiotMQ_Coag_MonoI_', 'BiotMQ_Coag_MonoI_ViraTrgt_', 'BiotMQ_Coag_MonoSP_', 'BiotMQ_Coag_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_', 'BiotMQ_Coag_Ster_Misc_', 'BiotMQ_Coag_Ster_Misc_MonoI_', 'BiotMQ_Coag_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_MonoSP_', 'BiotMQ_Coag_Ster_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoI_', 'BiotMQ_Coag_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoSP_', 'BiotMQ_Coag_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_', 'BiotMQ_Coag_ViraUnp_Misc_', 'BiotMQ_Coag_ViraUnp_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoI_', 'BiotMQ_Coag_ViraUnp_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoSP_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_ViraTrgt_', 'BiotMQ_Misc_', 'BiotMQ_Misc_MonoSP_', 'BiotMQ_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Misc_ViraTrgt_', 'BiotMQ_MonoI_', 'BiotMQ_MonoI_ViraTrgt_', 'BiotMQ_MonoSP_', 'BiotMQ_Ster_', 'BiotMQ_Ster_Misc_', 'BiotMQ_Ster_Misc_MonoI_', 'BiotMQ_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Ster_Misc_MonoSP_', 'BiotMQ_Ster_Misc_ViraTrgt_', 'BiotMQ_Ster_MonoI_', 'BiotMQ_Ster_MonoI_ViraTrgt_', 'BiotMQ_Ster_MonoSP_', 'BiotMQ_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Ster_ViraTrgt_', 'BiotMQ_ViraTrgt_', 'BiotMQ_ViraUnp_', 'BiotMQ_ViraUnp_Misc_', 'BiotMQ_ViraUnp_Misc_MonoI_', 'BiotMQ_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_MonoI_', 'BiotMQ_ViraUnp_Ster_', 'BiotMQ_ViraUnp_Ster_Misc_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoI_', 'BiotMQ_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoSP_', 'BiotMQ_ViraUnp_Ster_MonoSP_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_ViraUnp_ViraTrgt_', 'Coag_', 'Coag_Misc_', 'Coag_Misc_MonoI_', 'Coag_Misc_MonoI_ViraTrgt_', 'Coag_Misc_MonoSP_', 'Coag_Misc_MonoSP_ViraTrgt_', 'Coag_Misc_ViraTrgt_', 'Coag_MonoI_', 'Coag_MonoI_ViraTrgt_', 'Coag_MonoSP_', 'Coag_MonoSP_ViraTrgt_', 'Coag_Ster_', 'Coag_Ster_Misc_', 'Coag_Ster_Misc_MonoI_', 'Coag_Ster_Misc_MonoI_ViraTrgt_', 'Coag_Ster_Misc_MonoSP_', 'Coag_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_Ster_Misc_ViraTrgt_', 'Coag_Ster_MonoI_', 'Coag_Ster_MonoI_ViraTrgt_', 'Coag_Ster_MonoSP_', 'Coag_Ster_MonoSP_ViraTrgt_', 'Coag_Ster_ViraTrgt_', 'Coag_ViraTrgt_', 'Coag_ViraUnp_', 'Coag_ViraUnp_Misc_', 'Coag_ViraUnp_Misc_MonoI_', 'Coag_ViraUnp_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Misc_MonoSP_', 'Coag_ViraUnp_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Misc_ViraTrgt_', 'Coag_ViraUnp_MonoI_', 'Coag_ViraUnp_MonoI_ViraTrgt_', 'Coag_ViraUnp_MonoSP_', 'Coag_ViraUnp_Ster_', 'Coag_ViraUnp_Ster_Misc_', 'Coag_ViraUnp_Ster_Misc_MonoI_', 'Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_MonoSP_', 'Coag_ViraUnp_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoI_', 'Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoSP_', 'Coag_ViraUnp_Ster_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_ViraTrgt_', 'Coag_ViraUnp_ViraTrgt_', 'Misc_', 'Misc_MonoI_', 'Misc_MonoI_ViraTrgt_', 'Misc_MonoSP_', 'Misc_MonoSP_ViraTrgt_', 'Misc_ViraTrgt_', 'MonoI_', 'MonoI_ViraTrgt_', 'MonoSP_', 'MonoSP_ViraTrgt_', 'Ster_', 'Ster_Misc_', 'Ster_Misc_MonoI_', 'Ster_Misc_MonoI_ViraTrgt_', 'Ster_Misc_MonoSP_', 'Ster_Misc_MonoSP_ViraTrgt_', 'Ster_Misc_ViraTrgt_', 'Ster_MonoI_', 'Ster_MonoI_ViraTrgt_', 'Ster_MonoSP_', 'Ster_MonoSP_MonoI_', 'Ster_MonoSP_ViraTrgt_', 'Ster_ViraTrgt_', 'ViraTrgt_', 'ViraUnp_', 'ViraUnp_Misc_', 'ViraUnp_Misc_ViraTrgt_', 'ViraUnp_MonoI_', 'ViraUnp_MonoSP_', 'ViraUnp_Ster_', 'ViraUnp_Ster_Misc_', 'ViraUnp_Ster_Misc_MonoI_', 'ViraUnp_Ster_Misc_MonoSP_', 'ViraUnp_Ster_Misc_ViraTrgt_', 'ViraUnp_Ster_MonoI_', 'ViraUnp_Ster_MonoI_ViraTrgt_', 'ViraUnp_Ster_MonoSP_', 'ViraUnp_Ster_MonoSP_ViraTrgt_', 'ViraUnp_Ster_ViraTrgt_', 'ViraUnp_ViraTrgt_']

    # not Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Biot_Misc_Mono_
    
    # Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Biot_Misc_Mono_
    # for col in ['Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Biot_Misc_Mono_']:
    #     all_treat_new_l1[col] = np.nan

    all_treat_new = shap_get_it_1[treatments]
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns # pts_col_no_treat +   treat_unique

    # for drug, b_name in zip(['Steroid Preparations','Anticoagulants','Antiviral Agents','Miscellaneous Antibiotics','Miscellaneous','Monoclonal Antibodies'],
    #                           ['Ster',                'Coag',             'Vira',             'Biot',                     'Misc',         'Mono']):

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    # pts_col = [ x.replace('Ster','Steroid').replace('Coag','Anticoagulants').replace('Vira','Antiviral').replace('Biot','Mis.Antibiotics').replace('Misc','Miscellaneous').replace('Mono','Monoclonal') for x in pts_col]

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]

    fig = plt.figure(figsize=(10,4)) 
    fig.subplots_adjust(left=0.3)
    ax1 = plt.subplot2grid((2,2),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((2,2),(0,1), rowspan = 2,colspan = 1)

    positives = overall_importance_df[overall_importance_df>0].sort_values(ascending=True)
    negatives = overall_importance_df[overall_importance_df<=0].sort_values(ascending=False)

    positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, top )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, top )) # .9 positives.size

    positives[-top:].plot.barh( ax=ax1,color=color_p) # yerr=std[:24], ######[new_Col] [new_Col]
    negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]
    print(" negative.dtype",type(negatives[-top:]))

    ax2.set_xlim(negatives.max() * 1.03,negatives.min()* 1.2 )

    ax2.set_title("Negative Impacts ")
    ax2.set_xlabel("Mean Shap value (impact on model output)")
    ax2.set_ylabel("Features")
    ax1.set_title("Positive Impacts ")
    ax1.set_xlabel("Mean Shap value (impact on model output)")
    ax1.set_ylabel("Features")
    fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")

    combined_Frame = (positives[(-top):].append(negatives[(-top):])).to_frame().T 
    print(combined_Frame)
    return(     combined_Frame    )

@transform_pandas(
    Output(rid="ri.vector.main.execute.ba419135-815d-4e22-a1c9-c02c53ecbd94"),
    shap_get_it_3=Input(rid="ri.foundry.main.dataset.62d8fdfa-5e91-41f2-b57c-e6584a40d79c")
)
@output_image_type('svg')
def all_treat_new_l3_10_copied_1(shap_get_it_3):
    import numpy as np
    top=10

    treatments = ['BiotMQ_', 'BiotMQ_Coag_', 'BiotMQ_Coag_Misc_', 'BiotMQ_Coag_Misc_MonoI_', 'BiotMQ_Coag_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Misc_MonoSP_', 'BiotMQ_Coag_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Misc_ViraTrgt_', 'BiotMQ_Coag_MonoI_', 'BiotMQ_Coag_MonoI_ViraTrgt_', 'BiotMQ_Coag_MonoSP_', 'BiotMQ_Coag_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_', 'BiotMQ_Coag_Ster_Misc_', 'BiotMQ_Coag_Ster_Misc_MonoI_', 'BiotMQ_Coag_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_MonoSP_', 'BiotMQ_Coag_Ster_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoI_', 'BiotMQ_Coag_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoSP_', 'BiotMQ_Coag_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_', 'BiotMQ_Coag_ViraUnp_Misc_', 'BiotMQ_Coag_ViraUnp_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoI_', 'BiotMQ_Coag_ViraUnp_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoSP_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_ViraTrgt_', 'BiotMQ_Misc_', 'BiotMQ_Misc_MonoSP_', 'BiotMQ_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Misc_ViraTrgt_', 'BiotMQ_MonoI_', 'BiotMQ_MonoI_ViraTrgt_', 'BiotMQ_MonoSP_', 'BiotMQ_Ster_', 'BiotMQ_Ster_Misc_', 'BiotMQ_Ster_Misc_MonoI_', 'BiotMQ_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Ster_Misc_MonoSP_', 'BiotMQ_Ster_Misc_ViraTrgt_', 'BiotMQ_Ster_MonoI_', 'BiotMQ_Ster_MonoI_ViraTrgt_', 'BiotMQ_Ster_MonoSP_', 'BiotMQ_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Ster_ViraTrgt_', 'BiotMQ_ViraTrgt_', 'BiotMQ_ViraUnp_', 'BiotMQ_ViraUnp_Misc_', 'BiotMQ_ViraUnp_Misc_MonoI_', 'BiotMQ_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_MonoI_', 'BiotMQ_ViraUnp_Ster_', 'BiotMQ_ViraUnp_Ster_Misc_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoI_', 'BiotMQ_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoSP_', 'BiotMQ_ViraUnp_Ster_MonoSP_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_ViraUnp_ViraTrgt_', 'Coag_', 'Coag_Misc_', 'Coag_Misc_MonoI_', 'Coag_Misc_MonoI_ViraTrgt_', 'Coag_Misc_MonoSP_', 'Coag_Misc_MonoSP_ViraTrgt_', 'Coag_Misc_ViraTrgt_', 'Coag_MonoI_', 'Coag_MonoI_ViraTrgt_', 'Coag_MonoSP_', 'Coag_MonoSP_ViraTrgt_', 'Coag_Ster_', 'Coag_Ster_Misc_', 'Coag_Ster_Misc_MonoI_', 'Coag_Ster_Misc_MonoI_ViraTrgt_', 'Coag_Ster_Misc_MonoSP_', 'Coag_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_Ster_Misc_ViraTrgt_', 'Coag_Ster_MonoI_', 'Coag_Ster_MonoI_ViraTrgt_', 'Coag_Ster_MonoSP_', 'Coag_Ster_MonoSP_ViraTrgt_', 'Coag_Ster_ViraTrgt_', 'Coag_ViraTrgt_', 'Coag_ViraUnp_', 'Coag_ViraUnp_Misc_', 'Coag_ViraUnp_Misc_MonoI_', 'Coag_ViraUnp_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Misc_MonoSP_', 'Coag_ViraUnp_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Misc_ViraTrgt_', 'Coag_ViraUnp_MonoI_', 'Coag_ViraUnp_MonoI_ViraTrgt_', 'Coag_ViraUnp_MonoSP_', 'Coag_ViraUnp_Ster_', 'Coag_ViraUnp_Ster_Misc_', 'Coag_ViraUnp_Ster_Misc_MonoI_', 'Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_MonoSP_', 'Coag_ViraUnp_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoI_', 'Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoSP_', 'Coag_ViraUnp_Ster_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_ViraTrgt_', 'Coag_ViraUnp_ViraTrgt_', 'Misc_', 'Misc_MonoI_', 'Misc_MonoI_ViraTrgt_', 'Misc_MonoSP_', 'Misc_MonoSP_ViraTrgt_', 'Misc_ViraTrgt_', 'MonoI_', 'MonoI_ViraTrgt_', 'MonoSP_', 'MonoSP_ViraTrgt_', 'Ster_', 'Ster_Misc_', 'Ster_Misc_MonoI_', 'Ster_Misc_MonoI_ViraTrgt_', 'Ster_Misc_MonoSP_', 'Ster_Misc_MonoSP_ViraTrgt_', 'Ster_Misc_ViraTrgt_', 'Ster_MonoI_', 'Ster_MonoI_ViraTrgt_', 'Ster_MonoSP_', 'Ster_MonoSP_MonoI_', 'Ster_MonoSP_ViraTrgt_', 'Ster_ViraTrgt_', 'ViraTrgt_', 'ViraUnp_', 'ViraUnp_Misc_', 'ViraUnp_Misc_ViraTrgt_', 'ViraUnp_MonoI_', 'ViraUnp_MonoSP_', 'ViraUnp_Ster_', 'ViraUnp_Ster_Misc_', 'ViraUnp_Ster_Misc_MonoI_', 'ViraUnp_Ster_Misc_MonoSP_', 'ViraUnp_Ster_Misc_ViraTrgt_', 'ViraUnp_Ster_MonoI_', 'ViraUnp_Ster_MonoI_ViraTrgt_', 'ViraUnp_Ster_MonoSP_', 'ViraUnp_Ster_MonoSP_ViraTrgt_', 'ViraUnp_Ster_ViraTrgt_', 'ViraUnp_ViraTrgt_']

    # not Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Biot_Misc_Mono_
    
    # Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Biot_Misc_Mono_
    # for col in ['Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Biot_Misc_Mono_']:
    #     shap_get_it_9[col] = np.nan

    all_treat_new = shap_get_it_3[treatments]
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns # pts_col_no_treat +   treat_unique

    # for drug, b_name in zip(['Steroid Preparations','Anticoagulants','Antiviral Agents','Miscellaneous Antibiotics','Miscellaneous','Monoclonal Antibodies'],
    #                           ['Ster',                'Coag',             'Vira',             'Biot',                     'Misc',         'Mono']):

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    # pts_col = [ x.replace('Ster','Steroid').replace('Coag','Anticoagulants').replace('Vira','Antiviral').replace('Biot','Mis.Antibiotics').replace('Misc','Miscellaneous').replace('Mono','Monoclonal') for x in pts_col]

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]

    fig = plt.figure(figsize=(10,4)) 
    fig.subplots_adjust(left=0.3)
    ax1 = plt.subplot2grid((2,2),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((2,2),(0,1), rowspan = 2,colspan = 1)

    positives = overall_importance_df[overall_importance_df>0].sort_values(ascending=True)
    negatives = overall_importance_df[overall_importance_df<=0].sort_values(ascending=False)

    positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, top )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, top )) # .9 positives.size

    positives[-top:].plot.barh( ax=ax1,color=color_p) # yerr=std[:24], ######[new_Col] [new_Col]
    negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]
    print(" negative.dtype",type(negatives[-top:]))

    ax2.set_xlim(negatives.max() * 1.03,negatives.min()* 1.2 )

    ax2.set_title("Negative Impacts ")
    ax2.set_xlabel("Mean Shap value (impact on model output)")
    ax2.set_ylabel("Features")
    ax1.set_title("Positive Impacts ")
    ax1.set_xlabel("Mean Shap value (impact on model output)")
    ax1.set_ylabel("Features")
    fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")

    combined_Frame = (positives[(-top):].append(negatives[(-top):])).to_frame().T 
    print(combined_Frame)
    return(     combined_Frame    )

@transform_pandas(
    Output(rid="ri.vector.main.execute.9cc7bf8f-0325-4444-b0c8-eb8097984e18"),
    shap_get_it_5=Input(rid="ri.foundry.main.dataset.bdb0d392-e218-4f15-9c6f-95f44462585a")
)
@output_image_type('svg')
def all_treat_new_l5_10_copied_1(shap_get_it_5):
    import numpy as np
    top=10

    treatments = ['BiotMQ_', 'BiotMQ_Coag_', 'BiotMQ_Coag_Misc_', 'BiotMQ_Coag_Misc_MonoI_', 'BiotMQ_Coag_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Misc_MonoSP_', 'BiotMQ_Coag_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Misc_ViraTrgt_', 'BiotMQ_Coag_MonoI_', 'BiotMQ_Coag_MonoI_ViraTrgt_', 'BiotMQ_Coag_MonoSP_', 'BiotMQ_Coag_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_', 'BiotMQ_Coag_Ster_Misc_', 'BiotMQ_Coag_Ster_Misc_MonoI_', 'BiotMQ_Coag_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_MonoSP_', 'BiotMQ_Coag_Ster_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoI_', 'BiotMQ_Coag_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoSP_', 'BiotMQ_Coag_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_', 'BiotMQ_Coag_ViraUnp_Misc_', 'BiotMQ_Coag_ViraUnp_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoI_', 'BiotMQ_Coag_ViraUnp_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoSP_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_ViraTrgt_', 'BiotMQ_Misc_', 'BiotMQ_Misc_MonoSP_', 'BiotMQ_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Misc_ViraTrgt_', 'BiotMQ_MonoI_', 'BiotMQ_MonoI_ViraTrgt_', 'BiotMQ_MonoSP_', 'BiotMQ_Ster_', 'BiotMQ_Ster_Misc_', 'BiotMQ_Ster_Misc_MonoI_', 'BiotMQ_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Ster_Misc_MonoSP_', 'BiotMQ_Ster_Misc_ViraTrgt_', 'BiotMQ_Ster_MonoI_', 'BiotMQ_Ster_MonoI_ViraTrgt_', 'BiotMQ_Ster_MonoSP_', 'BiotMQ_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Ster_ViraTrgt_', 'BiotMQ_ViraTrgt_', 'BiotMQ_ViraUnp_', 'BiotMQ_ViraUnp_Misc_', 'BiotMQ_ViraUnp_Misc_MonoI_', 'BiotMQ_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_MonoI_', 'BiotMQ_ViraUnp_Ster_', 'BiotMQ_ViraUnp_Ster_Misc_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoI_', 'BiotMQ_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoSP_', 'BiotMQ_ViraUnp_Ster_MonoSP_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_ViraUnp_ViraTrgt_', 'Coag_', 'Coag_Misc_', 'Coag_Misc_MonoI_', 'Coag_Misc_MonoI_ViraTrgt_', 'Coag_Misc_MonoSP_', 'Coag_Misc_MonoSP_ViraTrgt_', 'Coag_Misc_ViraTrgt_', 'Coag_MonoI_', 'Coag_MonoI_ViraTrgt_', 'Coag_MonoSP_', 'Coag_MonoSP_ViraTrgt_', 'Coag_Ster_', 'Coag_Ster_Misc_', 'Coag_Ster_Misc_MonoI_', 'Coag_Ster_Misc_MonoI_ViraTrgt_', 'Coag_Ster_Misc_MonoSP_', 'Coag_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_Ster_Misc_ViraTrgt_', 'Coag_Ster_MonoI_', 'Coag_Ster_MonoI_ViraTrgt_', 'Coag_Ster_MonoSP_', 'Coag_Ster_MonoSP_ViraTrgt_', 'Coag_Ster_ViraTrgt_', 'Coag_ViraTrgt_', 'Coag_ViraUnp_', 'Coag_ViraUnp_Misc_', 'Coag_ViraUnp_Misc_MonoI_', 'Coag_ViraUnp_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Misc_MonoSP_', 'Coag_ViraUnp_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Misc_ViraTrgt_', 'Coag_ViraUnp_MonoI_', 'Coag_ViraUnp_MonoI_ViraTrgt_', 'Coag_ViraUnp_MonoSP_', 'Coag_ViraUnp_Ster_', 'Coag_ViraUnp_Ster_Misc_', 'Coag_ViraUnp_Ster_Misc_MonoI_', 'Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_MonoSP_', 'Coag_ViraUnp_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoI_', 'Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoSP_', 'Coag_ViraUnp_Ster_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_ViraTrgt_', 'Coag_ViraUnp_ViraTrgt_', 'Misc_', 'Misc_MonoI_', 'Misc_MonoI_ViraTrgt_', 'Misc_MonoSP_', 'Misc_MonoSP_ViraTrgt_', 'Misc_ViraTrgt_', 'MonoI_', 'MonoI_ViraTrgt_', 'MonoSP_', 'MonoSP_ViraTrgt_', 'Ster_', 'Ster_Misc_', 'Ster_Misc_MonoI_', 'Ster_Misc_MonoI_ViraTrgt_', 'Ster_Misc_MonoSP_', 'Ster_Misc_MonoSP_ViraTrgt_', 'Ster_Misc_ViraTrgt_', 'Ster_MonoI_', 'Ster_MonoI_ViraTrgt_', 'Ster_MonoSP_', 'Ster_MonoSP_MonoI_', 'Ster_MonoSP_ViraTrgt_', 'Ster_ViraTrgt_', 'ViraTrgt_', 'ViraUnp_', 'ViraUnp_Misc_', 'ViraUnp_Misc_ViraTrgt_', 'ViraUnp_MonoI_', 'ViraUnp_MonoSP_', 'ViraUnp_Ster_', 'ViraUnp_Ster_Misc_', 'ViraUnp_Ster_Misc_MonoI_', 'ViraUnp_Ster_Misc_MonoSP_', 'ViraUnp_Ster_Misc_ViraTrgt_', 'ViraUnp_Ster_MonoI_', 'ViraUnp_Ster_MonoI_ViraTrgt_', 'ViraUnp_Ster_MonoSP_', 'ViraUnp_Ster_MonoSP_ViraTrgt_', 'ViraUnp_Ster_ViraTrgt_', 'ViraUnp_ViraTrgt_']

    # not Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Biot_Misc_Mono_
    
    # Misc_Mono_', 'Ster_Biot_Misc_Mono_', 'Ster_Vira_Biot_Mono_', 'Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Coag_Biot_Misc_Mono_', 'Ster_Biot_Mono_', 'Vira_Biot_Misc_Mono_
    # for col in ['Misc_Mono_', 'Ster_Biot_Misc_Mono_', 'Ster_Vira_Biot_Mono_', 'Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Coag_Biot_Misc_Mono_', 'Ster_Biot_Mono_', 'Vira_Biot_Misc_Mono_']:
    #     shap_get_it_9[col] = np.nan

    all_treat_new = shap_get_it_5[treatments]
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns # pts_col_no_treat +   treat_unique

    # for drug, b_name in zip(['Steroid Preparations','Anticoagulants','Antiviral Agents','Miscellaneous Antibiotics','Miscellaneous','Monoclonal Antibodies'],
    #                           ['Ster',                'Coag',             'Vira',             'Biot',                     'Misc',         'Mono']):

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    # pts_col = [ x.replace('Ster','Steroid').replace('Coag','Anticoagulants').replace('Vira','Antiviral').replace('Biot','Mis.Antibiotics').replace('Misc','Miscellaneous').replace('Mono','Monoclonal') for x in pts_col]

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]

    fig = plt.figure(figsize=(10,4)) 
    fig.subplots_adjust(left=0.3)
    ax1 = plt.subplot2grid((2,2),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((2,2),(0,1), rowspan = 2,colspan = 1)

    positives = overall_importance_df[overall_importance_df>0].sort_values(ascending=True)
    negatives = overall_importance_df[overall_importance_df<=0].sort_values(ascending=False)

    positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, top )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, top )) # .9 positives.size

    positives[-top:].plot.barh( ax=ax1,color=color_p) # yerr=std[:24], ######[new_Col] [new_Col]
    negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]
    print(" negative.dtype",type(negatives[-top:]))

    ax2.set_xlim(negatives.max() * 1.03,negatives.min()* 1.2 )

    ax2.set_title("Negative Impacts ")
    ax2.set_xlabel("Mean Shap value (impact on model output)")
    ax2.set_ylabel("Features")
    ax1.set_title("Positive Impacts ")
    ax1.set_xlabel("Mean Shap value (impact on model output)")
    ax1.set_ylabel("Features")
    fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")

    combined_Frame = (positives[(-top):].append(negatives[(-top):])).to_frame().T 
    print(combined_Frame)
    return(     combined_Frame    )

@transform_pandas(
    Output(rid="ri.vector.main.execute.722be80e-8c67-4255-ba7a-78f7641eb76f"),
    shap_get_it_7=Input(rid="ri.foundry.main.dataset.6c215750-5262-4299-a16d-2dfbaf4a8a6e")
)
@output_image_type('svg')
def all_treat_new_l7_10_copied_1(shap_get_it_7):
    import numpy as np
    top=10

    treatments =  ['BiotMQ_', 'BiotMQ_Coag_', 'BiotMQ_Coag_Misc_', 'BiotMQ_Coag_Misc_MonoI_', 'BiotMQ_Coag_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Misc_MonoSP_', 'BiotMQ_Coag_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Misc_ViraTrgt_', 'BiotMQ_Coag_MonoI_', 'BiotMQ_Coag_MonoI_ViraTrgt_', 'BiotMQ_Coag_MonoSP_', 'BiotMQ_Coag_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_', 'BiotMQ_Coag_Ster_Misc_', 'BiotMQ_Coag_Ster_Misc_MonoI_', 'BiotMQ_Coag_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_MonoSP_', 'BiotMQ_Coag_Ster_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoI_', 'BiotMQ_Coag_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoSP_', 'BiotMQ_Coag_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_', 'BiotMQ_Coag_ViraUnp_Misc_', 'BiotMQ_Coag_ViraUnp_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoI_', 'BiotMQ_Coag_ViraUnp_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoSP_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_ViraTrgt_', 'BiotMQ_Misc_', 'BiotMQ_Misc_MonoSP_', 'BiotMQ_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Misc_ViraTrgt_', 'BiotMQ_MonoI_', 'BiotMQ_MonoI_ViraTrgt_', 'BiotMQ_MonoSP_', 'BiotMQ_Ster_', 'BiotMQ_Ster_Misc_', 'BiotMQ_Ster_Misc_MonoI_', 'BiotMQ_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Ster_Misc_MonoSP_', 'BiotMQ_Ster_Misc_ViraTrgt_', 'BiotMQ_Ster_MonoI_', 'BiotMQ_Ster_MonoI_ViraTrgt_', 'BiotMQ_Ster_MonoSP_', 'BiotMQ_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Ster_ViraTrgt_', 'BiotMQ_ViraTrgt_', 'BiotMQ_ViraUnp_', 'BiotMQ_ViraUnp_Misc_', 'BiotMQ_ViraUnp_Misc_MonoI_', 'BiotMQ_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_MonoI_', 'BiotMQ_ViraUnp_Ster_', 'BiotMQ_ViraUnp_Ster_Misc_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoI_', 'BiotMQ_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoSP_', 'BiotMQ_ViraUnp_Ster_MonoSP_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_ViraUnp_ViraTrgt_', 'Coag_', 'Coag_Misc_', 'Coag_Misc_MonoI_', 'Coag_Misc_MonoI_ViraTrgt_', 'Coag_Misc_MonoSP_', 'Coag_Misc_MonoSP_ViraTrgt_', 'Coag_Misc_ViraTrgt_', 'Coag_MonoI_', 'Coag_MonoI_ViraTrgt_', 'Coag_MonoSP_', 'Coag_MonoSP_ViraTrgt_', 'Coag_Ster_', 'Coag_Ster_Misc_', 'Coag_Ster_Misc_MonoI_', 'Coag_Ster_Misc_MonoI_ViraTrgt_', 'Coag_Ster_Misc_MonoSP_', 'Coag_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_Ster_Misc_ViraTrgt_', 'Coag_Ster_MonoI_', 'Coag_Ster_MonoI_ViraTrgt_', 'Coag_Ster_MonoSP_', 'Coag_Ster_MonoSP_ViraTrgt_', 'Coag_Ster_ViraTrgt_', 'Coag_ViraTrgt_', 'Coag_ViraUnp_', 'Coag_ViraUnp_Misc_', 'Coag_ViraUnp_Misc_MonoI_', 'Coag_ViraUnp_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Misc_MonoSP_', 'Coag_ViraUnp_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Misc_ViraTrgt_', 'Coag_ViraUnp_MonoI_', 'Coag_ViraUnp_MonoI_ViraTrgt_', 'Coag_ViraUnp_MonoSP_', 'Coag_ViraUnp_Ster_', 'Coag_ViraUnp_Ster_Misc_', 'Coag_ViraUnp_Ster_Misc_MonoI_', 'Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_MonoSP_', 'Coag_ViraUnp_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoI_', 'Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoSP_', 'Coag_ViraUnp_Ster_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_ViraTrgt_', 'Coag_ViraUnp_ViraTrgt_', 'Misc_', 'Misc_MonoI_', 'Misc_MonoI_ViraTrgt_', 'Misc_MonoSP_', 'Misc_MonoSP_ViraTrgt_', 'Misc_ViraTrgt_', 'MonoI_', 'MonoI_ViraTrgt_', 'MonoSP_', 'MonoSP_ViraTrgt_', 'Ster_', 'Ster_Misc_', 'Ster_Misc_MonoI_', 'Ster_Misc_MonoI_ViraTrgt_', 'Ster_Misc_MonoSP_', 'Ster_Misc_MonoSP_ViraTrgt_', 'Ster_Misc_ViraTrgt_', 'Ster_MonoI_', 'Ster_MonoI_ViraTrgt_', 'Ster_MonoSP_', 'Ster_MonoSP_MonoI_', 'Ster_MonoSP_ViraTrgt_', 'Ster_ViraTrgt_', 'ViraTrgt_', 'ViraUnp_', 'ViraUnp_Misc_', 'ViraUnp_Misc_ViraTrgt_', 'ViraUnp_MonoI_', 'ViraUnp_MonoSP_', 'ViraUnp_Ster_', 'ViraUnp_Ster_Misc_', 'ViraUnp_Ster_Misc_MonoI_', 'ViraUnp_Ster_Misc_MonoSP_', 'ViraUnp_Ster_Misc_ViraTrgt_', 'ViraUnp_Ster_MonoI_', 'ViraUnp_Ster_MonoI_ViraTrgt_', 'ViraUnp_Ster_MonoSP_', 'ViraUnp_Ster_MonoSP_ViraTrgt_', 'ViraUnp_Ster_ViraTrgt_', 'ViraUnp_ViraTrgt_']

    # not Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Biot_Misc_Mono_
    
    # Misc_Mono_', 'Ster_Biot_Misc_Mono_', 'Ster_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Vira_Misc_Mono_
    # for col in ['Misc_Mono_', 'Ster_Biot_Misc_Mono_', 'Ster_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Vira_Misc_Mono_']:
    #     shap_get_it_9[col] = np.nan

    all_treat_new = shap_get_it_7[treatments]
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns # pts_col_no_treat +   treat_unique

    # for drug, b_name in zip(['Steroid Preparations','Anticoagulants','Antiviral Agents','Miscellaneous Antibiotics','Miscellaneous','Monoclonal Antibodies'],
    #                           ['Ster',                'Coag',             'Vira',             'Biot',                     'Misc',         'Mono']):

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    # pts_col = [ x.replace('Ster','Steroid').replace('Coag','Anticoagulants').replace('Vira','Antiviral').replace('Biot','Mis.Antibiotics').replace('Misc','Miscellaneous').replace('Mono','Monoclonal') for x in pts_col]

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]

    fig = plt.figure(figsize=(10,4)) 
    fig.subplots_adjust(left=0.3)
    ax1 = plt.subplot2grid((2,2),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((2,2),(0,1), rowspan = 2,colspan = 1)

    positives = overall_importance_df[overall_importance_df>0].sort_values(ascending=True)
    negatives = overall_importance_df[overall_importance_df<=0].sort_values(ascending=False)

    positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, top )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, top )) # .9 positives.size

    positives[-top:].plot.barh( ax=ax1,color=color_p) # yerr=std[:24], ######[new_Col] [new_Col]
    negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]
    print(" negative.dtype",type(negatives[-top:]))

    ax2.set_xlim(negatives.max() * 1.03,negatives.min()* 1.2 )

    ax2.set_title("Negative Impacts ")
    ax2.set_xlabel("Mean Shap value (impact on model output)")
    ax2.set_ylabel("Features")
    ax1.set_title("Positive Impacts ")
    ax1.set_xlabel("Mean Shap value (impact on model output)")
    ax1.set_ylabel("Features")
    fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")

    combined_Frame = (positives[(-top):].append(negatives[(-top):])).to_frame().T 
    print(combined_Frame)
    return(     combined_Frame    )

@transform_pandas(
    Output(rid="ri.vector.main.execute.c1e00dee-e5f0-4247-a688-00a78adec7d2"),
    shap_get_it_9=Input(rid="ri.foundry.main.dataset.b0855839-cff8-402c-8945-0cd833d227f3")
)
@output_image_type('svg')
def all_treat_new_l9_10_copied_1(shap_get_it_9):
    import numpy as np
    top=10

    treatments =  ['BiotMQ_', 'BiotMQ_Coag_', 'BiotMQ_Coag_Misc_', 'BiotMQ_Coag_Misc_MonoI_', 'BiotMQ_Coag_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Misc_MonoSP_', 'BiotMQ_Coag_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Misc_ViraTrgt_', 'BiotMQ_Coag_MonoI_', 'BiotMQ_Coag_MonoI_ViraTrgt_', 'BiotMQ_Coag_MonoSP_', 'BiotMQ_Coag_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_', 'BiotMQ_Coag_Ster_Misc_', 'BiotMQ_Coag_Ster_Misc_MonoI_', 'BiotMQ_Coag_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_MonoSP_', 'BiotMQ_Coag_Ster_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoI_', 'BiotMQ_Coag_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoSP_', 'BiotMQ_Coag_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_', 'BiotMQ_Coag_ViraUnp_Misc_', 'BiotMQ_Coag_ViraUnp_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoI_', 'BiotMQ_Coag_ViraUnp_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoSP_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_ViraTrgt_', 'BiotMQ_Misc_', 'BiotMQ_Misc_MonoSP_', 'BiotMQ_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Misc_ViraTrgt_', 'BiotMQ_MonoI_', 'BiotMQ_MonoI_ViraTrgt_', 'BiotMQ_MonoSP_', 'BiotMQ_Ster_', 'BiotMQ_Ster_Misc_', 'BiotMQ_Ster_Misc_MonoI_', 'BiotMQ_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Ster_Misc_MonoSP_', 'BiotMQ_Ster_Misc_ViraTrgt_', 'BiotMQ_Ster_MonoI_', 'BiotMQ_Ster_MonoI_ViraTrgt_', 'BiotMQ_Ster_MonoSP_', 'BiotMQ_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Ster_ViraTrgt_', 'BiotMQ_ViraTrgt_', 'BiotMQ_ViraUnp_', 'BiotMQ_ViraUnp_Misc_', 'BiotMQ_ViraUnp_Misc_MonoI_', 'BiotMQ_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_MonoI_', 'BiotMQ_ViraUnp_Ster_', 'BiotMQ_ViraUnp_Ster_Misc_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoI_', 'BiotMQ_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoSP_', 'BiotMQ_ViraUnp_Ster_MonoSP_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_ViraUnp_ViraTrgt_', 'Coag_', 'Coag_Misc_', 'Coag_Misc_MonoI_', 'Coag_Misc_MonoI_ViraTrgt_', 'Coag_Misc_MonoSP_', 'Coag_Misc_MonoSP_ViraTrgt_', 'Coag_Misc_ViraTrgt_', 'Coag_MonoI_', 'Coag_MonoI_ViraTrgt_', 'Coag_MonoSP_', 'Coag_MonoSP_ViraTrgt_', 'Coag_Ster_', 'Coag_Ster_Misc_', 'Coag_Ster_Misc_MonoI_', 'Coag_Ster_Misc_MonoI_ViraTrgt_', 'Coag_Ster_Misc_MonoSP_', 'Coag_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_Ster_Misc_ViraTrgt_', 'Coag_Ster_MonoI_', 'Coag_Ster_MonoI_ViraTrgt_', 'Coag_Ster_MonoSP_', 'Coag_Ster_MonoSP_ViraTrgt_', 'Coag_Ster_ViraTrgt_', 'Coag_ViraTrgt_', 'Coag_ViraUnp_', 'Coag_ViraUnp_Misc_', 'Coag_ViraUnp_Misc_MonoI_', 'Coag_ViraUnp_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Misc_MonoSP_', 'Coag_ViraUnp_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Misc_ViraTrgt_', 'Coag_ViraUnp_MonoI_', 'Coag_ViraUnp_MonoI_ViraTrgt_', 'Coag_ViraUnp_MonoSP_', 'Coag_ViraUnp_Ster_', 'Coag_ViraUnp_Ster_Misc_', 'Coag_ViraUnp_Ster_Misc_MonoI_', 'Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_MonoSP_', 'Coag_ViraUnp_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoI_', 'Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoSP_', 'Coag_ViraUnp_Ster_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_ViraTrgt_', 'Coag_ViraUnp_ViraTrgt_', 'Misc_', 'Misc_MonoI_', 'Misc_MonoI_ViraTrgt_', 'Misc_MonoSP_', 'Misc_MonoSP_ViraTrgt_', 'Misc_ViraTrgt_', 'MonoI_', 'MonoI_ViraTrgt_', 'MonoSP_', 'MonoSP_ViraTrgt_', 'Ster_', 'Ster_Misc_', 'Ster_Misc_MonoI_', 'Ster_Misc_MonoI_ViraTrgt_', 'Ster_Misc_MonoSP_', 'Ster_Misc_MonoSP_ViraTrgt_', 'Ster_Misc_ViraTrgt_', 'Ster_MonoI_', 'Ster_MonoI_ViraTrgt_', 'Ster_MonoSP_', 'Ster_MonoSP_MonoI_', 'Ster_MonoSP_ViraTrgt_', 'Ster_ViraTrgt_', 'ViraTrgt_', 'ViraUnp_', 'ViraUnp_Misc_', 'ViraUnp_Misc_ViraTrgt_', 'ViraUnp_MonoI_', 'ViraUnp_MonoSP_', 'ViraUnp_Ster_', 'ViraUnp_Ster_Misc_', 'ViraUnp_Ster_Misc_MonoI_', 'ViraUnp_Ster_Misc_MonoSP_', 'ViraUnp_Ster_Misc_ViraTrgt_', 'ViraUnp_Ster_MonoI_', 'ViraUnp_Ster_MonoI_ViraTrgt_', 'ViraUnp_Ster_MonoSP_', 'ViraUnp_Ster_MonoSP_ViraTrgt_', 'ViraUnp_Ster_ViraTrgt_', 'ViraUnp_ViraTrgt_']

    # not Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Biot_Misc_Mono_
    
    # 
    # for col in ['Coag_Biot_Mono_', 'Vira_Biot_Mono_', 'Ster_Coag_Vira_Misc_Mono_', 'Coag_Vira_Biot_Misc_Mono_', 'Ster_Vira_Biot_Misc_Mono_', 'Vira_Misc_Mono_', 'Coag_Vira_Misc_Mono_', 'Ster_Coag_Biot_Misc_Mono_', 'Vira_Biot_Misc_Mono_']:
    #shap_get_it_9[col] = np.nan

    all_treat_new = shap_get_it_9[treatments]
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns # pts_col_no_treat +   treat_unique

    # for drug, b_name in zip(['Steroid Preparations','Anticoagulants','Antiviral Agents','Miscellaneous Antibiotics','Miscellaneous','Monoclonal Antibodies'],
    #                           ['Ster',                'Coag',             'Vira',             'Biot',                     'Misc',         'Mono']):

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    # pts_col = [ x.replace('Ster','Steroid').replace('Coag','Anticoagulants').replace('Vira','Antiviral').replace('Biot','Mis.Antibiotics').replace('Misc','Miscellaneous').replace('Mono','Monoclonal') for x in pts_col]

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]

    fig = plt.figure(figsize=(10,4)) 
    fig.subplots_adjust(left=0.3)
    ax1 = plt.subplot2grid((2,2),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((2,2),(0,1), rowspan = 2,colspan = 1)

    positives = overall_importance_df[overall_importance_df>0].sort_values(ascending=True)
    negatives = overall_importance_df[overall_importance_df<=0].sort_values(ascending=False)

    positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, top )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, top )) # .9 positives.size

    positives[-top:].plot.barh( ax=ax1,color=color_p) # yerr=std[:24], ######[new_Col] [new_Col]
    negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]
    print(" negative.dtype",type(negatives[-top:]))

    ax2.set_xlim(negatives.max() * 1.03,negatives.min()* 1.2 )

    ax2.set_title("Negative Impacts ")
    ax2.set_xlabel("Mean Shap value (impact on model output)")
    ax2.set_ylabel("Features")
    ax1.set_title("Positive Impacts ")
    ax1.set_xlabel("Mean Shap value (impact on model output)")
    ax1.set_ylabel("Features")
    fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")

    combined_Frame = (positives[(-top):].append(negatives[(-top):])).to_frame().T 
    print(combined_Frame)
    return(     combined_Frame    )

@transform_pandas(
    Output(rid="ri.vector.main.execute.bbbc3749-9392-48a1-b910-4e4c2318e1d7"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
@output_image_type('svg')
def days_on_comb(derived_cobination):
    derived_cobination = derived_cobination

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    import seaborn as sns
    from matplotlib import cm
    
    # plt.figure(figsize=(10,10)) 
    

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = derived_cobination

    ###
    days =  ['day'+str(x) for x in range(1,29,1) ]
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    print('comb_func(row)')
    new_Col =['comb_'+day for day in days ]

    treat_unique = np.array([ x for x in np.unique(df[new_Col].values) if x is not ''])

    print('unique():', len(treat_unique),treat_unique)

    print( map(set,df[new_Col].astype(str).values) )
    print( list(map(set,df[new_Col].astype(str).values)) )

    items = []
    for dic in list(map(set,df[new_Col].astype(str).values)):
        for item in dic:
            if item is not '' : items.append(item) 
            
    treat_np = np.array([   x.replace('_', ' ').strip().replace(' ',' & ').replace('os & day1 & ','Diagnosis OS ').replace('bmi','BMI').replace('mets','MetS').replace('age','Age').replace('renal','Renal').replace('Q','Q') for x in df[new_Col].astype(str).values.flatten()  if x is not ''  ]) # df[new_Col].astype(str).values.flatten() #   list(map(set,df[new_Col].astype(str).values)) 
    print("treat_np",len(treat_np),type(treat_np))

    treat_df = pd.DataFrame(data=treat_np,columns=["treat_np"]) # {'treat_np': ,index=list(range(len(treat_np)))  ,columns=["treat_np"]
    print(treat_df.head(3))
    # treat_df.columns = treat_df.columns.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    
    fig = plt.figure(figsize=(12,5)) 
    # fig, ax = plt.subplots(figsize=(10, 10))

    percentage = lambda i: len(i) / float(len(treat_df)) * 100

    color_n = cm.Greys(np.linspace(.9, .1, 20 )) 
    ax = sns.countplot(data=treat_df,y ='treat_np',order = treat_df['treat_np'].value_counts().iloc[:10].index, palette= color_n) # sns.barplot(x=x, y=x,  estimator=percentage)
    ax.set(ylabel="Single and Combination Treatments")
    ax.set(xlabel="Days on Treatment")
    
    total = len(treat_df)
    for p in ax.patches:
        percentage = f'{100 * p.get_width() / total:.1f}%\n'
        x = p.get_width()+ 40000
        print(x)
        y = p.get_y() + p.get_height()#/ 2
        ax.annotate(percentage, (x, y), ha='center', va='center')

    # plt.xticks(rotation = 90)
    # plt.tight_layout()
    plt.xlim(0, 690000)
    plt.subplots_adjust(left=0.45)

    SMALL_SIZE = 10
    MEDIUM_SIZE = 10
    BIGGER_SIZE = 10

    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title
    
    plt.show()

    return(treat_df['treat_np'].value_counts().to_frame().T)  

@transform_pandas(
    Output(rid="ri.vector.main.execute.45023671-01ca-49ec-b49c-c9a25e105f17"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def los(derived_cobination):

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    import seaborn as sns
    
    df = derived_cobination
 
    fig = plt.figure(figsize=(15,15)) 
    ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
    ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
    ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)
    
    Col =  'los'
    bin_size =28

    # title='Histogram Of Test Scores',
    # rot=45,
    # grid=True,

    df.los.plot.hist( bins=bin_size, ax=ax0) #column=Col,age
    # sns.displot(data=df,x =Col,ax=ax0).set(title=Col+' Distribution')
    
    df_cp =  df[ (df['os_day28']== 1) ]
    df_cp.los.plot.hist( bins=bin_size, ax=ax1)
    # sns.displot(data=df_cp,x =Col,ax=ax1).set(title=Col+' Survived') 

    df_cp =  df[ (df['os_day28']== 11) ]
    df_cp.los.plot.hist( bins=bin_size, ax=ax2)
    # sns.barplot(data=df_cp,x =Col,ax=ax2).set(title=Col+' Death') 

    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")

    # plt.xlabel('Test Score')
    # plt.ylabel("Number Of Students");

    plt.xticks(rotation = 90)
    plt.tight_layout()
    plt.show()

    return(pd.DataFrame() )

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.0b427f3e-5ed1-4ae1-8748-792f869d01da"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
# from transforms.api import transform, Input, Output
# from foundry_ml import Model
@output_image_type('svg')
def model_trained_copied(derived_cobination, shap_testing_for_explainer, all_treat_new):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        # sm = RandomOverSampler(random_state = 7) 
        # x_train, y_train = sm.fit_sample(x_train, y_train)
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_resample(x_train, y_train) # sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc),# 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    # tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)

    # model = Model(*model stages)
    # model.save(out_model)
    
    ## __plot
    if True: 
        # fig = plt.figure(figsize=(15,15)) 
        # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        # fpr, tpr, threshold = final_results['roc_curve_val']
        # roc_auc = auc(fpr, tpr)
        # ax1.set_title('Receiver Operating Characteristic')
        # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        # ax1.legend(loc = 'lower right')
        # ax1.plot([0, 1], [0, 1],'r--')
        # ax1.set_xlim([0, 1])
        # ax1.set_ylim([0, 1])
        # ax1.set_ylabel('True Positive Rate')
        # ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            # propensity = LogisticRegression()
            # propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            # pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            # print ('pscore[:5]',pscore[:5])
            # xy_overal['Propensity'] = pscore

            # print("\nMatchFrame")
            # g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            # matched_frame = pd.DataFrame()
            # counter =0
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # for index, row in g_death.iterrows():
            #     g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
            #     closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
            #     matched_frame = matched_frame.append(row, ignore_index = True)
            #     matched_frame = matched_frame.append(closest, ignore_index = True)
            #     g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
            #     # if counter ==1000:
            #     #     print(matched_frame[['Propensity','prop_diff']].head(10))
            #     #     break
            #     # counter +=1
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # print('matched_frame.shape',matched_frame.shape)
            # print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_resample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = shap_testing_for_explainer # matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = all_treat_new # explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            shap_values.columns = shap_values.columns.str.replace('dementia', 'Dementia').str.replace('gender_female', 'Female').str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
            ## training = centroids   
            # explainer = shap.KernelExplainer(model.predict_proba, training)
            # explainer = shap.Explainer(model)
            
            ## testing = shap_testing_for_explainer
            ## shap_values = all_treat_new
            # shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            # shap_values = explainer(X)

            # explainer = shap.Explainer(model)
            # shap_values = explainer(X)

              # In v0.20 force_plot now requires the base value as the first parameter
            # https://github.com/slundberg/shap
            # return(pd.DataFrame({'A' : [np.nan]}))
            # shap.plots.waterfall(all_treat_new.values[0])
            fig = plt.figure(figsize=(10,10))
            print("Plot Shap force")
            # https://towardsdatascience.com/tutorial-on-displaying-shap-force-plots-in-python-html-4883aeb0ee7c
            patient_num_Shap = 1456
            shap.plots.force(explainer.expected_value[0],all_treat_new.values[ patient_num_Shap],matplotlib=True,feature_names=all_treat_new.columns,plot_cmap=['#77dd77', '#f99191'])
            plt.show()
            return(pd.DataFrame({'A' : [np.nan]}))

            shap.plots.waterfall(all_treat_new.values[0])

            # explainer = shap.KernelExplainer(svm.predict_proba, X_train, link="logit")
            # shap_values = explainer.shap_values(X_test, nsamples=100)
            # shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link="logit")
            shap.plots.force(all_treat_new.values[0])
            plt.show()
            
            return(pd.DataFrame())
   

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.b4ee3cde-a69d-47ec-b3f8-69d561a45997"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
# from transforms.api import transform, Input, Output
# from foundry_ml import Model
@output_image_type('svg')
def model_trained_copied_1(derived_cobination, shap_testing_for_explainer, all_treat_new):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        # sm = RandomOverSampler(random_state = 7) 
        # x_train, y_train = sm.fit_sample(x_train, y_train)
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_resample(x_train, y_train) # sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc),# 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    # tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)

    # model = Model(*model stages)
    # model.save(out_model)
    
    ## __plot
    if True: 
        # fig = plt.figure(figsize=(15,15)) 
        # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        # fpr, tpr, threshold = final_results['roc_curve_val']
        # roc_auc = auc(fpr, tpr)
        # ax1.set_title('Receiver Operating Characteristic')
        # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        # ax1.legend(loc = 'lower right')
        # ax1.plot([0, 1], [0, 1],'r--')
        # ax1.set_xlim([0, 1])
        # ax1.set_ylim([0, 1])
        # ax1.set_ylabel('True Positive Rate')
        # ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            # propensity = LogisticRegression()
            # propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            # pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            # print ('pscore[:5]',pscore[:5])
            # xy_overal['Propensity'] = pscore

            # print("\nMatchFrame")
            # g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            # matched_frame = pd.DataFrame()
            # counter =0
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # for index, row in g_death.iterrows():
            #     g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
            #     closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
            #     matched_frame = matched_frame.append(row, ignore_index = True)
            #     matched_frame = matched_frame.append(closest, ignore_index = True)
            #     g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
            #     # if counter ==1000:
            #     #     print(matched_frame[['Propensity','prop_diff']].head(10))
            #     #     break
            #     # counter +=1
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # print('matched_frame.shape',matched_frame.shape)
            # print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_resample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = shap_testing_for_explainer # matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = all_treat_new # explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            shap_values.columns = shap_values.columns.str.replace('dementia', 'Dementia').str.replace('gender_female', 'Female').str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
            ## training = centroids   
            # explainer = shap.KernelExplainer(model.predict_proba, training)
            # explainer = shap.Explainer(model)
            
            ## testing = shap_testing_for_explainer
            ## shap_values = all_treat_new
            # shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            # shap_values = explainer(X)

            # explainer = shap.Explainer(model)
            # shap_values = explainer(X)

              # In v0.20 force_plot now requires the base value as the first parameter
            # https://github.com/slundberg/shap
            # return(pd.DataFrame({'A' : [np.nan]}))
            # shap.plots.waterfall(all_treat_new.values[0])
            fig = plt.figure(figsize=(10,10))
            print("Plot Shap force")
            # https://towardsdatascience.com/tutorial-on-displaying-shap-force-plots-in-python-html-4883aeb0ee7c
            patient_num_Shap = 156
            shap.plots.force(explainer.expected_value[0],all_treat_new.values[ patient_num_Shap],matplotlib=True,feature_names=all_treat_new.columns,plot_cmap=['#77dd77', '#f99191'])
            plt.show()
            return(pd.DataFrame({'A' : [np.nan]}))

            shap.plots.waterfall(all_treat_new.values[0])

            # explainer = shap.KernelExplainer(svm.predict_proba, X_train, link="logit")
            # shap_values = explainer.shap_values(X_test, nsamples=100)
            # shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link="logit")
            shap.plots.force(all_treat_new.values[0])
            plt.show()
            
            return(pd.DataFrame())
   

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.81e22892-8fd9-4028-aa17-97089c54bbb1"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
# from transforms.api import transform, Input, Output
# from foundry_ml import Model
@output_image_type('svg')
def model_trained_copied_copied(derived_cobination, shap_testing_for_explainer, all_treat_new):
    all_treat_new = all_treat_new

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc),# 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)#10
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    # tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)

    # model = Model(*model stages)
    # model.save(out_model)
    
    ## __plot
    if True: 
        # fig = plt.figure(figsize=(15,15)) 
        # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        # fpr, tpr, threshold = final_results['roc_curve_val']
        # roc_auc = auc(fpr, tpr)
        # ax1.set_title('Receiver Operating Characteristic')
        # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        # ax1.legend(loc = 'lower right')
        # ax1.plot([0, 1], [0, 1],'r--')
        # ax1.set_xlim([0, 1])
        # ax1.set_ylim([0, 1])
        # ax1.set_ylabel('True Positive Rate')
        # ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            # propensity = LogisticRegression()
            # propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            # pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            # print ('pscore[:5]',pscore[:5])
            # xy_overal['Propensity'] = pscore

            # print("\nMatchFrame")
            # g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            # matched_frame = pd.DataFrame()
            # counter =0
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # for index, row in g_death.iterrows():
            #     g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
            #     closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
            #     matched_frame = matched_frame.append(row, ignore_index = True)
            #     matched_frame = matched_frame.append(closest, ignore_index = True)
            #     g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
            #     # if counter ==1000:
            #     #     print(matched_frame[['Propensity','prop_diff']].head(10))
            #     #     break
            #     # counter +=1
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # print('matched_frame.shape',matched_frame.shape)
            # print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = shap_testing_for_explainer # matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = all_treat_new # explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            shap_values.columns = shap_values.columns.str.replace('dementia', 'Dementia').str.replace('gender_female', 'Female').str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
            ## training = centroids   
            # explainer = shap.KernelExplainer(model.predict_proba, training)
            # explainer = shap.Explainer(model)
            
            ## testing = shap_testing_for_explainer
            ## shap_values = all_treat_new
            # shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            # shap_values = explainer(X)

            # explainer = shap.Explainer(model)
            # shap_values = explainer(X)

              # In v0.20 force_plot now requires the base value as the first parameter
            # https://github.com/slundberg/shap
            # return(pd.DataFrame({'A' : [np.nan]}))
            # shap.plots.waterfall(all_treat_new.values[0])
            fig = plt.figure(figsize=(10,10))
            print("Plot Shap force")
            # https://towardsdatascience.com/tutorial-on-displaying-shap-force-plots-in-python-html-4883aeb0ee7c
            patient_num_Shap = 2220
            shap.initjs()
            f = shap.plots.force(explainer.expected_value[0],all_treat_new.values[ patient_num_Shap],feature_names=all_treat_new.columns,plot_cmap=["#00008b","#990000"], show=True,matplotlib=True) # ['#f99191','#77dd77']) # ,matplotlib=True # ,plot_cmap=["#00008b","#990000"],matplotlib=True
            # from matplotlib import pyplot as pl
            # f = plt.gcf()
            # plt.savefig('scratch.png')
            plt.show()
            return(pd.DataFrame({'A' : [np.nan]}))

            shap.plots.waterfall(all_treat_new.values[0])

            # explainer = shap.KernelExplainer(svm.predict_proba, X_train, link="logit")
            # shap_values = explainer.shap_values(X_test, nsamples=100)
            # shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link="logit")
            shap.plots.force(all_treat_new.values[0])
            plt.show()
            
            return(pd.DataFrame())
   

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.5afd7b95-9408-4518-aaca-998360216c86"),
    all_treat_new_copied_20q1q2=Input(rid="ri.foundry.main.dataset.533e5b12-9b39-4359-a3b4-050558d8e5f8")
)
@output_image_type('svg')
def name_it(all_treat_new_copied_20q1q2):
    all_treat_new = all_treat_new_copied_20q1q2

    top=10

    treatments =  ['BiotMQ_', 'BiotMQ_Coag_', 'BiotMQ_Coag_Misc_', 'BiotMQ_Coag_Misc_MonoI_', 'BiotMQ_Coag_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Misc_MonoSP_', 'BiotMQ_Coag_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Misc_ViraTrgt_', 'BiotMQ_Coag_MonoI_', 'BiotMQ_Coag_MonoI_ViraTrgt_', 'BiotMQ_Coag_MonoSP_', 'BiotMQ_Coag_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_', 'BiotMQ_Coag_Ster_Misc_', 'BiotMQ_Coag_Ster_Misc_MonoI_', 'BiotMQ_Coag_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_MonoSP_', 'BiotMQ_Coag_Ster_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoI_', 'BiotMQ_Coag_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoSP_', 'BiotMQ_Coag_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_', 'BiotMQ_Coag_ViraUnp_Misc_', 'BiotMQ_Coag_ViraUnp_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoI_', 'BiotMQ_Coag_ViraUnp_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoSP_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_ViraTrgt_', 'BiotMQ_Misc_', 'BiotMQ_Misc_MonoSP_', 'BiotMQ_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Misc_ViraTrgt_', 'BiotMQ_MonoI_', 'BiotMQ_MonoI_ViraTrgt_', 'BiotMQ_MonoSP_', 'BiotMQ_Ster_', 'BiotMQ_Ster_Misc_', 'BiotMQ_Ster_Misc_MonoI_', 'BiotMQ_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Ster_Misc_MonoSP_', 'BiotMQ_Ster_Misc_ViraTrgt_', 'BiotMQ_Ster_MonoI_', 'BiotMQ_Ster_MonoI_ViraTrgt_', 'BiotMQ_Ster_MonoSP_', 'BiotMQ_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Ster_ViraTrgt_', 'BiotMQ_ViraTrgt_', 'BiotMQ_ViraUnp_', 'BiotMQ_ViraUnp_Misc_', 'BiotMQ_ViraUnp_Misc_MonoI_', 'BiotMQ_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_MonoI_', 'BiotMQ_ViraUnp_Ster_', 'BiotMQ_ViraUnp_Ster_Misc_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoI_', 'BiotMQ_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoSP_', 'BiotMQ_ViraUnp_Ster_MonoSP_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_ViraUnp_ViraTrgt_', 'Coag_', 'Coag_Misc_', 'Coag_Misc_MonoI_', 'Coag_Misc_MonoI_ViraTrgt_', 'Coag_Misc_MonoSP_', 'Coag_Misc_MonoSP_ViraTrgt_', 'Coag_Misc_ViraTrgt_', 'Coag_MonoI_', 'Coag_MonoI_ViraTrgt_', 'Coag_MonoSP_', 'Coag_MonoSP_ViraTrgt_', 'Coag_Ster_', 'Coag_Ster_Misc_', 'Coag_Ster_Misc_MonoI_', 'Coag_Ster_Misc_MonoI_ViraTrgt_', 'Coag_Ster_Misc_MonoSP_', 'Coag_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_Ster_Misc_ViraTrgt_', 'Coag_Ster_MonoI_', 'Coag_Ster_MonoI_ViraTrgt_', 'Coag_Ster_MonoSP_', 'Coag_Ster_MonoSP_ViraTrgt_', 'Coag_Ster_ViraTrgt_', 'Coag_ViraTrgt_', 'Coag_ViraUnp_', 'Coag_ViraUnp_Misc_', 'Coag_ViraUnp_Misc_MonoI_', 'Coag_ViraUnp_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Misc_MonoSP_', 'Coag_ViraUnp_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Misc_ViraTrgt_', 'Coag_ViraUnp_MonoI_', 'Coag_ViraUnp_MonoI_ViraTrgt_', 'Coag_ViraUnp_MonoSP_', 'Coag_ViraUnp_Ster_', 'Coag_ViraUnp_Ster_Misc_', 'Coag_ViraUnp_Ster_Misc_MonoI_', 'Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_MonoSP_', 'Coag_ViraUnp_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoI_', 'Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoSP_', 'Coag_ViraUnp_Ster_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_ViraTrgt_', 'Coag_ViraUnp_ViraTrgt_', 'Misc_', 'Misc_MonoI_', 'Misc_MonoI_ViraTrgt_', 'Misc_MonoSP_', 'Misc_MonoSP_ViraTrgt_', 'Misc_ViraTrgt_', 'MonoI_', 'MonoI_ViraTrgt_', 'MonoSP_', 'MonoSP_ViraTrgt_', 'Ster_', 'Ster_Misc_', 'Ster_Misc_MonoI_', 'Ster_Misc_MonoI_ViraTrgt_', 'Ster_Misc_MonoSP_', 'Ster_Misc_MonoSP_ViraTrgt_', 'Ster_Misc_ViraTrgt_', 'Ster_MonoI_', 'Ster_MonoI_ViraTrgt_', 'Ster_MonoSP_', 'Ster_MonoSP_MonoI_', 'Ster_MonoSP_ViraTrgt_', 'Ster_ViraTrgt_', 'ViraTrgt_', 'ViraUnp_', 'ViraUnp_Misc_', 'ViraUnp_Misc_ViraTrgt_', 'ViraUnp_MonoI_', 'ViraUnp_MonoSP_', 'ViraUnp_Ster_', 'ViraUnp_Ster_Misc_', 'ViraUnp_Ster_Misc_MonoI_', 'ViraUnp_Ster_Misc_MonoSP_', 'ViraUnp_Ster_Misc_ViraTrgt_', 'ViraUnp_Ster_MonoI_', 'ViraUnp_Ster_MonoI_ViraTrgt_', 'ViraUnp_Ster_MonoSP_', 'ViraUnp_Ster_MonoSP_ViraTrgt_', 'ViraUnp_Ster_ViraTrgt_', 'ViraUnp_ViraTrgt_']

    all_treat_new = all_treat_new[treatments]
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns # pts_col_no_treat +   treat_unique

    # for drug, b_name in zip(['Steroid Preparations','Anticoagulants','Antiviral Agents','Miscellaneous Antibiotics','Miscellaneous','Monoclonal Antibodies'],
    #                           ['Ster',                'Coag',             'Vira',             'Biot',                     'Misc',         'Mono']):

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    # pts_col = [ x.replace('Ster','Steroid').replace('Coag','Anticoagulants').replace('Vira','Antiviral').replace('Biot','Mis.Antibiotics').replace('Misc','Miscellaneous').replace('Mono','Monoclonal') for x in pts_col]

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]

    fig = plt.figure(figsize=(10,4)) 
    fig.subplots_adjust(left=0.3)
    ax1 = plt.subplot2grid((2,2),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((2,2),(0,1), rowspan = 2,colspan = 1)

    positives = overall_importance_df[overall_importance_df>0].sort_values(ascending=True)
    negatives = overall_importance_df[overall_importance_df<=0].sort_values(ascending=False)

    positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, top )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, top )) # .9 positives.size

    positives[-top:].plot.barh( ax=ax1,color=color_p) # yerr=std[:24], ######[new_Col] [new_Col]
    negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]

    ax2.set_xlim(negatives.max() * 1.03,negatives.min()* 1.2 )

    ax2.set_title("Negative Impacts ")
    ax2.set_xlabel("Mean Shap value (impact on model output)")
    ax2.set_ylabel("Features")
    ax1.set_title("Positive Impacts ")
    ax1.set_xlabel("Mean Shap value (impact on model output)")
    ax1.set_ylabel("Features")
    fig.tight_layout()
    plt.tight_layout()
    
    for pat in [ax1,ax2]:
        for p in pat.patches:
            percentage = f'{p.get_width():.1f}%\n'
            x = p.get_width()+ 40000
            print(x)
            y = p.get_y() + p.get_height()#/ 2
            pat.annotate(percentage, (x, y), ha='center', va='center')

    # for p in ax.patches:
    #     percentage = f'{100 * p.get_width() / total:.1f}%\n'
    #     x = p.get_width()+ 40000
    #     print(x)
    #     y = p.get_y() + p.get_height()#/ 2
    #     ax.annotate(percentage, (x, y), ha='center', va='center')

    plt.show()
    print("show")
    # return(all_treat_new)
    combined_Frame = (positives[(-top):].append(negatives[(-top):])).to_frame().T 
    print(combined_Frame)
    return(     combined_Frame    )

@transform_pandas(
    Output(rid="ri.vector.main.execute.64a0a003-aac0-4be3-a9a9-2778389a9e98"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def os_count(derived_cobination):
    derived_cobination = derived_cobination

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    import seaborn as sns
    
    df = derived_cobination
 
    fig = plt.figure(figsize=(7,7)) 
    ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
    ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
    ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)
    
    Col =  'age'
    bin_size =[1,3,5,7,9,11]

    # title='Histogram Of Test Scores',
    # rot=45,
    # grid=True,

    df.os_day1.value_counts().plot(kind='bar', ax=ax0, sort_columns=True) #.plot.hist( bins=bin_size, ax=ax0) #column=Col,age
    # sns.displot(data=df,x =Col,ax=ax0).set(title=Col+' Distribution')
    
    df['max3'] = df[['os_day1','os_day2','os_day3']].values.max(1)
    # df_cp =  df[ (df['os_day28']== 1) ]
    df.max3.value_counts().plot(kind='bar', ax=ax1, sort_columns=True) #.plot.hist( bins=bin_size, ax=ax1) # .value_counts().plot(kind='bar', ax=ax1) #
    # sns.displot(data=df_cp,x =Col,ax=ax1).set(title=Col+' Survived') 

    df['max5'] = df[['os_day1','os_day2','os_day3','os_day4','os_day5']].values.max(1)
    # df_cp =  df[ (df['os_day28']== 1) ]
    df.max5.value_counts().plot(kind='bar', ax=ax2, sort_columns=True) #.plot.hist( bins=bin_size, ax=ax2)
    # sns.barplot(data=df_cp,x =Col,ax=ax2).set(title=Col+' Death') 

    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")
    for all_axes  in [ax0,ax1,ax2]:
        for p in all_axes.patches:
            all_axes.annotate(str(p.get_height()), (p.get_x() * 1.005, p.get_height() * 1.005))
            
    # plt.xlabel('Test Score')
    # plt.ylabel("Number Of Students");

    plt.xticks( ) # rotation = 90
    plt.tight_layout()
    plt.show()

    return(pd.DataFrame() )

@transform_pandas(
    Output(rid="ri.vector.main.execute.1162a4c7-9b22-46fb-82e7-e945b0196b75"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
@output_image_type('svg')
def patient_on_combination(derived_cobination):
    derived_cobination = derived_cobination

    import pandas as pd
    import numpy as np
    import matplotlib.pyplot as plt
    from datetime import datetime
    import seaborn as sns
    from matplotlib import cm
    
    # plt.figure(figsize=(10,10)) 
    

    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    
    df = derived_cobination

    ###
    days =  ['day'+str(x) for x in range(1,29,1) ]
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    print('comb_func(row)')
    new_Col =['comb_'+day for day in days ]

    treat_unique = np.array([ x for x in np.unique(df[new_Col].values) if x is not ''])

    print('unique():', len(treat_unique),treat_unique)

    print( map(set,df[new_Col].astype(str).values) )
    print( list(map(set,df[new_Col].astype(str).values)) )

    items = []
    for dic in list(map(set,df[new_Col].astype(str).values)):
        for item in dic:
            if item is not '' : items.append(item) 
            
    treat_np = np.array([   x.replace('_', ' ').strip().replace(' ',' & ').replace('os & day1 & ','Diagnosis OS ').replace('bmi','BMI').replace('mets','MetS').replace('age','Age').replace('renal','Renal').replace('Q','Q') for x in items  if x is not ''  ]) # df[new_Col].astype(str).values.flatten() #   list(map(set,df[new_Col].astype(str).values)) 
    print("treat_np",len(treat_np),type(treat_np))

    treat_df = pd.DataFrame(data=treat_np,columns=["treat_np"]) # {'treat_np': ,index=list(range(len(treat_np)))  ,columns=["treat_np"]
    print(treat_df.head(3))
    # treat_df.columns = treat_df.columns.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    
    fig = plt.figure(figsize=(12,5)) 
    # fig, ax = plt.subplots(figsize=(10, 10))

    percentage = lambda i: len(i) / float(len(treat_df)) * 100

    color_n = cm.Greys(np.linspace(.9, .1, 20 )) 
    ax = sns.countplot(data=treat_df,y ='treat_np',order = treat_df['treat_np'].value_counts().iloc[:10].index, palette= color_n) # sns.barplot(x=x, y=x,  estimator=percentage)
    ax.set(ylabel="Single and Combination Treatments")
    ax.set(xlabel="Patients on Treatment")
    
    total = len(treat_df)
    for p in ax.patches:
        percentage = f'{100 * p.get_width() / total:.1f}%\n'
        x = p.get_width()+ 10000
        print(x)
        y = p.get_y() + p.get_height()#/ 2
        ax.annotate(percentage, (x, y), ha='center', va='center')

    # plt.xticks(rotation = 90)
    # plt.tight_layout()
    plt.xlim(0, 100000)
    plt.subplots_adjust(left=0.45)

    SMALL_SIZE = 10
    MEDIUM_SIZE = 10
    BIGGER_SIZE = 10

    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title
    
    plt.show()

    return(treat_df['treat_np'].value_counts().to_frame().T)  

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.861bcd4e-3156-4f98-b7fb-342a61fa8227"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
@output_image_type('svg')
def roc_l1_cnn_1(derived_cobination):
    derived_cobination = derived_cobination

    # https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html

    
    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    # from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    from sklearn.model_selection import StratifiedKFold
    from sklearn.model_selection import KFold
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    model_name = 'full'
    # model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    # pts =  pts[   (pts['os_day1']== 1)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1',
                'pressor_days', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    tprs = []
    aucs = []
    f1= []
    rec= []
    per=[]
    c1=[]
    c0=[] #
    f101=[] #.append(final_results['f101'])
    rec01=[] #.append(final_results['rec01'])
    per01=[] #.append(final_results['pre01'])
    mean_fpr = np.linspace(0, 1, 100)
    fig, ax = plt.subplots(figsize=(5,5), dpi=600)
    # fig = plt.figure(figsize=(5,5))
    ### SPLIT
    print('\nsplit')
    counter_fold = 0
    # kf = KFold(n_splits=20)
    pts=pts.sample(frac=1, random_state=9)

    skf = StratifiedKFold(n_splits=2) #1000 #, random_state=7, # Stratified # , random_state=9
    # skf.split(X, y):
    for train_index, test_index in skf.split(pts[pts_col ],pts[outcome]): # 
        counter_fold+=1
        print("\n\n\n#####################################\n FOLD ",counter_fold,"\n#################################\n")
        # x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
        x_train= pts.iloc[train_index][pts_col ]
        x_test= pts.iloc[test_index][pts_col ]
        y_train= pts.iloc[train_index][outcome ]
        y_test= pts.iloc[test_index][outcome ]
        
        # y_train = pd.DataFrame(data=y_train,columns=outcome)
        # x_train = pd.DataFrame(data=x_train,columns=pts_col)

        print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
        print('onehot y_train')
        print(onehot.sum(axis=0))
    
        ### Class weight
        print("\nClass weight")
        num_classes = len(pts[outcome[0]].unique())
        if model_type == 'binary' or model_type == 'regression':
            num_classes = 1
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
        class_counts = onehot.sum(axis=0).values
        total_count = sum(class_counts)
        class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
        class_weights = dict(enumerate(class_rate))
        print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

        print("\nscale")
        scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
        #  'age',
        print(scale) 
        scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
        x_train.loc[:,scale] = scale_x.transform(x_train[scale])
        x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
        
        original_x_train = x_train.copy()
        original_y_train = y_train.copy()
        original_x_test = x_test.copy()
        original_y_test = y_test.copy()
        ### SMOTE
        if resample:
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7) 
            x_train, y_train = sm.fit_resample(x_train, y_train) # sm.fit_sample(x_train, y_train)
            print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
            x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
            y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
            for c in range(num_classes):
                class_weights[c]=1
            print('class weights reseted: ',class_weights)
        
        ### split learn/val
        print('\nsplit learn/val')
        x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
        print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

        print('\nonehot train/test/learn/val')
        if model_type == 'classification' :
            y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_train.sum(axis=0))
            y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_test.sum(axis=0))
            y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_learn.sum(axis=0))
            y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_val.sum(axis=0))

        ### Hyper Param 
        ######################################
        param_grid = []
        if optimize == True:
            neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
            layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
            drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
            batchNorm = hp.choice( 'batchNorm',[True, False] )
            activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
            learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
            epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
            batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

            param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                    drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                    learning_ratio=learning_ratio, # compile
                    epochs=epochs, batch_size=batch_size ) # fit           

            if model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
    
        ### Functions ### 
        ######################################
        print("Model_name full cnn")
        def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            print('TEST__ param_grid:', param_grid)

            # Model
            print("Conv1D")
            neuron_comb =  param_grid['neuron_comb']
            layers_comb = param_grid['layers_comb']
            drop_rate =  param_grid['drop_rate'] 
            batchNorm = param_grid['batchNorm'] 
            activation_out = param_grid['activation_out']
            # compile
            learning_ratio = param_grid['learning_ratio']
            # fit
            epochs = param_grid['epochs']
            batch_size= param_grid['batch_size']
        
            ## model__fn
            inputA = Input(shape=(demo_shape,1))
            if final_model:
                print('\nFinal_model inputA.shape',inputA.shape,demo_shape)
            print('\nFinal_model inputA.shape',inputA.shape,demo_shape)
    
            ############################################################################################
            num_filters = 128
            conv_num = "1x7-base"
            conv = Conv1D(num_filters , kernel_size=7, strides=1, padding="same", name=("Conv-" + conv_num))(inputA)  # valid # orig (1,3) # larger (5,5) (3,3)
            if batchNorm: conv = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
            act = Activation("relu", name=("Act-" + conv_num))(conv)
            base = MaxPooling1D(pool_size=2, name=("Pooling-" + conv_num))(act)  # MaxPooling1D

            conv_num = "1x3-1-64"
            conv = Conv1D(num_filters, kernel_size=3, strides=1, padding="same", name=("Conv-" + conv_num))(base)
            if batchNorm: conv = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
            act = Activation("relu", name=("Act-" + conv_num))(conv)

            conv_num = "1x3-2-64"
            conv = Conv1D(num_filters, kernel_size=3, strides=1, padding="same", name=("Conv-" + conv_num))(act)
            if batchNorm: conv = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
            act = Activation("relu", name=("Act-" + conv_num))(conv)
            block = keras.layers.Add(name="add-"+conv_num)([base,act])

            conv_num = "64to128"
            feature_increase = Conv1D(num_filters * 2, kernel_size=1, strides=2, padding="same", name=("Conv-" + conv_num))(block)

            conv_num = "1x3-1-128"
            conv = Conv1D(num_filters*2, kernel_size=3, strides=2, padding="same", name=("Conv-" + conv_num))(block)
            if batchNorm: conv  = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
            act = Activation("relu", name=("Act-" + conv_num))(conv)

            conv_num = "1x3-2-128"
            conv = Conv1D(num_filters*2, kernel_size=3, strides=1, padding="same", name=("Conv-" + conv_num))(act)
            if batchNorm: conv  = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
            act = Activation("relu", name=("Act-" + conv_num))(conv)
            block = keras.layers.Add(name="add-"+conv_num)([feature_increase, act])

            conv_num = "128to256"
            feature_increase = Conv1D(num_filters * 2*2, kernel_size=1, strides=2, padding="same", name=("Conv-" + conv_num))(block)

            conv_num = "3x3-1-256"
            conv = Conv1D(num_filters * 2* 2, kernel_size=3, strides=2, padding="same", name=("Conv-" + conv_num))(block)
            if batchNorm: conv  = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
            act = Activation("relu", name=("Act-" + conv_num))(conv)

            conv_num = "3x3-2-256"
            conv = Conv1D(num_filters * 2* 2, kernel_size=3, strides=1, padding="same", name=("Conv-" + conv_num))(act)
            if batchNorm: conv  = BatchNormalization(axis=1, name=("BatchNorm-" + conv_num))(conv)  # for Relu
            act = Activation("relu", name=("Act-" + conv_num))(conv)
            block = keras.layers.Add(name="add-"+conv_num)([feature_increase, act])

            flat9 = Flatten()(block)

            # combined = concatenate([inputAA, flat9 ]) # flat1, flat2, flat3, flat4, flat5, flat6
            # combined outputs
            fully = Dense(neuron_comb, activation="relu", name="Fully-con-1")(flat9)
            fully = Dropout(drop_rate, name="DropF-1")(fully)
            for l in range(2, layers_comb + 1, 1):
                fully = Dense( (neuron_comb/(2**(l-1))), activation="relu", name="Fully-con-" + str(l))(fully)
                fully = Dropout(drop_rate, name="DropF-" + str(l))(fully)

            # initializer = []
            # if activation_out == 'softmax' or activation_out == 'sigmoid': initializer = tf.keras.initializers.HeNormal() # or uniform
            # else: initializer = tf.keras.initializers.GlorotNormal()

            out = Dense(2, activation=activation_out, name="active-Out-sig/soft")(fully)
            # sigmoid (original) # tanh (-1 1 binary binary) # softmax (cat-cross) prob-distribution # relu #? PReLU - Leaky ReLU -
            ############################################################################################
            model = Model(inputs=inputA, outputs=out)

            if final_model:
                model.summary()

            loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
            metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
            if model_type == 'binary':
                loss = 'binary_crossentropy'
            if model_type == 'regression':
                loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
                metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
            model.compile(loss = loss,
                        metrics=metrics,
                        optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

            es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
            if model_type == 'regression':
                es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
                mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

            history = []
            if model_type == 'classification':
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values), 
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc],
                                    class_weight=class_weights
                                    )
                model = load_model('best_model.h5')
            else:
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values),
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc]
                                )
                model = load_model('best_model.h5')

            pred_prob = model.predict( x_val[pts_col].values)
            if np.isnan(pred_prob).any():
                print("ERROR np.nan_to_num(pred_prob)")
                pred_prob = np.nan_to_num(pred_prob)
            predictions = pred_prob.argmax(axis=1)
            if model_type == 'binary':
                fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
                gmeans = np.sqrt(tpr * (1-fpr))
                ix = np.argmax(gmeans)
                print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
                predictions = (pred_prob >thresholds[ix]).astype(np.int) 
            if model_type == 'regression':
                predictions = model.predict( x_val[pts_col].values)
            y_val_binary = y_val.copy()
            if model_type == 'classification':
                y_val = y_val.values.argmax(axis=1)
        
            ### for ploting send out
            print('history.history.keys()',history.history.keys())
            train_acc=[]
            val_acc=[]
            if model_type == 'classification' or model_type == 'binary':
                train_acc = history.history['auc'] 
                val_acc = history.history['val_auc']
            elif model_type == 'regression':
                train_acc = history.history['mape_metric']
                val_acc = history.history['val_mape_metric']
            else:
                print("Error model type")
                return(0)
            train_loss = history.history['loss']
            val_loss = history.history['val_loss']
            epochs = range(len(train_acc))
        
            if model_type == 'regression' :
                y_val = y_val_binary
                actual, pred = np.array(y_val), np.array(predictions)
                mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
                mape_acc = 100 - mape_error 
                results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                            'accuracy_score_val': mape_acc, 
                            'train_acc':train_acc, 'val_acc':val_acc, 
                            'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                        }
                print("TEST__ accuracy_score:mape_acc ", mape_acc)
                print("datetime.now()",datetime.now())
                return (results)

            if model_type == 'binary' or model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)
                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)

                f101 = f1_score(y_val, predictions, average=None)
                pre01 = precision_score(y_val,predictions, average=None)
                rec01 = recall_score(y_val, predictions, average=None)
            
                class_1= 0
                class_0 = 0
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    if class_ == 0:  class_0 = class_accuracy
                    if class_ == 1:  class_1 = class_accuracy
                    print('TEST__ confusion:',class_str,class_accuracy)
    

                results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,"f101":f101, "rec01":rec01, "pre01":pre01, 'class_0':class_0, "class_1":class_1,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
        ######################################
        print("Model_name rf")
        def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            
            ## model__rf
            # model = RandomForestClassifier(random_state=7)
            model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000) #1000 __trees __ft
            # model = sk.neighbors.KNeighborsClassifier()
            # model = sk.svm.SVC()

            model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
            predictions = model.predict(x_val[pts_col].values)
            pred_prob = model.predict_proba(x_val[pts_col].values)
            y_val_binary = y_val
            y_val = y_val.values.argmax(axis=1)
            
            ### for ploting send out
            train_acc=0
            val_acc=0
            train_loss = 0
            val_loss = 0
            epochs = 0
        
            if model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)

                f101 = f1_score(y_val, predictions, average=None)
                pre01 = precision_score(y_val,predictions, average=None)
                rec01 = recall_score(y_val, predictions, average=None)

                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
                class_1= 0
                class_0 = 0
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    if class_ == 0:  class_0 = class_accuracy
                    if class_ == 1:  class_1 = class_accuracy
                    print('TEST__ confusion:',class_str,class_accuracy)
    

                results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val,  'class_0':class_0, "class_1":class_1, "f101":f101, "rec01":rec01, "pre01":pre01,
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
            else:
                print("\nwrong model type")
                return(0)
        ######################################
        print("Results #############################")
        ### trials 
        # tpe_trials = Trials()
        tpe_best = []
        if optimize:
            if model_name == 'full':
                tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
            elif model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
            else:
                print("Error optimize: model not found")
                return(0)
            param_grid= space_eval(param_grid, tpe_best)
            print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
            print("datetime.now(): ", datetime.now())
        else: # __param
            if model_name == 'full':
                param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 32, 'drop_rate': 0.3, 'epochs': 40, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 900}
                param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 64, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 800}
                if max_level == 3:
                    param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600}
                elif max_level == 5:
                    param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 64, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 400}
                elif max_level == 7:
                    param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 16, 'drop_rate': 0.5, 'epochs': 40, 'layers_comb': 1, 'learning_ratio': 0.0001, 'neuron_comb': 300}
                elif max_level == 9:
                    param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 32, 'drop_rate': 0.3, 'epochs': 40, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300}

                if model_type == 'binary':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
                if model_type == 'regression':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif model_name == 'rf':
                print("rf not optimize")
            else:
                print("Error optimize: model not found")
                return(0)

        print("\nFinal Model")
        final_results = []
        final_model =  True
        # __call
        if model_name == 'full':
            final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
        if model_name == 'rf':
            final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
        
        ## __plot
        if True: 
            # fig = plt.figure(figsize=(5,5)) 
            # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
            # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
            # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

            # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
            # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
            # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
            # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
            # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
            # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

            #  f1= []
            # rec= []
            # per=[]
            # 'f1_score_val': f1_score_val, 
            # 'precision_score_val': precision_score_val, 
            # 'recall_score_val': recall_score_val,

            f101.append(final_results['f101'])
            rec01.append(final_results['rec01'])
            per01.append(final_results['pre01'])
            
            f1.append(final_results['f1_score_val'])
            rec.append(final_results['recall_score_val'])
            per.append(final_results['precision_score_val'])
            c1.append(final_results['class_1'])
            c0.append(final_results['class_0'])

            fpr, tpr, threshold = final_results['roc_curve_val']
            roc_auc = auc(fpr, tpr)
            # ax1.set_title('Receiver Operating Characteristic')
            # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
            # ax1.legend(loc = 'lower right')
            # ax1.plot([0, 1], [0, 1],'r--')
            # ax1.set_xlim([0, 1])
            # ax1.set_ylim([0, 1])
            # ax1.set_ylabel('True Positive Rate')
            # ax1.set_xlabel('False Positive Rate')

            interp_tpr = np.interp(mean_fpr, fpr, tpr)
            interp_tpr[0] = 0.0
            tprs.append(interp_tpr)
            aucs.append(roc_auc)
            if counter_fold==1: break

    # __ft
    
    ax.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)

    print("f1:",np.mean(f1))
    print("per:",np.mean(per))
    print("rec:",np.mean(rec))

    print("f101:",np.mean(f101, axis=0))
    print("per01:",np.mean(per01, axis=0))
    print("rec01:",np.mean(rec01, axis=0))

    print("class_1",np.mean(c1))
    print("class_0",np.mean(c0))
    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs)
    print("std_auc: ",std_auc)
    ax.plot( #1000
        mean_fpr,
        mean_tpr,
        color="b",
        label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, 0.01), # std_auc 
        # label=r"Mean ROC (AUC = %0.2f)" % (mean_auc),

        lw=2,
        alpha=0.8,
    )
    count_ratio = 0
    colors_sns = sns.color_palette("tab10")
    for t in tprs:
        count_ratio+=1
        ax.plot(
        mean_fpr,
        t,
        color=colors_sns[count_ratio-1],
        # label=r"Fold %f (AUC = %0.2f $\pm$ %0.2f)" % (count_ratio,mean_auc, std_auc),
        # label=r"Fold %f (AUC = %0.2f $\pm$ %0.2f)" % (count_ratio,mean_auc, std_auc),
        lw=1,
        alpha=0.2,
        )

    std_tpr = np.std(tprs, axis=0)
    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr, 0) # tpr*1.5
    ax.fill_between(
        mean_fpr,
        tprs_lower,
        tprs_upper,
        color="grey",
        alpha=0.4,
        label=r"$\pm$ 1 std. dev.",
    )

    ax.set(
        xlim=[-0.05, 1.05],
        ylim=[-0.05, 1.05],
        title="Receiver Operating Characteristic (ROC)",
    )
    ax.legend(loc="lower right")
    ax.set(xlabel='False Positive Rate')
    ax.set(ylabel='True Positive Rate')

    SMALL_SIZE = 8
    MEDIUM_SIZE = 10
    BIGGER_SIZE = 12

    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

    plt.show()

    print("after KFold for : datetime.now(): ", datetime.now())
    # return( pd.DataFrame( {'return' : [0]} ) ) 
    return(pd.DataFrame({'mean_tpr': mean_tpr, 'mean_fpr': mean_fpr}) )

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.46e53ad0-a8b7-4df2-81c3-25691f61b4d3"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
@output_image_type('svg')
def roc_l1_nn(derived_cobination, roc_l9):
    derived_cobination = derived_cobination

    # https://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html

    
    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    # from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    from sklearn.model_selection import StratifiedKFold
    from sklearn.model_selection import KFold
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    model_name = 'full'
    # model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    # pts =  pts[   (pts['os_day1']== 1)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                'pressor_days',
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    tprs = []
    aucs = []
    f1= []
    rec= []
    per=[]
    c1=[]
    c0=[] #
    f101=[] #.append(final_results['f101'])
    rec01=[] #.append(final_results['rec01'])
    per01=[] #.append(final_results['pre01'])
    mean_fpr = np.linspace(0, 1, 100)
    fig, ax = plt.subplots(figsize=(5,5), dpi=600)
    # fig = plt.figure(figsize=(5,5))
    ### SPLIT
    print('\nsplit')
    counter_fold = 0
    # kf = KFold(n_splits=20)
    pts=pts.sample(frac=1)

    skf = StratifiedKFold(n_splits=5, random_state=9) #1000 #, random_state=7, # Stratified
    # skf.split(X, y):
    for train_index, test_index in skf.split(pts[pts_col ],pts[outcome]): # 
        counter_fold+=1
        print("\n\n\n#####################################\n FOLD ",counter_fold,"\n#################################\n")
        # x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
        x_train= pts.iloc[train_index][pts_col ]
        x_test= pts.iloc[test_index][pts_col ]
        y_train= pts.iloc[train_index][outcome ]
        y_test= pts.iloc[test_index][outcome ]
        
        # y_train = pd.DataFrame(data=y_train,columns=outcome)
        # x_train = pd.DataFrame(data=x_train,columns=pts_col)

        print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
        print('onehot y_train')
        print(onehot.sum(axis=0))
    
        ### Class weight
        print("\nClass weight")
        num_classes = len(pts[outcome[0]].unique())
        if model_type == 'binary' or model_type == 'regression':
            num_classes = 1
        onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
        class_counts = onehot.sum(axis=0).values
        total_count = sum(class_counts)
        class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
        class_weights = dict(enumerate(class_rate))
        print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

        print("\nscale")
        scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
        #  'age',
        print(scale) 
        scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
        x_train.loc[:,scale] = scale_x.transform(x_train[scale])
        x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
        
        original_x_train = x_train.copy()
        original_y_train = y_train.copy()
        original_x_test = x_test.copy()
        original_y_test = y_test.copy()
        ### SMOTE
        if resample:
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7) 
            x_train, y_train = sm.fit_sample(x_train, y_train)
            print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
            x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
            y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
            for c in range(num_classes):
                class_weights[c]=1
            print('class weights reseted: ',class_weights)
        
        ### split learn/val
        print('\nsplit learn/val')
        x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
        print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

        print('\nonehot train/test/learn/val')
        if model_type == 'classification' :
            y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_train.sum(axis=0))
            y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_test.sum(axis=0))
            y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_learn.sum(axis=0))
            y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            print(y_val.sum(axis=0))

        ### Hyper Param 
        ######################################
        param_grid = []
        if optimize == True:
            neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
            layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
            drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
            batchNorm = hp.choice( 'batchNorm',[True, False] )
            activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
            learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
            epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
            batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

            param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                    drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                    learning_ratio=learning_ratio, # compile
                    epochs=epochs, batch_size=batch_size ) # fit           

            if model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
    
        ### Functions ### 
        ######################################
        print("Model_name full")
        def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            print('TEST__ param_grid:', param_grid)

            # Model
            neuron_comb =  param_grid['neuron_comb']
            layers_comb = param_grid['layers_comb']
            drop_rate =  param_grid['drop_rate'] 
            batchNorm = param_grid['batchNorm'] 
            activation_out = param_grid['activation_out']
            # compile
            learning_ratio = param_grid['learning_ratio']
            # fit
            epochs = param_grid['epochs']
            batch_size= param_grid['batch_size']
        
            ## model__fn
            inputA = Input(shape=(demo_shape,))
            if final_model:
                print('\nFinal_model inputA.shape',inputA.shape)
    
            attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
            inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
            
            fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
            fully = Dropout(drop_rate,name="Drop-1")(fully)
            if batchNorm:
                fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

            for l in range(2,layers_comb+1,1):
                    fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                    fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                    if batchNorm:
                        fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

            out= []
            if model_type != 'regression':
                out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
                # sigmoid softmax tanh relu PReLU Leaky ReLU 
            else:
                out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
                # linear' bias_initializer="zeros",
                # kernel_regularizer=None,
                # bias_regularizer=None,
                # activity_regularizer=None,
            model = Model(inputs=inputA, outputs=out)

            if final_model:
                model.summary()

            loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
            metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
            if model_type == 'binary':
                loss = 'binary_crossentropy'
            if model_type == 'regression':
                loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
                metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
            model.compile(loss = loss,
                        metrics=metrics,
                        optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

            es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
            if model_type == 'regression':
                es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
                mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

            history = []
            if model_type == 'classification':
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values), 
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc],
                                    class_weight=class_weights
                                    )
                model = load_model('best_model.h5')
            else:
                history = model.fit(x=x_learn[pts_col].values,  
                                    y=y_learn.values,
                                    validation_data=(x_val[pts_col].values,  y_val.values),
                                    epochs= epochs, 
                                    batch_size= batch_size,                            
                                    verbose=verbose_model, 
                                    callbacks=[es,mc]
                                )
                model = load_model('best_model.h5')

            pred_prob = model.predict( x_val[pts_col].values)
            if np.isnan(pred_prob).any():
                print("ERROR np.nan_to_num(pred_prob)")
                pred_prob = np.nan_to_num(pred_prob)
            predictions = pred_prob.argmax(axis=1)
            if model_type == 'binary':
                fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
                gmeans = np.sqrt(tpr * (1-fpr))
                ix = np.argmax(gmeans)
                print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
                predictions = (pred_prob >thresholds[ix]).astype(np.int) 
            if model_type == 'regression':
                predictions = model.predict( x_val[pts_col].values)
            y_val_binary = y_val.copy()
            if model_type == 'classification':
                y_val = y_val.values.argmax(axis=1)
        
            ### for ploting send out
            print('history.history.keys()',history.history.keys())
            train_acc=[]
            val_acc=[]
            if model_type == 'classification' or model_type == 'binary':
                train_acc = history.history['auc'] 
                val_acc = history.history['val_auc']
            elif model_type == 'regression':
                train_acc = history.history['mape_metric']
                val_acc = history.history['val_mape_metric']
            else:
                print("Error model type")
                return(0)
            train_loss = history.history['loss']
            val_loss = history.history['val_loss']
            epochs = range(len(train_acc))
        
            if model_type == 'regression' :
                y_val = y_val_binary
                actual, pred = np.array(y_val), np.array(predictions)
                mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
                mape_acc = 100 - mape_error 
                results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                            'accuracy_score_val': mape_acc, 
                            'train_acc':train_acc, 'val_acc':val_acc, 
                            'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                        }
                print("TEST__ accuracy_score:mape_acc ", mape_acc)
                print("datetime.now()",datetime.now())
                return (results)

            if model_type == 'binary' or model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)
                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)

                f101 = f1_score(y_val, predictions, average=None)
                pre01 = precision_score(y_val,predictions, average=None)
                rec01 = recall_score(y_val, predictions, average=None)
                
                class_1= 0
                class_0 = 0
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    if class_ == 0:  class_0 = class_accuracy
                    if class_ == 1:  class_1 = class_accuracy
                    print('TEST__ confusion:',class_str,class_accuracy)

                results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,"f101":f101, "rec01":rec01, "pre01":pre01, 'class_0':class_0, "class_1":class_1,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
        ######################################
        print("Model_name rf")
        def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
            
            ## model__rf
            # model = RandomForestClassifier(random_state=7)
            model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000) #1000 __trees __ft
            # model = sk.neighbors.KNeighborsClassifier()
            # model = sk.svm.SVC()

            model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
            predictions = model.predict(x_val[pts_col].values)
            pred_prob = model.predict_proba(x_val[pts_col].values)
            y_val_binary = y_val
            y_val = y_val.values.argmax(axis=1)
            
            ### for ploting send out
            train_acc=0
            val_acc=0
            train_loss = 0
            val_loss = 0
            epochs = 0
        
            if model_type =='classification':
                accuracy_score_val =round(accuracy_score(y_val,predictions),2)
                f1_score_val = f1_score(y_val, predictions, average=score_average)
                precision_score_val = precision_score(y_val,predictions, average=score_average)
                recall_score_val = recall_score(y_val, predictions, average=score_average)

                f101 = f1_score(y_val, predictions, average=None)
                pre01 = precision_score(y_val,predictions, average=None)
                rec01 = recall_score(y_val, predictions, average=None)

                Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
                roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
                print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
                class_1= 0
                class_0 = 0
                confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
                if model_type == 'binary':
                    confusion_= confusion_matrix(y_val_binary.values,predictions)
                for class_ in range(len(confusion_)):
                    class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                    class_str ="class_"+str(class_)+"_accuracy"
                    if class_ == 0:  class_0 = class_accuracy
                    if class_ == 1:  class_1 = class_accuracy
                    print('TEST__ confusion:',class_str,class_accuracy)
    

                results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                            'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val,  'class_0':class_0, "class_1":class_1, "f101":f101, "rec01":rec01, "pre01":pre01,
                            'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                            'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                            'val_acc':val_acc, 'val_loss':val_loss,
                            'train_acc':train_acc, 'train_loss':train_loss,
                            'epochs':epochs,
                            'y_val_binary':y_val_binary,'y_val':y_val,
                            'pred_prob':pred_prob, 'predictions':predictions,
                            'model': model
                            }
                print("datetime.now()",datetime.now())
                return (results)
            else:
                print("\nwrong model type")
                return(0)
        ######################################
        print("Results #############################")
        ### trials 
        # tpe_trials = Trials()
        tpe_best = []
        if optimize:
            if model_name == 'full':
                tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
            elif model_name == 'rf':
                print("*** Turn optimize off for rf")
                return()
            else:
                print("Error optimize: model not found")
                return(0)
            param_grid= space_eval(param_grid, tpe_best)
            print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
            print("datetime.now(): ", datetime.now())
        else: # __param
            if model_name == 'full':
                param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 32, 'drop_rate': 0.3, 'epochs': 40, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 900}
                if max_level == 3:
                    param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600}
                elif max_level == 5:
                    param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 64, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 400}
                elif max_level == 7:
                    param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 16, 'drop_rate': 0.5, 'epochs': 40, 'layers_comb': 1, 'learning_ratio': 0.0001, 'neuron_comb': 300}
                elif max_level == 9:
                    param_grid = {'activation_out': 'sigmoid', 'batchNorm': True, 'batch_size': 32, 'drop_rate': 0.3, 'epochs': 40, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300}

                if model_type == 'binary':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
                if model_type == 'regression':
                    param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif model_name == 'rf':
                print("rf not optimize")
            else:
                print("Error optimize: model not found")
                return(0)

        print("\nFinal Model")
        final_results = []
        final_model =  True
        # __call
        if model_name == 'full':
            final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
        if model_name == 'rf':
            final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
        
        ## __plot
        if True: 
            # fig = plt.figure(figsize=(5,5)) 
            # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
            # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
            # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

            # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
            # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
            # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
            # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
            # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
            # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

            #  f1= []
            # rec= []
            # per=[]
            # 'f1_score_val': f1_score_val, 
            # 'precision_score_val': precision_score_val, 
            # 'recall_score_val': recall_score_val,

            f101.append(final_results['f101'])
            rec01.append(final_results['rec01'])
            per01.append(final_results['pre01'])
            
            f1.append(final_results['f1_score_val'])
            rec.append(final_results['recall_score_val'])
            per.append(final_results['precision_score_val'])
            c1.append(final_results['class_1'])
            c0.append(final_results['class_0'])

            fpr, tpr, threshold = final_results['roc_curve_val']
            roc_auc = auc(fpr, tpr)
            # ax1.set_title('Receiver Operating Characteristic')
            # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
            # ax1.legend(loc = 'lower right')
            # ax1.plot([0, 1], [0, 1],'r--')
            # ax1.set_xlim([0, 1])
            # ax1.set_ylim([0, 1])
            # ax1.set_ylabel('True Positive Rate')
            # ax1.set_xlabel('False Positive Rate')

            interp_tpr = np.interp(mean_fpr, fpr, tpr)
            interp_tpr[0] = 0.0
            tprs.append(interp_tpr)
            aucs.append(roc_auc)

    # __ft
    
    ax.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)

    print("f1:",np.mean(f1))
    print("per:",np.mean(per))
    print("rec:",np.mean(rec))

    print("f101:",np.mean(f101, axis=0))
    print("per01:",np.mean(per01, axis=0))
    print("rec01:",np.mean(rec01, axis=0))

    print("class_1",np.mean(c1))
    print("class_0",np.mean(c0))
    mean_tpr = np.mean(tprs, axis=0)
    mean_tpr[-1] = 1.0
    mean_auc = auc(mean_fpr, mean_tpr)
    std_auc = np.std(aucs)
    print("std_auc: ",std_auc)
    ax.plot( #1000
        mean_fpr,
        mean_tpr,
        color="b",
        label=r"Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, 0.01), # std_auc 
        # label=r"Mean ROC (AUC = %0.2f)" % (mean_auc),

        lw=2,
        alpha=0.8,
    )
    count_ratio = 0
    colors_sns = sns.color_palette("tab10")
    for t in tprs:
        count_ratio+=1
        ax.plot(
        mean_fpr,
        t,
        color=colors_sns[count_ratio-1],
        # label=r"Fold %f (AUC = %0.2f $\pm$ %0.2f)" % (count_ratio,mean_auc, std_auc),
        # label=r"Fold %f (AUC = %0.2f $\pm$ %0.2f)" % (count_ratio,mean_auc, std_auc),
        lw=1,
        alpha=0.2,
        )

    std_tpr = np.std(tprs, axis=0)
    tprs_upper = np.minimum(mean_tpr + std_tpr, 1)
    tprs_lower = np.maximum(mean_tpr - std_tpr, 0) # tpr*1.5
    ax.fill_between(
        mean_fpr,
        tprs_lower,
        tprs_upper,
        color="grey",
        alpha=0.4,
        label=r"$\pm$ 1 std. dev.",
    )

    ax.set(
        xlim=[-0.05, 1.05],
        ylim=[-0.05, 1.05],
        title="Receiver Operating Characteristic (ROC)",
    )
    ax.legend(loc="lower right")
    ax.set(xlabel='False Positive Rate')
    ax.set(ylabel='True Positive Rate')

    SMALL_SIZE = 8
    MEDIUM_SIZE = 10
    BIGGER_SIZE = 12

    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title

    plt.show()

    print("after KFold for : datetime.now(): ", datetime.now())
    # return( pd.DataFrame( {'return' : [0]} ) ) 
    return(pd.DataFrame({'mean_tpr': mean_tpr, 'mean_fpr': mean_fpr}) )

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.827c3607-b5ca-430e-b48d-32144df3a08e"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
@output_image_type('svg')
def shap_get_it_1(all_treat_new, shap_testing_for_explainer):
    shap_testing = shap_testing_for_explainer
    all_treat_new = all_treat_new
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    import shap

    return(all_treat_new[shap_testing_for_explainer['os_day1_1.0']== 1 ])
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')

    shap.summary_plot(shap_values[target_shap_class], shap_testing, feature_names=pts_col) # plot_type="bar"
    # shap.plots.beeswarm(shap_values, max_display=20)
    # shap.plots.beeswarm(shap_values, order=shap_values.abs.max(0))
    # https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html
    
    # fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")
    

    # print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    # print('type shap: ',type(shap_values))
    # print('type shap class: ', type(shap_values[0]))

    # num_class_samples_columns =len(shap_values[target_shap_class][0])
    # all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    # for x in range(shap_values[target_shap_class].shape[0]) :
    #     all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    # all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    # print('all_variables avegrafe Shap - shape',all_variables.shape)

    # overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    # overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    # overall_importance_df = overall_importance_df.iloc[0]

    # fig = plt.figure(figsize=(15,5)) 
    # ax2 = plt.subplot2grid((3,3),(0,0), rowspan = 3,colspan = 3)

    # overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")
    # ax2.set_xlabel("Features")
    # fig.tight_layout()
    # plt.tight_layout()
    # plt.show()
    # print("show")
    return(all_treat_new)

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.62d8fdfa-5e91-41f2-b57c-e6584a40d79c"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
@output_image_type('svg')
def shap_get_it_3(all_treat_new, shap_testing_for_explainer):
    shap_testing = shap_testing_for_explainer
    all_treat_new = all_treat_new
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    import shap

    return(all_treat_new[shap_testing_for_explainer['os_day1_3.0']== 1 ])
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')

    shap.summary_plot(shap_values[target_shap_class], shap_testing, feature_names=pts_col) # plot_type="bar"
    # shap.plots.beeswarm(shap_values, max_display=20)
    # shap.plots.beeswarm(shap_values, order=shap_values.abs.max(0))
    # https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html
    
    # fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")
    

    # print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    # print('type shap: ',type(shap_values))
    # print('type shap class: ', type(shap_values[0]))

    # num_class_samples_columns =len(shap_values[target_shap_class][0])
    # all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    # for x in range(shap_values[target_shap_class].shape[0]) :
    #     all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    # all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    # print('all_variables avegrafe Shap - shape',all_variables.shape)

    # overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    # overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    # overall_importance_df = overall_importance_df.iloc[0]

    # fig = plt.figure(figsize=(15,5)) 
    # ax2 = plt.subplot2grid((3,3),(0,0), rowspan = 3,colspan = 3)

    # overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")
    # ax2.set_xlabel("Features")
    # fig.tight_layout()
    # plt.tight_layout()
    # plt.show()
    # print("show")
    return(all_treat_new)

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.bdb0d392-e218-4f15-9c6f-95f44462585a"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
@output_image_type('svg')
def shap_get_it_5(all_treat_new, shap_testing_for_explainer):
    shap_testing = shap_testing_for_explainer
    all_treat_new = all_treat_new
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    import shap

    return(all_treat_new[shap_testing_for_explainer['os_day1_5.0']== 1 ])
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')

    shap.summary_plot(shap_values[target_shap_class], shap_testing, feature_names=pts_col) # plot_type="bar"
    # shap.plots.beeswarm(shap_values, max_display=20)
    # shap.plots.beeswarm(shap_values, order=shap_values.abs.max(0))
    # https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html
    
    # fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")
    

    # print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    # print('type shap: ',type(shap_values))
    # print('type shap class: ', type(shap_values[0]))

    # num_class_samples_columns =len(shap_values[target_shap_class][0])
    # all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    # for x in range(shap_values[target_shap_class].shape[0]) :
    #     all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    # all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    # print('all_variables avegrafe Shap - shape',all_variables.shape)

    # overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    # overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    # overall_importance_df = overall_importance_df.iloc[0]

    # fig = plt.figure(figsize=(15,5)) 
    # ax2 = plt.subplot2grid((3,3),(0,0), rowspan = 3,colspan = 3)

    # overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")
    # ax2.set_xlabel("Features")
    # fig.tight_layout()
    # plt.tight_layout()
    # plt.show()
    # print("show")
    return(all_treat_new)

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.6c215750-5262-4299-a16d-2dfbaf4a8a6e"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
@output_image_type('svg')
def shap_get_it_7(all_treat_new, shap_testing_for_explainer):
    shap_testing = shap_testing_for_explainer
    all_treat_new = all_treat_new
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    import shap

    return(all_treat_new[shap_testing_for_explainer['os_day1_7.0']== 1 ])
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')

    shap.summary_plot(shap_values[target_shap_class], shap_testing, feature_names=pts_col) # plot_type="bar"
    # shap.plots.beeswarm(shap_values, max_display=20)
    # shap.plots.beeswarm(shap_values, order=shap_values.abs.max(0))
    # https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html
    
    # fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")
    

    # print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    # print('type shap: ',type(shap_values))
    # print('type shap class: ', type(shap_values[0]))

    # num_class_samples_columns =len(shap_values[target_shap_class][0])
    # all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    # for x in range(shap_values[target_shap_class].shape[0]) :
    #     all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    # all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    # print('all_variables avegrafe Shap - shape',all_variables.shape)

    # overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    # overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    # overall_importance_df = overall_importance_df.iloc[0]

    # fig = plt.figure(figsize=(15,5)) 
    # ax2 = plt.subplot2grid((3,3),(0,0), rowspan = 3,colspan = 3)

    # overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")
    # ax2.set_xlabel("Features")
    # fig.tight_layout()
    # plt.tight_layout()
    # plt.show()
    # print("show")
    return(all_treat_new)

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.b0855839-cff8-402c-8945-0cd833d227f3"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
@output_image_type('svg')
def shap_get_it_9(all_treat_new, shap_testing_for_explainer):
    shap_testing = shap_testing_for_explainer
    all_treat_new = all_treat_new
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    import shap

    return(all_treat_new[shap_testing_for_explainer['os_day1_9.0']== 1 ])
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')

    shap.summary_plot(shap_values[target_shap_class], shap_testing, feature_names=pts_col) # plot_type="bar"
    # shap.plots.beeswarm(shap_values, max_display=20)
    # shap.plots.beeswarm(shap_values, order=shap_values.abs.max(0))
    # https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html
    
    # fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")
    

    # print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    # print('type shap: ',type(shap_values))
    # print('type shap class: ', type(shap_values[0]))

    # num_class_samples_columns =len(shap_values[target_shap_class][0])
    # all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    # for x in range(shap_values[target_shap_class].shape[0]) :
    #     all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    # all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    # print('all_variables avegrafe Shap - shape',all_variables.shape)

    # overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    # overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    # overall_importance_df = overall_importance_df.iloc[0]

    # fig = plt.figure(figsize=(15,5)) 
    # ax2 = plt.subplot2grid((3,3),(0,0), rowspan = 3,colspan = 3)

    # overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")
    # ax2.set_xlabel("Features")
    # fig.tight_layout()
    # plt.tight_layout()
    # plt.show()
    # print("show")
    return(all_treat_new)

@transform_pandas(
    Output(rid="ri.vector.main.execute.e9050241-1998-4d48-a2b1-f597cd7c9796"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859")
)
@output_image_type('svg')
def shap_graph_top20_med(all_treat_new):
    all_treat_new_1 = all_treat_new

    top=10

    treatments =  ['BiotMQ_', 'BiotMQ_Coag_', 'BiotMQ_Coag_Misc_', 'BiotMQ_Coag_Misc_MonoI_', 'BiotMQ_Coag_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Misc_MonoSP_', 'BiotMQ_Coag_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Misc_ViraTrgt_', 'BiotMQ_Coag_MonoI_', 'BiotMQ_Coag_MonoI_ViraTrgt_', 'BiotMQ_Coag_MonoSP_', 'BiotMQ_Coag_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_', 'BiotMQ_Coag_Ster_Misc_', 'BiotMQ_Coag_Ster_Misc_MonoI_', 'BiotMQ_Coag_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_MonoSP_', 'BiotMQ_Coag_Ster_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoI_', 'BiotMQ_Coag_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoSP_', 'BiotMQ_Coag_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_', 'BiotMQ_Coag_ViraUnp_Misc_', 'BiotMQ_Coag_ViraUnp_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoI_', 'BiotMQ_Coag_ViraUnp_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoSP_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_ViraTrgt_', 'BiotMQ_Misc_', 'BiotMQ_Misc_MonoSP_', 'BiotMQ_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Misc_ViraTrgt_', 'BiotMQ_MonoI_', 'BiotMQ_MonoI_ViraTrgt_', 'BiotMQ_MonoSP_', 'BiotMQ_Ster_', 'BiotMQ_Ster_Misc_', 'BiotMQ_Ster_Misc_MonoI_', 'BiotMQ_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Ster_Misc_MonoSP_', 'BiotMQ_Ster_Misc_ViraTrgt_', 'BiotMQ_Ster_MonoI_', 'BiotMQ_Ster_MonoI_ViraTrgt_', 'BiotMQ_Ster_MonoSP_', 'BiotMQ_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Ster_ViraTrgt_', 'BiotMQ_ViraTrgt_', 'BiotMQ_ViraUnp_', 'BiotMQ_ViraUnp_Misc_', 'BiotMQ_ViraUnp_Misc_MonoI_', 'BiotMQ_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_MonoI_', 'BiotMQ_ViraUnp_Ster_', 'BiotMQ_ViraUnp_Ster_Misc_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoI_', 'BiotMQ_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoSP_', 'BiotMQ_ViraUnp_Ster_MonoSP_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_ViraUnp_ViraTrgt_', 'Coag_', 'Coag_Misc_', 'Coag_Misc_MonoI_', 'Coag_Misc_MonoI_ViraTrgt_', 'Coag_Misc_MonoSP_', 'Coag_Misc_MonoSP_ViraTrgt_', 'Coag_Misc_ViraTrgt_', 'Coag_MonoI_', 'Coag_MonoI_ViraTrgt_', 'Coag_MonoSP_', 'Coag_MonoSP_ViraTrgt_', 'Coag_Ster_', 'Coag_Ster_Misc_', 'Coag_Ster_Misc_MonoI_', 'Coag_Ster_Misc_MonoI_ViraTrgt_', 'Coag_Ster_Misc_MonoSP_', 'Coag_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_Ster_Misc_ViraTrgt_', 'Coag_Ster_MonoI_', 'Coag_Ster_MonoI_ViraTrgt_', 'Coag_Ster_MonoSP_', 'Coag_Ster_MonoSP_ViraTrgt_', 'Coag_Ster_ViraTrgt_', 'Coag_ViraTrgt_', 'Coag_ViraUnp_', 'Coag_ViraUnp_Misc_', 'Coag_ViraUnp_Misc_MonoI_', 'Coag_ViraUnp_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Misc_MonoSP_', 'Coag_ViraUnp_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Misc_ViraTrgt_', 'Coag_ViraUnp_MonoI_', 'Coag_ViraUnp_MonoI_ViraTrgt_', 'Coag_ViraUnp_MonoSP_', 'Coag_ViraUnp_Ster_', 'Coag_ViraUnp_Ster_Misc_', 'Coag_ViraUnp_Ster_Misc_MonoI_', 'Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_MonoSP_', 'Coag_ViraUnp_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoI_', 'Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoSP_', 'Coag_ViraUnp_Ster_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_ViraTrgt_', 'Coag_ViraUnp_ViraTrgt_', 'Misc_', 'Misc_MonoI_', 'Misc_MonoI_ViraTrgt_', 'Misc_MonoSP_', 'Misc_MonoSP_ViraTrgt_', 'Misc_ViraTrgt_', 'MonoI_', 'MonoI_ViraTrgt_', 'MonoSP_', 'MonoSP_ViraTrgt_', 'Ster_', 'Ster_Misc_', 'Ster_Misc_MonoI_', 'Ster_Misc_MonoI_ViraTrgt_', 'Ster_Misc_MonoSP_', 'Ster_Misc_MonoSP_ViraTrgt_', 'Ster_Misc_ViraTrgt_', 'Ster_MonoI_', 'Ster_MonoI_ViraTrgt_', 'Ster_MonoSP_', 'Ster_MonoSP_MonoI_', 'Ster_MonoSP_ViraTrgt_', 'Ster_ViraTrgt_', 'ViraTrgt_', 'ViraUnp_', 'ViraUnp_Misc_', 'ViraUnp_Misc_ViraTrgt_', 'ViraUnp_MonoI_', 'ViraUnp_MonoSP_', 'ViraUnp_Ster_', 'ViraUnp_Ster_Misc_', 'ViraUnp_Ster_Misc_MonoI_', 'ViraUnp_Ster_Misc_MonoSP_', 'ViraUnp_Ster_Misc_ViraTrgt_', 'ViraUnp_Ster_MonoI_', 'ViraUnp_Ster_MonoI_ViraTrgt_', 'ViraUnp_Ster_MonoSP_', 'ViraUnp_Ster_MonoSP_ViraTrgt_', 'ViraUnp_Ster_ViraTrgt_', 'ViraUnp_ViraTrgt_']

    all_treat_new = all_treat_new[treatments]
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns # pts_col_no_treat +   treat_unique

    # for drug, b_name in zip(['Steroid Preparations','Anticoagulants','Antiviral Agents','Miscellaneous Antibiotics','Miscellaneous','Monoclonal Antibodies'],
    #                           ['Ster',                'Coag',             'Vira',             'Biot',                     'Misc',         'Mono']):

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    # pts_col = [ x.replace('Ster','Steroid').replace('Coag','Anticoagulants').replace('Vira','Antiviral').replace('Biot','Mis.Antibiotics').replace('Misc','Miscellaneous').replace('Mono','Monoclonal') for x in pts_col]

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]

    fig = plt.figure(figsize=(10,4)) 
    fig.subplots_adjust(left=0.3)
    ax1 = plt.subplot2grid((2,2),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((2,2),(0,1), rowspan = 2,colspan = 1)

    positives = overall_importance_df[overall_importance_df>0].sort_values(ascending=True)
    negatives = overall_importance_df[overall_importance_df<=0].sort_values(ascending=False)

    positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, top )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, top )) # .9 positives.size

    positives[-top:].plot.barh( ax=ax1,color=color_p) # yerr=std[:24], ######[new_Col] [new_Col]
    negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]

    ax2.set_xlim(negatives.max() * 1.03,negatives.min()* 1.2 )

    ax2.set_title("Negative Impacts ")
    ax2.set_xlabel("Mean Shap value (impact on model output)")
    ax2.set_ylabel("Features")
    ax1.set_title("Positive Impacts ")
    ax1.set_xlabel("Mean Shap value (impact on model output)")
    ax1.set_ylabel("Features")
    fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")
    # return(all_treat_new)
    combined_Frame = (positives[(-top):].append(negatives[(-top):])).to_frame().T 
    print(combined_Frame)
    return(     combined_Frame    )

@transform_pandas(
    Output(rid="ri.vector.main.execute.1c01975c-967a-4d87-8fb6-0ff0ee522dbb"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859")
)
@output_image_type('svg')
def shap_graph_top20_med_1(all_treat_new):
    all_treat_new_1 = all_treat_new

    top=10

    treatments =  ['BiotMQ_', 'BiotMQ_Coag_', 'BiotMQ_Coag_Misc_', 'BiotMQ_Coag_Misc_MonoI_', 'BiotMQ_Coag_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Misc_MonoSP_', 'BiotMQ_Coag_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Misc_ViraTrgt_', 'BiotMQ_Coag_MonoI_', 'BiotMQ_Coag_MonoI_ViraTrgt_', 'BiotMQ_Coag_MonoSP_', 'BiotMQ_Coag_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_', 'BiotMQ_Coag_Ster_Misc_', 'BiotMQ_Coag_Ster_Misc_MonoI_', 'BiotMQ_Coag_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_MonoSP_', 'BiotMQ_Coag_Ster_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoI_', 'BiotMQ_Coag_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoSP_', 'BiotMQ_Coag_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_', 'BiotMQ_Coag_ViraUnp_Misc_', 'BiotMQ_Coag_ViraUnp_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoI_', 'BiotMQ_Coag_ViraUnp_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoSP_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_ViraTrgt_', 'BiotMQ_Misc_', 'BiotMQ_Misc_MonoSP_', 'BiotMQ_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Misc_ViraTrgt_', 'BiotMQ_MonoI_', 'BiotMQ_MonoI_ViraTrgt_', 'BiotMQ_MonoSP_', 'BiotMQ_Ster_', 'BiotMQ_Ster_Misc_', 'BiotMQ_Ster_Misc_MonoI_', 'BiotMQ_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Ster_Misc_MonoSP_', 'BiotMQ_Ster_Misc_ViraTrgt_', 'BiotMQ_Ster_MonoI_', 'BiotMQ_Ster_MonoI_ViraTrgt_', 'BiotMQ_Ster_MonoSP_', 'BiotMQ_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Ster_ViraTrgt_', 'BiotMQ_ViraTrgt_', 'BiotMQ_ViraUnp_', 'BiotMQ_ViraUnp_Misc_', 'BiotMQ_ViraUnp_Misc_MonoI_', 'BiotMQ_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_MonoI_', 'BiotMQ_ViraUnp_Ster_', 'BiotMQ_ViraUnp_Ster_Misc_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoI_', 'BiotMQ_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoSP_', 'BiotMQ_ViraUnp_Ster_MonoSP_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_ViraUnp_ViraTrgt_', 'Coag_', 'Coag_Misc_', 'Coag_Misc_MonoI_', 'Coag_Misc_MonoI_ViraTrgt_', 'Coag_Misc_MonoSP_', 'Coag_Misc_MonoSP_ViraTrgt_', 'Coag_Misc_ViraTrgt_', 'Coag_MonoI_', 'Coag_MonoI_ViraTrgt_', 'Coag_MonoSP_', 'Coag_MonoSP_ViraTrgt_', 'Coag_Ster_', 'Coag_Ster_Misc_', 'Coag_Ster_Misc_MonoI_', 'Coag_Ster_Misc_MonoI_ViraTrgt_', 'Coag_Ster_Misc_MonoSP_', 'Coag_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_Ster_Misc_ViraTrgt_', 'Coag_Ster_MonoI_', 'Coag_Ster_MonoI_ViraTrgt_', 'Coag_Ster_MonoSP_', 'Coag_Ster_MonoSP_ViraTrgt_', 'Coag_Ster_ViraTrgt_', 'Coag_ViraTrgt_', 'Coag_ViraUnp_', 'Coag_ViraUnp_Misc_', 'Coag_ViraUnp_Misc_MonoI_', 'Coag_ViraUnp_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Misc_MonoSP_', 'Coag_ViraUnp_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Misc_ViraTrgt_', 'Coag_ViraUnp_MonoI_', 'Coag_ViraUnp_MonoI_ViraTrgt_', 'Coag_ViraUnp_MonoSP_', 'Coag_ViraUnp_Ster_', 'Coag_ViraUnp_Ster_Misc_', 'Coag_ViraUnp_Ster_Misc_MonoI_', 'Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_MonoSP_', 'Coag_ViraUnp_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoI_', 'Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoSP_', 'Coag_ViraUnp_Ster_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_ViraTrgt_', 'Coag_ViraUnp_ViraTrgt_', 'Misc_', 'Misc_MonoI_', 'Misc_MonoI_ViraTrgt_', 'Misc_MonoSP_', 'Misc_MonoSP_ViraTrgt_', 'Misc_ViraTrgt_', 'MonoI_', 'MonoI_ViraTrgt_', 'MonoSP_', 'MonoSP_ViraTrgt_', 'Ster_', 'Ster_Misc_', 'Ster_Misc_MonoI_', 'Ster_Misc_MonoI_ViraTrgt_', 'Ster_Misc_MonoSP_', 'Ster_Misc_MonoSP_ViraTrgt_', 'Ster_Misc_ViraTrgt_', 'Ster_MonoI_', 'Ster_MonoI_ViraTrgt_', 'Ster_MonoSP_', 'Ster_MonoSP_MonoI_', 'Ster_MonoSP_ViraTrgt_', 'Ster_ViraTrgt_', 'ViraTrgt_', 'ViraUnp_', 'ViraUnp_Misc_', 'ViraUnp_Misc_ViraTrgt_', 'ViraUnp_MonoI_', 'ViraUnp_MonoSP_', 'ViraUnp_Ster_', 'ViraUnp_Ster_Misc_', 'ViraUnp_Ster_Misc_MonoI_', 'ViraUnp_Ster_Misc_MonoSP_', 'ViraUnp_Ster_Misc_ViraTrgt_', 'ViraUnp_Ster_MonoI_', 'ViraUnp_Ster_MonoI_ViraTrgt_', 'ViraUnp_Ster_MonoSP_', 'ViraUnp_Ster_MonoSP_ViraTrgt_', 'ViraUnp_Ster_ViraTrgt_', 'ViraUnp_ViraTrgt_']

    # all_treat_new = all_treat_new[treatments]
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns # pts_col_no_treat +   treat_unique

    # for drug, b_name in zip(['Steroid Preparations','Anticoagulants','Antiviral Agents','Miscellaneous Antibiotics','Miscellaneous','Monoclonal Antibodies'],
    #                           ['Ster',                'Coag',             'Vira',             'Biot',                     'Misc',         'Mono']):

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    # pts_col = [ x.replace('Ster','Steroid').replace('Coag','Anticoagulants').replace('Vira','Antiviral').replace('Biot','Mis.Antibiotics').replace('Misc','Miscellaneous').replace('Mono','Monoclonal') for x in pts_col]

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]
    overall_importance_df['pressor_days']=0
    overall_importance_df['PVD']=0

    fig = plt.figure(figsize=(10,4)) 
    ax1 = plt.subplot2grid((2,2),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((2,2),(0,1), rowspan = 2,colspan = 1)

    positives = overall_importance_df[overall_importance_df>0].sort_values(ascending=True)
    negatives = overall_importance_df[overall_importance_df<=0].sort_values(ascending=False)

    positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 - ').str.replace('1.0','1').str.replace('OS day 1 -','OS level')
    negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 - ').str.replace('age','Age').str.replace('renal','Renal').str.replace('liversevere','LiverSevere ').str.replace('3.0','3').str.replace('7.0','7').str.replace('OS day 1 -','OS level')
    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, top )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, top )) # .9 positives.size

    positives[-top:].plot.barh( ax=ax1,color=color_p) # yerr=std[:24], ######[new_Col] [new_Col]
    negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]

    ax2.set_xlim(negatives.max() * 1.03,negatives.min()* 1.2 )

    ax2.set_title("Negative Impacts ")
    ax2.set_xlabel("Mean Shap value (impact on model output)")
    ax2.set_ylabel("Features")
    ax1.set_title("Positive Impacts ")
    ax1.set_xlabel("Mean Shap value (impact on model output)")
    ax1.set_ylabel("Features")
    fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")
    # return(all_treat_new)
    combined_Frame = (positives[(-top):].append(negatives[(-top):])).to_frame().T 
    print(combined_Frame)
    return(     combined_Frame    )

@transform_pandas(
    Output(rid="ri.vector.main.execute.34b5883a-3fa6-4de3-afee-74fd683d91ac"),
    all_treat_new_copied_20q3q4=Input(rid="ri.foundry.main.dataset.75025029-953e-45ee-bb89-f75addb1eb0c")
)
@output_image_type('svg')
def shap_graph_top20_med_2(all_treat_new_copied_20q3q4):
    all_treat_new = all_treat_new_copied_20q3q4

    top=10

    treatments =  ['BiotMQ_', 'BiotMQ_Coag_', 'BiotMQ_Coag_Misc_', 'BiotMQ_Coag_Misc_MonoI_', 'BiotMQ_Coag_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Misc_MonoSP_', 'BiotMQ_Coag_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Misc_ViraTrgt_', 'BiotMQ_Coag_MonoI_', 'BiotMQ_Coag_MonoI_ViraTrgt_', 'BiotMQ_Coag_MonoSP_', 'BiotMQ_Coag_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_', 'BiotMQ_Coag_Ster_Misc_', 'BiotMQ_Coag_Ster_Misc_MonoI_', 'BiotMQ_Coag_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_MonoSP_', 'BiotMQ_Coag_Ster_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoI_', 'BiotMQ_Coag_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoSP_', 'BiotMQ_Coag_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_', 'BiotMQ_Coag_ViraUnp_Misc_', 'BiotMQ_Coag_ViraUnp_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoI_', 'BiotMQ_Coag_ViraUnp_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoSP_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_ViraTrgt_', 'BiotMQ_Misc_', 'BiotMQ_Misc_MonoSP_', 'BiotMQ_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Misc_ViraTrgt_', 'BiotMQ_MonoI_', 'BiotMQ_MonoI_ViraTrgt_', 'BiotMQ_MonoSP_', 'BiotMQ_Ster_', 'BiotMQ_Ster_Misc_', 'BiotMQ_Ster_Misc_MonoI_', 'BiotMQ_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Ster_Misc_MonoSP_', 'BiotMQ_Ster_Misc_ViraTrgt_', 'BiotMQ_Ster_MonoI_', 'BiotMQ_Ster_MonoI_ViraTrgt_', 'BiotMQ_Ster_MonoSP_', 'BiotMQ_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Ster_ViraTrgt_', 'BiotMQ_ViraTrgt_', 'BiotMQ_ViraUnp_', 'BiotMQ_ViraUnp_Misc_', 'BiotMQ_ViraUnp_Misc_MonoI_', 'BiotMQ_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_MonoI_', 'BiotMQ_ViraUnp_Ster_', 'BiotMQ_ViraUnp_Ster_Misc_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoI_', 'BiotMQ_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoSP_', 'BiotMQ_ViraUnp_Ster_MonoSP_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_ViraUnp_ViraTrgt_', 'Coag_', 'Coag_Misc_', 'Coag_Misc_MonoI_', 'Coag_Misc_MonoI_ViraTrgt_', 'Coag_Misc_MonoSP_', 'Coag_Misc_MonoSP_ViraTrgt_', 'Coag_Misc_ViraTrgt_', 'Coag_MonoI_', 'Coag_MonoI_ViraTrgt_', 'Coag_MonoSP_', 'Coag_MonoSP_ViraTrgt_', 'Coag_Ster_', 'Coag_Ster_Misc_', 'Coag_Ster_Misc_MonoI_', 'Coag_Ster_Misc_MonoI_ViraTrgt_', 'Coag_Ster_Misc_MonoSP_', 'Coag_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_Ster_Misc_ViraTrgt_', 'Coag_Ster_MonoI_', 'Coag_Ster_MonoI_ViraTrgt_', 'Coag_Ster_MonoSP_', 'Coag_Ster_MonoSP_ViraTrgt_', 'Coag_Ster_ViraTrgt_', 'Coag_ViraTrgt_', 'Coag_ViraUnp_', 'Coag_ViraUnp_Misc_', 'Coag_ViraUnp_Misc_MonoI_', 'Coag_ViraUnp_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Misc_MonoSP_', 'Coag_ViraUnp_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Misc_ViraTrgt_', 'Coag_ViraUnp_MonoI_', 'Coag_ViraUnp_MonoI_ViraTrgt_', 'Coag_ViraUnp_MonoSP_', 'Coag_ViraUnp_Ster_', 'Coag_ViraUnp_Ster_Misc_', 'Coag_ViraUnp_Ster_Misc_MonoI_', 'Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_MonoSP_', 'Coag_ViraUnp_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoI_', 'Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoSP_', 'Coag_ViraUnp_Ster_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_ViraTrgt_', 'Coag_ViraUnp_ViraTrgt_', 'Misc_', 'Misc_MonoI_', 'Misc_MonoI_ViraTrgt_', 'Misc_MonoSP_', 'Misc_MonoSP_ViraTrgt_', 'Misc_ViraTrgt_', 'MonoI_', 'MonoI_ViraTrgt_', 'MonoSP_', 'MonoSP_ViraTrgt_', 'Ster_', 'Ster_Misc_', 'Ster_Misc_MonoI_', 'Ster_Misc_MonoI_ViraTrgt_', 'Ster_Misc_MonoSP_', 'Ster_Misc_MonoSP_ViraTrgt_', 'Ster_Misc_ViraTrgt_', 'Ster_MonoI_', 'Ster_MonoI_ViraTrgt_', 'Ster_MonoSP_', 'Ster_MonoSP_MonoI_', 'Ster_MonoSP_ViraTrgt_', 'Ster_ViraTrgt_', 'ViraTrgt_', 'ViraUnp_', 'ViraUnp_Misc_', 'ViraUnp_Misc_ViraTrgt_', 'ViraUnp_MonoI_', 'ViraUnp_MonoSP_', 'ViraUnp_Ster_', 'ViraUnp_Ster_Misc_', 'ViraUnp_Ster_Misc_MonoI_', 'ViraUnp_Ster_Misc_MonoSP_', 'ViraUnp_Ster_Misc_ViraTrgt_', 'ViraUnp_Ster_MonoI_', 'ViraUnp_Ster_MonoI_ViraTrgt_', 'ViraUnp_Ster_MonoSP_', 'ViraUnp_Ster_MonoSP_ViraTrgt_', 'ViraUnp_Ster_ViraTrgt_', 'ViraUnp_ViraTrgt_']

    all_treat_new = all_treat_new[treatments]
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns # pts_col_no_treat +   treat_unique

    # for drug, b_name in zip(['Steroid Preparations','Anticoagulants','Antiviral Agents','Miscellaneous Antibiotics','Miscellaneous','Monoclonal Antibodies'],
    #                           ['Ster',                'Coag',             'Vira',             'Biot',                     'Misc',         'Mono']):

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    # pts_col = [ x.replace('Ster','Steroid').replace('Coag','Anticoagulants').replace('Vira','Antiviral').replace('Biot','Mis.Antibiotics').replace('Misc','Miscellaneous').replace('Mono','Monoclonal') for x in pts_col]

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]

    fig = plt.figure(figsize=(10,4)) 
    fig.subplots_adjust(left=0.3)
    ax1 = plt.subplot2grid((2,2),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((2,2),(0,1), rowspan = 2,colspan = 1)

    positives = overall_importance_df[overall_importance_df>0].sort_values(ascending=True)
    negatives = overall_importance_df[overall_importance_df<=0].sort_values(ascending=False)

    positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, top )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, top )) # .9 positives.size

    positives[-top:].plot.barh( ax=ax1,color=color_p) # yerr=std[:24], ######[new_Col] [new_Col]
    negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]

    ax2.set_xlim(negatives.max() * 1.03,negatives.min()* 1.2 )

    ax2.set_title("Negative Impacts ")
    ax2.set_xlabel("Mean Shap value (impact on model output)")
    ax2.set_ylabel("Features")
    ax1.set_title("Positive Impacts ")
    ax1.set_xlabel("Mean Shap value (impact on model output)")
    ax1.set_ylabel("Features")
    fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")
    # return(all_treat_new)
    combined_Frame = (positives[(-top):].append(negatives[(-top):])).to_frame().T 
    print(combined_Frame)
    return(     combined_Frame    )

@transform_pandas(
    Output(rid="ri.vector.main.execute.f92c6936-8c60-49c0-82a3-b90e6d3758de"),
    all_treat_new_copied_21q1q2=Input(rid="ri.foundry.main.dataset.d3e0335b-c044-40a1-8f33-bbd2b1ebab3b")
)
@output_image_type('svg')
def shap_graph_top20_med_2_1(all_treat_new_copied_21q1q2):
    all_treat_new = all_treat_new_copied_21q1q2

    top=10

    treatments =  ['BiotMQ_', 'BiotMQ_Coag_', 'BiotMQ_Coag_Misc_', 'BiotMQ_Coag_Misc_MonoI_', 'BiotMQ_Coag_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Misc_MonoSP_', 'BiotMQ_Coag_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Misc_ViraTrgt_', 'BiotMQ_Coag_MonoI_', 'BiotMQ_Coag_MonoI_ViraTrgt_', 'BiotMQ_Coag_MonoSP_', 'BiotMQ_Coag_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_', 'BiotMQ_Coag_Ster_Misc_', 'BiotMQ_Coag_Ster_Misc_MonoI_', 'BiotMQ_Coag_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_MonoSP_', 'BiotMQ_Coag_Ster_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoI_', 'BiotMQ_Coag_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoSP_', 'BiotMQ_Coag_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_', 'BiotMQ_Coag_ViraUnp_Misc_', 'BiotMQ_Coag_ViraUnp_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoI_', 'BiotMQ_Coag_ViraUnp_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoSP_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_ViraTrgt_', 'BiotMQ_Misc_', 'BiotMQ_Misc_MonoSP_', 'BiotMQ_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Misc_ViraTrgt_', 'BiotMQ_MonoI_', 'BiotMQ_MonoI_ViraTrgt_', 'BiotMQ_MonoSP_', 'BiotMQ_Ster_', 'BiotMQ_Ster_Misc_', 'BiotMQ_Ster_Misc_MonoI_', 'BiotMQ_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Ster_Misc_MonoSP_', 'BiotMQ_Ster_Misc_ViraTrgt_', 'BiotMQ_Ster_MonoI_', 'BiotMQ_Ster_MonoI_ViraTrgt_', 'BiotMQ_Ster_MonoSP_', 'BiotMQ_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Ster_ViraTrgt_', 'BiotMQ_ViraTrgt_', 'BiotMQ_ViraUnp_', 'BiotMQ_ViraUnp_Misc_', 'BiotMQ_ViraUnp_Misc_MonoI_', 'BiotMQ_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_MonoI_', 'BiotMQ_ViraUnp_Ster_', 'BiotMQ_ViraUnp_Ster_Misc_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoI_', 'BiotMQ_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoSP_', 'BiotMQ_ViraUnp_Ster_MonoSP_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_ViraUnp_ViraTrgt_', 'Coag_', 'Coag_Misc_', 'Coag_Misc_MonoI_', 'Coag_Misc_MonoI_ViraTrgt_', 'Coag_Misc_MonoSP_', 'Coag_Misc_MonoSP_ViraTrgt_', 'Coag_Misc_ViraTrgt_', 'Coag_MonoI_', 'Coag_MonoI_ViraTrgt_', 'Coag_MonoSP_', 'Coag_MonoSP_ViraTrgt_', 'Coag_Ster_', 'Coag_Ster_Misc_', 'Coag_Ster_Misc_MonoI_', 'Coag_Ster_Misc_MonoI_ViraTrgt_', 'Coag_Ster_Misc_MonoSP_', 'Coag_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_Ster_Misc_ViraTrgt_', 'Coag_Ster_MonoI_', 'Coag_Ster_MonoI_ViraTrgt_', 'Coag_Ster_MonoSP_', 'Coag_Ster_MonoSP_ViraTrgt_', 'Coag_Ster_ViraTrgt_', 'Coag_ViraTrgt_', 'Coag_ViraUnp_', 'Coag_ViraUnp_Misc_', 'Coag_ViraUnp_Misc_MonoI_', 'Coag_ViraUnp_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Misc_MonoSP_', 'Coag_ViraUnp_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Misc_ViraTrgt_', 'Coag_ViraUnp_MonoI_', 'Coag_ViraUnp_MonoI_ViraTrgt_', 'Coag_ViraUnp_MonoSP_', 'Coag_ViraUnp_Ster_', 'Coag_ViraUnp_Ster_Misc_', 'Coag_ViraUnp_Ster_Misc_MonoI_', 'Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_MonoSP_', 'Coag_ViraUnp_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoI_', 'Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoSP_', 'Coag_ViraUnp_Ster_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_ViraTrgt_', 'Coag_ViraUnp_ViraTrgt_', 'Misc_', 'Misc_MonoI_', 'Misc_MonoI_ViraTrgt_', 'Misc_MonoSP_', 'Misc_MonoSP_ViraTrgt_', 'Misc_ViraTrgt_', 'MonoI_', 'MonoI_ViraTrgt_', 'MonoSP_', 'MonoSP_ViraTrgt_', 'Ster_', 'Ster_Misc_', 'Ster_Misc_MonoI_', 'Ster_Misc_MonoI_ViraTrgt_', 'Ster_Misc_MonoSP_', 'Ster_Misc_MonoSP_ViraTrgt_', 'Ster_Misc_ViraTrgt_', 'Ster_MonoI_', 'Ster_MonoI_ViraTrgt_', 'Ster_MonoSP_', 'Ster_MonoSP_MonoI_', 'Ster_MonoSP_ViraTrgt_', 'Ster_ViraTrgt_', 'ViraTrgt_', 'ViraUnp_', 'ViraUnp_Misc_', 'ViraUnp_Misc_ViraTrgt_', 'ViraUnp_MonoI_', 'ViraUnp_MonoSP_', 'ViraUnp_Ster_', 'ViraUnp_Ster_Misc_', 'ViraUnp_Ster_Misc_MonoI_', 'ViraUnp_Ster_Misc_MonoSP_', 'ViraUnp_Ster_Misc_ViraTrgt_', 'ViraUnp_Ster_MonoI_', 'ViraUnp_Ster_MonoI_ViraTrgt_', 'ViraUnp_Ster_MonoSP_', 'ViraUnp_Ster_MonoSP_ViraTrgt_', 'ViraUnp_Ster_ViraTrgt_', 'ViraUnp_ViraTrgt_']

    all_treat_new = all_treat_new[treatments]
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns # pts_col_no_treat +   treat_unique

    # for drug, b_name in zip(['Steroid Preparations','Anticoagulants','Antiviral Agents','Miscellaneous Antibiotics','Miscellaneous','Monoclonal Antibodies'],
    #                           ['Ster',                'Coag',             'Vira',             'Biot',                     'Misc',         'Mono']):

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    # pts_col = [ x.replace('Ster','Steroid').replace('Coag','Anticoagulants').replace('Vira','Antiviral').replace('Biot','Mis.Antibiotics').replace('Misc','Miscellaneous').replace('Mono','Monoclonal') for x in pts_col]

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]

    fig = plt.figure(figsize=(10,4)) 
    fig.subplots_adjust(left=0.3)
    ax1 = plt.subplot2grid((2,2),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((2,2),(0,1), rowspan = 2,colspan = 1)

    positives = overall_importance_df[overall_importance_df>0].sort_values(ascending=True)
    negatives = overall_importance_df[overall_importance_df<=0].sort_values(ascending=False)

    positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, top )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, top )) # .9 positives.size

    positives[-top:].plot.barh( ax=ax1,color=color_p) # yerr=std[:24], ######[new_Col] [new_Col]
    negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]

    ax2.set_xlim(negatives.max() * 1.03,negatives.min()* 1.2 )

    ax2.set_title("Negative Impacts ")
    ax2.set_xlabel("Mean Shap value (impact on model output)")
    ax2.set_ylabel("Features")
    ax1.set_title("Positive Impacts ")
    ax1.set_xlabel("Mean Shap value (impact on model output)")
    ax1.set_ylabel("Features")
    fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")
    # return(all_treat_new)
    combined_Frame = (positives[(-top):].append(negatives[(-top):])).to_frame().T 
    print(combined_Frame)
    return(     combined_Frame    )

@transform_pandas(
    Output(rid="ri.vector.main.execute.76fc66cc-5b54-4145-ad84-6c1bcef27953"),
    all_treat_new_copied_21q1q2_1=Input(rid="ri.foundry.main.dataset.b26bd452-dea3-438d-9e7c-437d8e6b5b79")
)
@output_image_type('svg')
def shap_graph_top20_med_2_1_1(all_treat_new_copied_21q1q2_1):
    all_treat_new = all_treat_new_copied_21q1q2_1

    top=10

    treatments =  ['BiotMQ_', 'BiotMQ_Coag_', 'BiotMQ_Coag_Misc_', 'BiotMQ_Coag_Misc_MonoI_', 'BiotMQ_Coag_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Misc_MonoSP_', 'BiotMQ_Coag_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Misc_ViraTrgt_', 'BiotMQ_Coag_MonoI_', 'BiotMQ_Coag_MonoI_ViraTrgt_', 'BiotMQ_Coag_MonoSP_', 'BiotMQ_Coag_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_', 'BiotMQ_Coag_Ster_Misc_', 'BiotMQ_Coag_Ster_Misc_MonoI_', 'BiotMQ_Coag_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_MonoSP_', 'BiotMQ_Coag_Ster_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoI_', 'BiotMQ_Coag_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_Ster_MonoSP_', 'BiotMQ_Coag_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Coag_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_', 'BiotMQ_Coag_ViraUnp_Misc_', 'BiotMQ_Coag_ViraUnp_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoI_', 'BiotMQ_Coag_ViraUnp_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_MonoSP_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_', 'BiotMQ_Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_Ster_MonoSP_', 'BiotMQ_Coag_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_Coag_ViraUnp_ViraTrgt_', 'BiotMQ_Misc_', 'BiotMQ_Misc_MonoSP_', 'BiotMQ_Misc_MonoSP_ViraTrgt_', 'BiotMQ_Misc_ViraTrgt_', 'BiotMQ_MonoI_', 'BiotMQ_MonoI_ViraTrgt_', 'BiotMQ_MonoSP_', 'BiotMQ_Ster_', 'BiotMQ_Ster_Misc_', 'BiotMQ_Ster_Misc_MonoI_', 'BiotMQ_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_Ster_Misc_MonoSP_', 'BiotMQ_Ster_Misc_ViraTrgt_', 'BiotMQ_Ster_MonoI_', 'BiotMQ_Ster_MonoI_ViraTrgt_', 'BiotMQ_Ster_MonoSP_', 'BiotMQ_Ster_MonoSP_ViraTrgt_', 'BiotMQ_Ster_ViraTrgt_', 'BiotMQ_ViraTrgt_', 'BiotMQ_ViraUnp_', 'BiotMQ_ViraUnp_Misc_', 'BiotMQ_ViraUnp_Misc_MonoI_', 'BiotMQ_ViraUnp_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_MonoI_', 'BiotMQ_ViraUnp_Ster_', 'BiotMQ_ViraUnp_Ster_Misc_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_', 'BiotMQ_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_Misc_MonoSP_', 'BiotMQ_ViraUnp_Ster_Misc_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoI_', 'BiotMQ_ViraUnp_Ster_MonoI_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_MonoSP_', 'BiotMQ_ViraUnp_Ster_MonoSP_ViraTrgt_', 'BiotMQ_ViraUnp_Ster_ViraTrgt_', 'BiotMQ_ViraUnp_ViraTrgt_', 'Coag_', 'Coag_Misc_', 'Coag_Misc_MonoI_', 'Coag_Misc_MonoI_ViraTrgt_', 'Coag_Misc_MonoSP_', 'Coag_Misc_MonoSP_ViraTrgt_', 'Coag_Misc_ViraTrgt_', 'Coag_MonoI_', 'Coag_MonoI_ViraTrgt_', 'Coag_MonoSP_', 'Coag_MonoSP_ViraTrgt_', 'Coag_Ster_', 'Coag_Ster_Misc_', 'Coag_Ster_Misc_MonoI_', 'Coag_Ster_Misc_MonoI_ViraTrgt_', 'Coag_Ster_Misc_MonoSP_', 'Coag_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_Ster_Misc_ViraTrgt_', 'Coag_Ster_MonoI_', 'Coag_Ster_MonoI_ViraTrgt_', 'Coag_Ster_MonoSP_', 'Coag_Ster_MonoSP_ViraTrgt_', 'Coag_Ster_ViraTrgt_', 'Coag_ViraTrgt_', 'Coag_ViraUnp_', 'Coag_ViraUnp_Misc_', 'Coag_ViraUnp_Misc_MonoI_', 'Coag_ViraUnp_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Misc_MonoSP_', 'Coag_ViraUnp_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Misc_ViraTrgt_', 'Coag_ViraUnp_MonoI_', 'Coag_ViraUnp_MonoI_ViraTrgt_', 'Coag_ViraUnp_MonoSP_', 'Coag_ViraUnp_Ster_', 'Coag_ViraUnp_Ster_Misc_', 'Coag_ViraUnp_Ster_Misc_MonoI_', 'Coag_ViraUnp_Ster_Misc_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_MonoSP_', 'Coag_ViraUnp_Ster_Misc_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_Misc_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoI_', 'Coag_ViraUnp_Ster_MonoI_ViraTrgt_', 'Coag_ViraUnp_Ster_MonoSP_', 'Coag_ViraUnp_Ster_MonoSP_ViraTrgt_', 'Coag_ViraUnp_Ster_ViraTrgt_', 'Coag_ViraUnp_ViraTrgt_', 'Misc_', 'Misc_MonoI_', 'Misc_MonoI_ViraTrgt_', 'Misc_MonoSP_', 'Misc_MonoSP_ViraTrgt_', 'Misc_ViraTrgt_', 'MonoI_', 'MonoI_ViraTrgt_', 'MonoSP_', 'MonoSP_ViraTrgt_', 'Ster_', 'Ster_Misc_', 'Ster_Misc_MonoI_', 'Ster_Misc_MonoI_ViraTrgt_', 'Ster_Misc_MonoSP_', 'Ster_Misc_MonoSP_ViraTrgt_', 'Ster_Misc_ViraTrgt_', 'Ster_MonoI_', 'Ster_MonoI_ViraTrgt_', 'Ster_MonoSP_', 'Ster_MonoSP_MonoI_', 'Ster_MonoSP_ViraTrgt_', 'Ster_ViraTrgt_', 'ViraTrgt_', 'ViraUnp_', 'ViraUnp_Misc_', 'ViraUnp_Misc_ViraTrgt_', 'ViraUnp_MonoI_', 'ViraUnp_MonoSP_', 'ViraUnp_Ster_', 'ViraUnp_Ster_Misc_', 'ViraUnp_Ster_Misc_MonoI_', 'ViraUnp_Ster_Misc_MonoSP_', 'ViraUnp_Ster_Misc_ViraTrgt_', 'ViraUnp_Ster_MonoI_', 'ViraUnp_Ster_MonoI_ViraTrgt_', 'ViraUnp_Ster_MonoSP_', 'ViraUnp_Ster_MonoSP_ViraTrgt_', 'ViraUnp_Ster_ViraTrgt_', 'ViraUnp_ViraTrgt_']

    all_treat_new = all_treat_new[treatments]
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    from matplotlib import cm

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns # pts_col_no_treat +   treat_unique

    # for drug, b_name in zip(['Steroid Preparations','Anticoagulants','Antiviral Agents','Miscellaneous Antibiotics','Miscellaneous','Monoclonal Antibodies'],
    #                           ['Ster',                'Coag',             'Vira',             'Biot',                     'Misc',         'Mono']):

    print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    print('type shap: ',type(shap_values))
    print('type shap class: ', type(shap_values[0]))

    num_class_samples_columns =len(shap_values[target_shap_class][0])
    all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    for x in range(shap_values[target_shap_class].shape[0]) :
        all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    print('all_variables avegrafe Shap - shape',all_variables.shape)

    # pts_col = [ x.replace('Ster','Steroid').replace('Coag','Anticoagulants').replace('Vira','Antiviral').replace('Biot','Mis.Antibiotics').replace('Misc','Miscellaneous').replace('Mono','Monoclonal') for x in pts_col]

    overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    overall_importance_df = overall_importance_df.iloc[0]

    fig = plt.figure(figsize=(10,4)) 
    fig.subplots_adjust(left=0.3)
    ax1 = plt.subplot2grid((2,2),(0,0), rowspan = 2,colspan = 1)
    ax2 = plt.subplot2grid((2,2),(0,1), rowspan = 2,colspan = 1)

    positives = overall_importance_df[overall_importance_df>0].sort_values(ascending=True)
    negatives = overall_importance_df[overall_importance_df<=0].sort_values(ascending=False)

    positives.index = positives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    negatives.index = negatives.index.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','OS day 1 ')
    # str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
    color_n = cm.OrRd(np.linspace(.1, .9, top )) # .9 negatives.size
    color_p = cm.GnBu(np.linspace(.1, .9, top )) # .9 positives.size

    positives[-top:].plot.barh( ax=ax1,color=color_p) # yerr=std[:24], ######[new_Col] [new_Col]
    negatives[-top:].plot.barh( ax=ax2,color=color_n) # yerr=std[:24], ######[new_Col] [new_Col]

    ax2.set_xlim(negatives.max() * 1.03,negatives.min()* 1.2 )

    ax2.set_title("Negative Impacts ")
    ax2.set_xlabel("Mean Shap value (impact on model output)")
    ax2.set_ylabel("Features")
    ax1.set_title("Positive Impacts ")
    ax1.set_xlabel("Mean Shap value (impact on model output)")
    ax1.set_ylabel("Features")
    fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")
    # return(all_treat_new)
    combined_Frame = (positives[(-top):].append(negatives[(-top):])).to_frame().T 
    print(combined_Frame)
    return(     combined_Frame    )

@transform_pandas(
    Output(rid="ri.vector.main.execute.8d599bb1-5cd3-4de2-8c62-56b2dd613965"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
@output_image_type('svg')
def shap_swarm(all_treat_new, shap_testing_for_explainer):
    shap_testing = shap_testing_for_explainer
    all_treat_new = all_treat_new
    

    import pandas as pd
    import numpy as np
    from matplotlib import pyplot as plt
    import shap

    
    target_shap_class = 1 
    shap_values = [ all_treat_new.values, all_treat_new.values ]

    pts_col = all_treat_new.columns.str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')

    shap.summary_plot(shap_values[target_shap_class], shap_testing[:-2], feature_names=pts_col) # plot_type="bar"
    # shap.plots.beeswarm(shap_values, max_display=20)
    # shap.plots.beeswarm(shap_values, order=shap_values.abs.max(0))
    # https://shap.readthedocs.io/en/latest/example_notebooks/api_examples/plots/beeswarm.html
    
    # fig.tight_layout()
    plt.tight_layout()
    plt.show()
    print("show")
    

    # print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    # print('type shap: ',type(shap_values))
    # print('type shap class: ', type(shap_values[0]))

    # num_class_samples_columns =len(shap_values[target_shap_class][0])
    # all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    # for x in range(shap_values[target_shap_class].shape[0]) :
    #     all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    # all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    # print('all_variables avegrafe Shap - shape',all_variables.shape)

    # overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    # overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    # overall_importance_df = overall_importance_df.iloc[0]

    # fig = plt.figure(figsize=(15,5)) 
    # ax2 = plt.subplot2grid((3,3),(0,0), rowspan = 3,colspan = 3)

    # overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
    # ax2.set_title("Feature importances ")
    # ax2.set_ylabel("Shap value")
    # ax2.set_xlabel("Features")
    # fig.tight_layout()
    # plt.tight_layout()
    # plt.show()
    # print("show")
    return(all_treat_new)

@transform_pandas(
    Output(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def shap_testing_for_explainer(derived_cobination):
    derived_cobination = derived_cobination

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'pressor_days',
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] #+  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc), 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), 'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=100)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    # tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)
    
    ## __plot
    if True: 
        fig = plt.figure(figsize=(15,15)) 
        ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        fpr, tpr, threshold = final_results['roc_curve_val']
        roc_auc = auc(fpr, tpr)
        ax1.set_title('Receiver Operating Characteristic')
        ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        ax1.legend(loc = 'lower right')
        ax1.plot([0, 1], [0, 1],'r--')
        ax1.set_xlim([0, 1])
        ax1.set_ylim([0, 1])
        ax1.set_ylabel('True Positive Rate')
        ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            propensity = LogisticRegression()
            propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            print ('pscore[:5]',pscore[:5])
            xy_overal['Propensity'] = pscore

            print("\nMatchFrame")
            g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            matched_frame = pd.DataFrame()
            counter =0
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            for index, row in g_death.iterrows():
                g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
                closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
                matched_frame = matched_frame.append(row, ignore_index = True)
                matched_frame = matched_frame.append(closest, ignore_index = True)
                g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
                # if counter ==1000:
                #     print(matched_frame[['Propensity','prop_diff']].head(10))
                #     break
                # counter +=1
            print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            print('matched_frame.shape',matched_frame.shape)
            print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            # print('\nSMOTE')
            # sm = RandomOverSampler(random_state = 7)
            # print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            # x_overal, y_overal = sm.fit_sample(x_overal[pts_col], y_overal[outcome])
            # print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            # if model_type == 'classification' :
            #     y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
            #     print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            # if model_name == 'full':
            #     final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            # if model_name == 'rf':
            #     final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            # model = final_results['model']

            # print("\nKmean start datetime.now(): ", datetime.now())
            # kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            # kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            # print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            # print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            # centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            # print('centroids',centroids.shape)
            # pts_col.extend(['os_day1_1.0',  'os_day1_3.0',  'os_day1_5.0',  'os_day1_7.0',  'os_day1_9.0']) #'os_day1')
            # training = centroids           
            testing = matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            # if model_name == 'rf':
            #     explainer = shap.KernelExplainer(model.predict_proba, training)
            # else:
            #     explainer = shap.KernelExplainer(model.predict, training)
            # shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)

            return(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1))

    #         print("Shap datetime.now(): ", datetime.now())
                
    #         print('\nSHAP: \nClasses:',len(shap_values),'\nSamples:', len(shap_values[0]),'\nColumns/features:',len(shap_values[0][0]),'\nvalue:',shap_values[0][0][0])
    #         print('type shap: ',type(shap_values))
    #         print('type shap class: ', type(shap_values[0]))

    #         num_class_samples_columns =len(shap_values[target_shap_class][0])
    #         all_variables = np.array([0 for _ in range(num_class_samples_columns)])
    #         for x in range(shap_values[target_shap_class].shape[0]) :
    #             all_variables = np.add(all_variables,shap_values[target_shap_class][x] )
    #         all_variables = np.divide(all_variables, shap_values[target_shap_class].shape[0])
    #         print('all_variables avegrafe Shap - shape',all_variables.shape)

    #         overall_importance_df = pd.DataFrame(data=all_variables, columns=['values'],index=pts_col)
    #         overall_importance_df = overall_importance_df.T.sort_values(by="values", axis=1, ascending=False) 
    #         overall_importance_df = overall_importance_df.iloc[0]

    #         overall_importance_df.sort_values(ascending=False).plot.bar( ax=ax2) # yerr=std[:24], ######[new_Col] [new_Col]
    #         ax2.set_title("Feature importances ")
    #         ax2.set_ylabel("Shap value")
    #         fig.tight_layout()
    #         plt.tight_layout()
    #         plt.show()
    #         print("show")
   
    #         print("overall_importance_df Shap")
    #         imp_rf = pd.DataFrame() 
    #         imp_rf = pd.concat([imp_rf, overall_importance_df.to_frame().T]) 
    #         # return( imp_rf ) 

    #         print("Detail Shap")
    #         detailed_shap_df = pd.DataFrame(data=shap_values[target_shap_class], columns=pts_col)
    #         return(detailed_shap_df)
            
    #     print('\nFinal accuracy Table')
    #     param_grid_ = []
    #     if model_name != 'rf':
    #         param_grid_ = pd.DataFrame(data={'param_grid': [str(param_grid)]})

    #     accuracy_ = pd.DataFrame(data={'accuracy_score_val': [final_results['accuracy_score_val']]})
    #     f1_ =  pd.DataFrame(data={'f1_score_val':[final_results['f1_score_val']]})
    #     precision_ = pd.DataFrame(data={'precision_score_val':[final_results['precision_score_val']] })
    #     recall_ = pd.DataFrame(data={'recall_score_val':[final_results['recall_score_val']]})
    #     roc_ = pd.DataFrame(data={'Roc_auc_score_val':[final_results['Roc_auc_score_val']]})
    #     finalResults = pd.concat([  accuracy_, f1_ , precision_ , recall_,roc_ ], axis=1)
        
    #     confusion_= confusion_matrix(final_results['y_val'],final_results['predictions']) 
    #     for class_ in range(len(confusion_)):
    #         class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
    #         class_str ="class_"+str(class_)+"_accuracy"
    #         class_accuracy_df = pd.DataFrame(data={class_str: [class_accuracy]})
    #         finalResults = pd.concat([  finalResults , class_accuracy_df ], axis=1)
    #     finalResults = finalResults.columns.to_frame().T.append(finalResults, ignore_index=True)
    #     print("datetime.now(): ", datetime.now())
    #     plt.tight_layout()
    #     plt.show()
    #     print("show")
    #     return(finalResults)
    
    # print("Error shoud not get here: datetime.now(): ", datetime.now())
    # return( pd.DataFrame( {'return' : [0]} ) ) 

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.2091cf32-b05e-4dc9-a3c2-5256872ebb6a"),
    derived_cobination_delta_2_216=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64")
)
def stat(derived_cobination_delta_2_216):
    # all_treat_new_copied_q3q4 = all_treat_new_copied_20q3q4

    ######
    input_table = derived_cobination_delta_2_216 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    import tensorflow as tf
    from tensorflow import keras
    from tensorflow.keras import backend as K
    print(K.image_data_format())
    from tensorflow.keras import models
    from tensorflow.keras import layers
    from tensorflow.keras import activations
    from tensorflow.keras.models import Model
    from tensorflow.keras.models import Sequential
    from tensorflow.keras.layers import BatchNormalization
    from tensorflow.keras.layers import AveragePooling2D
    from tensorflow.keras.layers import MaxPooling2D
    from tensorflow.keras.layers import AveragePooling1D
    from tensorflow.keras.layers import MaxPooling1D
    from tensorflow.keras.layers import Conv2D
    from tensorflow.keras.layers import Conv1D
    from tensorflow.keras.layers import Activation
    from tensorflow.keras.layers import Dropout
    from tensorflow.keras.layers import Flatten
    from tensorflow.keras.layers import Input
    from tensorflow.keras.layers import Dense
    from tensorflow.keras.layers import concatenate
    from tensorflow.keras.layers import Attention
    from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    from tensorflow.keras.preprocessing import image
    from tensorflow.keras.preprocessing.image import img_to_array, load_img
    from tensorflow.keras.preprocessing.image import ImageDataGenerator
    from tensorflow.keras.applications.inception_v3 import InceptionV3
    from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    print("Keras ", keras.__version__)
    print("TF ",tf.__version__)
    if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    
  

    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)

    # '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1'
    # pts =  pts[   (pts['2020Q2']== 1) | (pts['2020Q1']== 11)  ]
    # pts =  pts[   (pts['date_of_earliest_covid_diagnosis']== '2021Q3') | (pts['date_of_earliest_covid_diagnosis']== '2021Q4')  ]
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                 '2021Q3', '2021Q4', #'2021Q2', '2021Q1','2020Q4','2020Q3',
                'pressor_days',
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))
        pts = pd.concat([ pts , onehot ], axis=1)

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    print
    for x,y in [ ['2020Q1','2020Q2'],['2020Q3','2020Q4'],['2021Q1','2021Q2'],['2021Q3','2021Q4'] ]:
        print(x,y)
        pts_of_qXqY =  pts[   (pts['date_of_earliest_covid_diagnosis']== x) | (pts['date_of_earliest_covid_diagnosis']== y)  ]
        dis = pts_of_qXqY['2_code_1.0'].sum()
        death = pts_of_qXqY['2_code_0.0'].sum()
        print(dis,death,dis+death,(dis)/(dis+death))
        for treat1, treat2 in [['Coag_','Coag_Ster_'], ['Coag_Ster_','Coag_']]:
            print("Treatment !=0", treat1)
            pts_treat = pts_of_qXqY[   (pts_of_qXqY[treat1]!= 0)  &    (pts_of_qXqY[treat2]== 0) ]
            dis = pts_treat['2_code_1.0'].sum()
            death = pts_treat['2_code_0.0'].sum()
            print(dis,death,dis+death,(dis)/(dis+death))

@transform_pandas(
    Output(rid="ri.vector.main.execute.47e90751-1e20-44b3-b887-e4d228fb3cd9"),
    ROC_CURVE_10fold=Input(rid="ri.foundry.main.dataset.9d0c0395-cfa5-4b15-9d02-7bd9d9ddf0c5"),
    roc_l1_cnn_1=Input(rid="ri.foundry.main.dataset.861bcd4e-3156-4f98-b7fb-342a61fa8227"),
    roc_l1_nn=Input(rid="ri.foundry.main.dataset.46e53ad0-a8b7-4df2-81c3-25691f61b4d3")
)
@output_image_type('svg')
def unnamed( roc_l1_nn, ROC_CURVE_10fold, roc_l1_cnn_1):

    roc_l1_cnn = roc_l1_cnn_1
    
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    from sklearn.metrics import auc

    print(pd.__version__)
    
    fig, ax = plt.subplots(figsize=(5,5), dpi=600)
    
    ROC_CURVE_10fold['mean_fpr'] = ROC_CURVE_10fold[['mean_fpr']].sort_values(ascending=True, by=['mean_fpr']).reset_index(drop=True) # ,ignore_index=True
    ROC_CURVE_10fold['mean_tpr'] = ROC_CURVE_10fold[['mean_tpr']].sort_values(ascending=True, by=['mean_tpr']).reset_index(drop=True) # ,ignore_index=True
    # for col in ROC_CURVE_10fold:
    #     ROC_CURVE_10fold[col] = ROC_CURVE_10fold[col].sort_values(ignore_index=True)
    mean_auc = auc(ROC_CURVE_10fold['mean_fpr'],ROC_CURVE_10fold['mean_tpr'])
    std_auc = 0.1 # np.std(aucs)
    label=r"GBDT (AUC = %0.2f)" % mean_auc # std_auc Mean ROC 
    plt.plot(ROC_CURVE_10fold['mean_fpr'],ROC_CURVE_10fold['mean_tpr'],label=label)
    # plt.legend()

    roc_l1_cnn['mean_fpr'] = roc_l1_cnn[['mean_fpr']].sort_values(ascending=True, by=['mean_fpr']).reset_index(drop=True) # ,ignore_index=True
    roc_l1_cnn['mean_tpr'] = roc_l1_cnn[['mean_tpr']].sort_values(ascending=True, by=['mean_tpr']).reset_index(drop=True) # ,ignore_index=True
    # for col in ROC_CURVE_10fold:
    #     roc_l1_cnn[col] = roc_l1_cnn[col].sort_values(ignore_index=True)
    mean_auc = auc(roc_l1_cnn['mean_fpr'],roc_l1_cnn['mean_tpr'])
    std_auc = 0.1 # np.std(aucs)
    label=r"CNN (AUC = %0.2f)" % mean_auc # std_auc Mean ROC 
    plt.plot(roc_l1_cnn['mean_fpr'],roc_l1_cnn['mean_tpr'],label=label)

    roc_l1_nn['mean_fpr'] = roc_l1_nn[['mean_fpr']].sort_values(ascending=True, by=['mean_fpr']).reset_index(drop=True) # ,ignore_index=True
    roc_l1_nn['mean_tpr'] = roc_l1_nn[['mean_tpr']].sort_values(ascending=True, by=['mean_tpr']).reset_index(drop=True) # ,ignore_index=True
    # for col in ROC_CURVE_10fold:
    #     roc_l1_nn[col] = roc_l1_nn[col].sort_values(ignore_index=True)
    mean_auc = auc(roc_l1_nn['mean_fpr'],roc_l1_nn['mean_tpr'])
    std_auc = 0.1 # np.std(aucs)
    label=r"DNN (AUC = %0.2f)" % mean_auc # std_auc Mean ROC 
    plt.plot(roc_l1_nn['mean_fpr'],roc_l1_nn['mean_tpr'],label=label)
    
    ax.plot([0, 1], [0, 1], linestyle="--", lw=2, color="r", label="Chance", alpha=0.8)
    #     label=r"Neural Network Mean ROC (AUC = %0.2f $\pm$ %0.2f)" % (mean_auc, std_auc), # std_auc 
    ax.set(
        xlim=[-0.05, 1.05],
        ylim=[-0.05, 1.05],
        title="Receiver Operating Characteristic (ROC)",
    )
    ax.legend(loc="lower right")
    ax.set(xlabel='False Positive Rate')
    ax.set(ylabel='True Positive Rate')

    SMALL_SIZE = 8
    MEDIUM_SIZE = 10
    BIGGER_SIZE = 12

    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes
    plt.rc('axes', titlesize=BIGGER_SIZE)     # fontsize of the axes title
    plt.rc('axes', labelsize=BIGGER_SIZE)    # fontsize of the x and y labels
    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels
    plt.rc('legend', fontsize=MEDIUM_SIZE)    # legend fontsize
    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure titlesize

    plt.show()

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.d52a28e2-e3ec-4f41-9b45-a211cdb5ff8a"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
# from transforms.api import transform, Input, Output
# from foundry_ml import Model
@output_image_type('svg')
def unnamed_1_1(derived_cobination, shap_testing_for_explainer, all_treat_new):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        # sm = RandomOverSampler(random_state = 7) 
        # x_train, y_train = sm.fit_sample(x_train, y_train)
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_resample(x_train, y_train) # sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc),# 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    # tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)

    # model = Model(*model stages)
    # model.save(out_model)
    
    ## __plot
    if True: 
        # fig = plt.figure(figsize=(15,15)) 
        # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        # fpr, tpr, threshold = final_results['roc_curve_val']
        # roc_auc = auc(fpr, tpr)
        # ax1.set_title('Receiver Operating Characteristic')
        # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        # ax1.legend(loc = 'lower right')
        # ax1.plot([0, 1], [0, 1],'r--')
        # ax1.set_xlim([0, 1])
        # ax1.set_ylim([0, 1])
        # ax1.set_ylabel('True Positive Rate')
        # ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            # propensity = LogisticRegression()
            # propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            # pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            # print ('pscore[:5]',pscore[:5])
            # xy_overal['Propensity'] = pscore

            # print("\nMatchFrame")
            # g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            # matched_frame = pd.DataFrame()
            # counter =0
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # for index, row in g_death.iterrows():
            #     g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
            #     closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
            #     matched_frame = matched_frame.append(row, ignore_index = True)
            #     matched_frame = matched_frame.append(closest, ignore_index = True)
            #     g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
            #     # if counter ==1000:
            #     #     print(matched_frame[['Propensity','prop_diff']].head(10))
            #     #     break
            #     # counter +=1
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # print('matched_frame.shape',matched_frame.shape)
            # print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_resample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = shap_testing_for_explainer # matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = all_treat_new # explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            shap_values.columns = shap_values.columns.str.replace('dementia', 'Dementia').str.replace('gender_female', 'Female').str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
            ## training = centroids   
            # explainer = shap.KernelExplainer(model.predict_proba, training)
            # explainer = shap.Explainer(model)
            
            ## testing = shap_testing_for_explainer
            ## shap_values = all_treat_new
            # shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            # shap_values = explainer(X)

            # explainer = shap.Explainer(model)
            # shap_values = explainer(X)

              # In v0.20 force_plot now requires the base value as the first parameter
            # https://github.com/slundberg/shap
            # return(pd.DataFrame({'A' : [np.nan]}))
            # shap.plots.waterfall(all_treat_new.values[0])
            fig = plt.figure(figsize=(10,10))
            print("Plot Shap force")
            # https://towardsdatascience.com/tutorial-on-displaying-shap-force-plots-in-python-html-4883aeb0ee7c
            patient_num_Shap = 2456
            shap.plots.force(explainer.expected_value[0],all_treat_new.values[ patient_num_Shap],matplotlib=True,feature_names=all_treat_new.columns,plot_cmap=['#77dd77', '#f99191'])
            plt.show()
            return(pd.DataFrame({'A' : [np.nan]}))

            shap.plots.waterfall(all_treat_new.values[0])

            # explainer = shap.KernelExplainer(svm.predict_proba, X_train, link="logit")
            # shap_values = explainer.shap_values(X_test, nsamples=100)
            # shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link="logit")
            shap.plots.force(all_treat_new.values[0])
            plt.show()
            
            return(pd.DataFrame())
   

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.72d17748-10e3-4f09-bd45-5aafa031e1b5"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
# from transforms.api import transform, Input, Output
# from foundry_ml import Model
@output_image_type('svg')
def unnamed_2_1(derived_cobination, shap_testing_for_explainer, all_treat_new):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        # sm = RandomOverSampler(random_state = 7) 
        # x_train, y_train = sm.fit_sample(x_train, y_train)
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_resample(x_train, y_train) # sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc),# 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    # tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)

    # model = Model(*model stages)
    # model.save(out_model)
    
    ## __plot
    if True: 
        # fig = plt.figure(figsize=(15,15)) 
        # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        # fpr, tpr, threshold = final_results['roc_curve_val']
        # roc_auc = auc(fpr, tpr)
        # ax1.set_title('Receiver Operating Characteristic')
        # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        # ax1.legend(loc = 'lower right')
        # ax1.plot([0, 1], [0, 1],'r--')
        # ax1.set_xlim([0, 1])
        # ax1.set_ylim([0, 1])
        # ax1.set_ylabel('True Positive Rate')
        # ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            # propensity = LogisticRegression()
            # propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            # pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            # print ('pscore[:5]',pscore[:5])
            # xy_overal['Propensity'] = pscore

            # print("\nMatchFrame")
            # g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            # matched_frame = pd.DataFrame()
            # counter =0
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # for index, row in g_death.iterrows():
            #     g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
            #     closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
            #     matched_frame = matched_frame.append(row, ignore_index = True)
            #     matched_frame = matched_frame.append(closest, ignore_index = True)
            #     g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
            #     # if counter ==1000:
            #     #     print(matched_frame[['Propensity','prop_diff']].head(10))
            #     #     break
            #     # counter +=1
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # print('matched_frame.shape',matched_frame.shape)
            # print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_resample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = shap_testing_for_explainer # matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = all_treat_new # explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            shap_values.columns = shap_values.columns.str.replace('dementia', 'Dementia').str.replace('gender_female', 'Female').str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
            ## training = centroids   
            # explainer = shap.KernelExplainer(model.predict_proba, training)
            # explainer = shap.Explainer(model)
            
            ## testing = shap_testing_for_explainer
            ## shap_values = all_treat_new
            # shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            # shap_values = explainer(X)

            # explainer = shap.Explainer(model)
            # shap_values = explainer(X)

              # In v0.20 force_plot now requires the base value as the first parameter
            # https://github.com/slundberg/shap
            # return(pd.DataFrame({'A' : [np.nan]}))
            # shap.plots.waterfall(all_treat_new.values[0])
            fig = plt.figure(figsize=(10,10))
            print("Plot Shap force")
            # https://towardsdatascience.com/tutorial-on-displaying-shap-force-plots-in-python-html-4883aeb0ee7c
            patient_num_Shap = 1956
            shap.plots.force(explainer.expected_value[0],all_treat_new.values[ patient_num_Shap],matplotlib=True,feature_names=all_treat_new.columns,plot_cmap=['#77dd77', '#f99191'])
            plt.show()
            return(pd.DataFrame({'A' : [np.nan]}))

            shap.plots.waterfall(all_treat_new.values[0])

            # explainer = shap.KernelExplainer(svm.predict_proba, X_train, link="logit")
            # shap_values = explainer.shap_values(X_test, nsamples=100)
            # shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link="logit")
            shap.plots.force(all_treat_new.values[0])
            plt.show()
            
            return(pd.DataFrame())
   

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.8fb2724f-c5c9-4aff-83e9-319e7c043e32"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
# from transforms.api import transform, Input, Output
# from foundry_ml import Model
@output_image_type('svg')
def unnamed_3_1(derived_cobination, shap_testing_for_explainer, all_treat_new):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        # sm = RandomOverSampler(random_state = 7) 
        # x_train, y_train = sm.fit_sample(x_train, y_train)
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_resample(x_train, y_train) # sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc),# 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    # tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)

    # model = Model(*model stages)
    # model.save(out_model)
    
    ## __plot
    if True: 
        # fig = plt.figure(figsize=(15,15)) 
        # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        # fpr, tpr, threshold = final_results['roc_curve_val']
        # roc_auc = auc(fpr, tpr)
        # ax1.set_title('Receiver Operating Characteristic')
        # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        # ax1.legend(loc = 'lower right')
        # ax1.plot([0, 1], [0, 1],'r--')
        # ax1.set_xlim([0, 1])
        # ax1.set_ylim([0, 1])
        # ax1.set_ylabel('True Positive Rate')
        # ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            # propensity = LogisticRegression()
            # propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            # pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            # print ('pscore[:5]',pscore[:5])
            # xy_overal['Propensity'] = pscore

            # print("\nMatchFrame")
            # g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            # matched_frame = pd.DataFrame()
            # counter =0
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # for index, row in g_death.iterrows():
            #     g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
            #     closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
            #     matched_frame = matched_frame.append(row, ignore_index = True)
            #     matched_frame = matched_frame.append(closest, ignore_index = True)
            #     g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
            #     # if counter ==1000:
            #     #     print(matched_frame[['Propensity','prop_diff']].head(10))
            #     #     break
            #     # counter +=1
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # print('matched_frame.shape',matched_frame.shape)
            # print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_resample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = shap_testing_for_explainer # matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = all_treat_new # explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            shap_values.columns = shap_values.columns.str.replace('dementia', 'Dementia').str.replace('gender_female', 'Female').str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
            ## training = centroids   
            # explainer = shap.KernelExplainer(model.predict_proba, training)
            # explainer = shap.Explainer(model)
            
            ## testing = shap_testing_for_explainer
            ## shap_values = all_treat_new
            # shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            # shap_values = explainer(X)

            # explainer = shap.Explainer(model)
            # shap_values = explainer(X)

              # In v0.20 force_plot now requires the base value as the first parameter
            # https://github.com/slundberg/shap
            # return(pd.DataFrame({'A' : [np.nan]}))
            # shap.plots.waterfall(all_treat_new.values[0])
            fig = plt.figure(figsize=(10,10))
            print("Plot Shap force")
            # https://towardsdatascience.com/tutorial-on-displaying-shap-force-plots-in-python-html-4883aeb0ee7c
            patient_num_Shap = 3456
            shap.plots.force(explainer.expected_value[0],all_treat_new.values[ patient_num_Shap],matplotlib=True,feature_names=all_treat_new.columns,plot_cmap=['#77dd77', '#f99191'])
            plt.show()
            return(pd.DataFrame({'A' : [np.nan]}))

            shap.plots.waterfall(all_treat_new.values[0])

            # explainer = shap.KernelExplainer(svm.predict_proba, X_train, link="logit")
            # shap_values = explainer.shap_values(X_test, nsamples=100)
            # shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link="logit")
            shap.plots.force(all_treat_new.values[0])
            plt.show()
            
            return(pd.DataFrame())
   

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.2f362997-7c73-4b8c-8755-96be342becd2"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
# from transforms.api import transform, Input, Output
# from foundry_ml import Model
@output_image_type('svg')
def unnamed_4_1(derived_cobination, shap_testing_for_explainer, all_treat_new):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        # sm = RandomOverSampler(random_state = 7) 
        # x_train, y_train = sm.fit_sample(x_train, y_train)
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_resample(x_train, y_train) # sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc),# 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    # tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)

    # model = Model(*model stages)
    # model.save(out_model)
    
    ## __plot
    if True: 
        # fig = plt.figure(figsize=(15,15)) 
        # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        # fpr, tpr, threshold = final_results['roc_curve_val']
        # roc_auc = auc(fpr, tpr)
        # ax1.set_title('Receiver Operating Characteristic')
        # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        # ax1.legend(loc = 'lower right')
        # ax1.plot([0, 1], [0, 1],'r--')
        # ax1.set_xlim([0, 1])
        # ax1.set_ylim([0, 1])
        # ax1.set_ylabel('True Positive Rate')
        # ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            # propensity = LogisticRegression()
            # propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            # pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            # print ('pscore[:5]',pscore[:5])
            # xy_overal['Propensity'] = pscore

            # print("\nMatchFrame")
            # g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            # matched_frame = pd.DataFrame()
            # counter =0
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # for index, row in g_death.iterrows():
            #     g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
            #     closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
            #     matched_frame = matched_frame.append(row, ignore_index = True)
            #     matched_frame = matched_frame.append(closest, ignore_index = True)
            #     g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
            #     # if counter ==1000:
            #     #     print(matched_frame[['Propensity','prop_diff']].head(10))
            #     #     break
            #     # counter +=1
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # print('matched_frame.shape',matched_frame.shape)
            # print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_resample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = shap_testing_for_explainer # matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = all_treat_new # explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            shap_values.columns = shap_values.columns.str.replace('dementia', 'Dementia').str.replace('gender_female', 'Female').str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
            ## training = centroids   
            # explainer = shap.KernelExplainer(model.predict_proba, training)
            # explainer = shap.Explainer(model)
            
            ## testing = shap_testing_for_explainer
            ## shap_values = all_treat_new
            # shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            # shap_values = explainer(X)

            # explainer = shap.Explainer(model)
            # shap_values = explainer(X)

              # In v0.20 force_plot now requires the base value as the first parameter
            # https://github.com/slundberg/shap
            # return(pd.DataFrame({'A' : [np.nan]}))
            # shap.plots.waterfall(all_treat_new.values[0])
            fig = plt.figure(figsize=(10,10))
            print("Plot Shap force")
            # https://towardsdatascience.com/tutorial-on-displaying-shap-force-plots-in-python-html-4883aeb0ee7c
            patient_num_Shap = 3153
            shap.plots.force(explainer.expected_value[0],all_treat_new.values[ patient_num_Shap],matplotlib=True,feature_names=all_treat_new.columns,plot_cmap=['#77dd77', '#f99191'])
            plt.show()
            return(pd.DataFrame({'A' : [np.nan]}))

            shap.plots.waterfall(all_treat_new.values[0])

            # explainer = shap.KernelExplainer(svm.predict_proba, X_train, link="logit")
            # shap_values = explainer.shap_values(X_test, nsamples=100)
            # shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link="logit")
            shap.plots.force(all_treat_new.values[0])
            plt.show()
            
            return(pd.DataFrame())
   

    

@transform_pandas(
    Output(rid="ri.vector.main.execute.3a0bdb47-7795-48f8-8b8c-1cb9c4ee2bbe"),
    all_treat_new=Input(rid="ri.foundry.main.dataset.14090ac1-20f4-4d1c-a75d-d7783c1d7859"),
    derived_cobination=Input(rid="ri.foundry.main.dataset.da3e5d54-de18-4712-b4a0-f2a9359c1f64"),
    shap_testing_for_explainer=Input(rid="ri.foundry.main.dataset.e300cef7-e277-4fc4-bb3a-7930f874eb1c")
)
# from transforms.api import transform, Input, Output
# from foundry_ml import Model
@output_image_type('svg')
def unnamed_5_1(derived_cobination, shap_testing_for_explainer, all_treat_new):

    ######
    input_table = derived_cobination 
    ######################################
    # hyperopt: added to env.
    # Shap
    # Lime
    # seaborn
    # ADD ? 
    # imbalance-learn
    # seaborn: added to env.
    # Keras: added to env.
    # sklearn-pandas
    # keras, keras-vis removed
    # from hyperopt import fmin, tpe, hp, STATUS_OK, Trials,space_eval # rand, GridSearch 
    import time
    import numpy as np
    import pandas as pd
    import matplotlib as mpl
    import matplotlib.cm as cm
    from matplotlib import pyplot as plt
    import shap
    # from vis.visualization import visualize_cam, overlay
    # from vis.utils import utils
    # import torch
    # from tab_transformer_pytorch import TabTransformer
    import seaborn as sns
    from os import path
    import sklearn as sk
    from sklearn.model_selection import train_test_split
    from sklearn.linear_model import LogisticRegression
    from sklearn.metrics import classification_report, confusion_matrix,accuracy_score, auc
    from sklearn.svm import SVC
    from sklearn.naive_bayes import GaussianNB
    from sklearn.neighbors import KNeighborsClassifier
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.neural_network import MLPClassifier
    from sklearn.metrics import roc_auc_score, roc_curve
    from sklearn.metrics import f1_score
    from sklearn.metrics import precision_score
    from sklearn.metrics import recall_score
    from sklearn.metrics import mean_squared_error
    from sklearn.metrics import mean_absolute_error
    # from sklearn.metrics import mean_absolute_percentage_error
    from sklearn.utils import class_weight
    import imblearn 
    from imblearn.over_sampling import SMOTE ####
    from imblearn.under_sampling import RandomUnderSampler
    from imblearn.over_sampling import RandomOverSampler,BorderlineSMOTE, SVMSMOTE, ADASYN
    from sklearn.inspection import permutation_importance
    from sklearn.feature_selection import RFE
    from sklearn.neighbors import LocalOutlierFactor
    from sklearn import preprocessing
    from sklearn.preprocessing import StandardScaler, MinMaxScaler
    from sklearn import model_selection
    from sklearn.utils import shuffle
    # from sklearn_pandas import DataFrameMapper
    import random 
    # import tensorflow as tf
    # from tensorflow import keras
    # from tensorflow.keras import backend as K
    # print(K.image_data_format())
    # from tensorflow.keras import models
    # from tensorflow.keras import layers
    # from tensorflow.keras import activations
    # from tensorflow.keras.models import Model
    # from tensorflow.keras.models import Sequential
    # from tensorflow.keras.layers import BatchNormalization
    # from tensorflow.keras.layers import AveragePooling2D
    # from tensorflow.keras.layers import MaxPooling2D
    # from tensorflow.keras.layers import AveragePooling1D
    # from tensorflow.keras.layers import MaxPooling1D
    # from tensorflow.keras.layers import Conv2D
    # from tensorflow.keras.layers import Conv1D
    # from tensorflow.keras.layers import Activation
    # from tensorflow.keras.layers import Dropout
    # from tensorflow.keras.layers import Flatten
    # from tensorflow.keras.layers import Input
    # from tensorflow.keras.layers import Dense
    # from tensorflow.keras.layers import concatenate
    # from tensorflow.keras.layers import Attention
    # from tensorflow.keras.optimizers import RMSprop, SGD, Adam
    # from keras.models import load_model
    # from tensorflow.keras.models import load_model
    from numpy import expand_dims
    # import tensorflow_datasets as tensorflow_datasets
    # from tensorflow.keras.preprocessing import image
    # from tensorflow.keras.preprocessing.image import img_to_array, load_img
    # from tensorflow.keras.preprocessing.image import ImageDataGenerator
    # from tensorflow.keras.applications.inception_v3 import InceptionV3
    # from tensorflow.keras.callbacks import EarlyStopping  # keras.callbacks 
    # from tensorflow.keras.callbacks import ModelCheckpoint # keras.callbacks 
    # from __future__ import absolute_import, division, print_function, unicode_literals ###
    import matplotlib.image as mpimg
    from scipy import misc 
    from sklearn.model_selection import GridSearchCV,RandomizedSearchCV
    # from tensorflow.keras.wrappers.scikit_learn import KerasClassifier # keras.wrappers.scikit_learn
    np.random.seed(7)
    # tf.random.set_seed(12) 
    from datetime import datetime
    start = datetime.now()
    seconds_in_day = 24 * 60 * 60
    print("datetime.now()",datetime.now())
    print("Numpy ",np.version.version)
    print("Sklearn ",sk.__version__)
    # print("Keras ", keras.__version__)
    # print("TF ",tf.__version__)
    # if tf.test.gpu_device_name(): print('    GPU: {}'.format(tf.test.gpu_device_name()))
    # else: print('    CPU version')
    pd.set_option('display.max_columns', None)
    pd.set_option('display.width', None)
    from sklearn.exceptions import ConvergenceWarning
    import warnings
    warnings.filterwarnings('ignore', category=ConvergenceWarning)
    # warnings.filterwarnings('ignore')
    print()
    ######################################
    def getXfromBestModelfromTrials(trials,x):
        valid_trial_list = [trial for trial in trials if STATUS_OK == trial['result']['status']]
        losses = [ float(trial['result']['loss']) for trial in valid_trial_list]
        index_having_minumum_loss = np.argmin(losses)
        best_trial_obj = valid_trial_list[index_having_minumum_loss]
        return best_trial_obj['result'][x]
    ######################################
    pts = input_table

    ## __control
    #######################################
    # \nscale
    # pts_col
    max_level = False 

    outcome = ['2_code']
    # outcome = ['3_code']
    resample = True # class_weight
    under_sample = False
    
    # model_name = 'full'
    model_name = 'rf'
    # model_name = 'tabt'

    model_type = 'classification'
    # model_type = 'binary'
    # model_type = 'regression'

    model_importance = True
    shap_precentage_test = 10
    target_shap_class = 1 
    optimize =False
    max_eval = 30 
    patience = 15
    verbose = 0
    verbose_model = 0
    
    # 'micro' = total true positives, 'macro' = for each label,unweighted 'weighted' = account for label imbalance, 
    score_average = 'macro'

    input_shape = (6,14,1)
    input_reshape = [6,14]
    paddings= 'valid' # vali no # same do #
    ######################################

    print('\nLimit to 1 & 11')
    print(pts.shape)
    pts =  pts[   (pts['os_day28']== 1) | (pts['os_day28']== 11)  ]
    print(pts.shape)

    ### pts_info
    print('\npts_info lowerCase')
    for col in ['gender','race','ethnicity']:
        pts.loc[:,col]=pts[col].str.lower()
        print(col,": ",pts[col].unique())

    print('\npts_info others-cat add,stat,impute')
    pts.loc[    pts['gender'].isin(['no matching concept','other', 'gender unknown']) ,'gender'    ] = 'other'
    pts.loc[    pts['race'].isin(['no matching concept','unknown', 'no information','unknown racial group','other', 'other race', 'more than one race', 'missing/unknown','asian','native hawaiian or other pacific islander']) ,'race'   ] = 'other'
    pts.loc[    pts['ethnicity'].isin([ 'no matching concept','other','missing/unknown']),'ethnicity'    ] = 'other'
    print('gender, race, ethnicity: added others')
    pts = pts[    pts['age']>0    ].copy() 
    pts = pts[    pts['age']<110    ].copy() 
    pts = pts[    pts['los']<365    ].copy() 
    print('age:' ,pts['age'].min(),pts['age'].max(),pts['age'].mean(), pts.shape)
    print('bmi:' ,pts['bmi'].min(),pts['bmi'].max(),pts['bmi'].mean())
    pts.loc[    pts['bmi'].isna(),'bmi'    ] = pts['bmi'].mean() # 50%
    pts.loc[    pts['age'].isna(),'age'    ] = pts['age'].mean() # 319

    print('\ndate conversion')
    pts.loc[:,'date_of_earliest_covid_diagnosis']= pd.to_datetime(pts['date_of_earliest_covid_diagnosis'],  infer_datetime_format=True) 
    pts.loc[:,'date_of_earliest_covid_diagnosis'] = pd.PeriodIndex(pts['date_of_earliest_covid_diagnosis'], freq='Q').astype(str)       #################################
    first_diagnosis =  pts['date_of_earliest_covid_diagnosis'].min() 
    print('Min dates covid diag: ', first_diagnosis )
    # pts.loc[:,'date_of_earliest_covid_diagnosis']= ((pts['date_of_earliest_covid_diagnosis']- first_diagnosis) / np.timedelta64(1, 'M') )    
    # pts.loc[:,'date_of_earliest_covid_diagnosis'] = pts['date_of_earliest_covid_diagnosis'].astype(int)
        
    print("\nunique column")
    for col in ['gender','race','ethnicity','date_of_earliest_covid_diagnosis']:
        print(col,": ",pts[col].unique())

    print("\nunique column")
    days =  ['day'+str(x) for x in range(1,29,1) ]
    unique_comb_col_val =['comb_'+day for day in days ]
    treat_unique = [x for x in np.unique(pts[unique_comb_col_val].values) if x is not '' ]
    treat_unique = treat_unique
    print('unique():',len(treat_unique),type(treat_unique),treat_unique)
   

    print("\nMissigness")
    for col in pts.columns:
        missigness = pts[col].isna().astype(int).sum() 
        if missigness/float(pts.shape[0])!=0:
            print(col, ": " , missigness/float(pts.shape[0]), missigness,pts.shape[0])
    print("Before Dropna pts:",pts.shape[0])
    pts.dropna(inplace=True) # 
    print("After Dropna pts:",pts.shape[0],"\n")

    print("\npts onehot") # 
    for col in ['gender','race','ethnicity']: # use column name as prefix
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        pts = pts.drop([col], axis=1)
        pts = pts.drop([col+'_other'], axis=1)
    for col in ['os_day1']: #,'max_level']: # use column name as prefix # do not remove
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
    for col in ['date_of_earliest_covid_diagnosis']: # not used column name as prefix 
        onehot = pd.get_dummies(pts[col], drop_first=False)#,prefix=str(col))
        print(onehot.columns)
        pts = pd.concat([ pts , onehot ], axis=1)
        # pts = pts.drop([col], axis=1)

    age_bucket=[]
    for age in range(0,int(pts['age'].max())+1,10):
        pts['age'+str(age+10)]= pts['age'].apply(lambda x:1 if x>=age and x<(age+10)  else 0)
        age_bucket.append('age'+str(age+10))
    pts['age65p']= pts['age'].apply(lambda x:1 if x>=65  else 0)
    pts['age65m']= pts['age'].apply(lambda x:1 if x<65  else 0)
    pts['cci3p']= pts['CCI_INDEX'].apply(lambda x:1 if x>=3  else 0)
    pts['cci3m']= pts['CCI_INDEX'].apply(lambda x:1 if x<3  else 0)

    num_pts = pts['person_id'].nunique()
    print('\nunique patients with change in ordinal scale, not dead with in the first 3 days: ',num_pts)
    print('ROWS   patients with change in ordinal scale, not dead with in the first 3 days: ',pts.shape[0])

    print('\nrename columns')
    print(list(pts.columns))
    new_column_name = dict( [(x, x.replace(' ','_')) for x in pts.columns])
    pts = pts.rename(columns=new_column_name)
    print(pts.head(1)) 
    print("pts.columns: ",str([x for x in pts.columns]))
    print("pts.columns len(): ",len(pts.columns)) 

    ### Generate column names/features
    print('\nset column name')
    days28 =  ['day'+str(x) for x in range(1,29,1) ]
    days14 =  ['day'+str(x) for x in range(1,15,1) ]
    
    os_col = ['os_'+x for x in days28 ]
    
    treatments = ['Ster_','Coag_','Vira_','Biot_','Misc_','Mono_']
    drug_col = [x+y for x in treatments for y in days28]
    
    max_level_s =  [x for x in range (3,max_level+1,2)]
    max_level_treatments = [treatment+str(level) for treatment in treatments  for level in max_level_s ] 

    

    # os_col = [ 'os_day1', 'os_day2', ...]
    # drug_col = ['Ster_day1',..]
    # max_level_treatments  =  ['Ster_3', 'Coag_3' ...etc. 
    # age_bucket = ['age10', ....] # 'age60p' 'age60m'
    # 'time_to_outcome', 'max_level', 'days_to_max_level', 
    # 'los', 'days_max_to_outcome', 
    # 'deter_speed', 'enhanc_speed', 
    
    treat_beforeAfter = []
    for treat in treatments:
        treat_beforeAfter.append(treat +'before_max')
        treat_beforeAfter.append(treat+'after_max')  
    
    print('\nFeatures:')
    pts_col_no_treat = [  'bmi',  'hypertension', 'upper_gi_bleed', 'MI', 'CHF', 'PVD', 'stroke', 'dementia', 'pulmonary', 'rheumatic', 'PUD', 
                'liver_mild', 'liversevere', 'diabetes', 'dmcx', 'paralysis', 'renal', 'cancer', 'mets', 'hiv',
                'gender_female', 'gender_male', # 'gender_other', 
                'race_black_or_african_american', 'race_white', # 'race_asian',, 'race_other', 'race_native_hawaiian_or_other_pacific_islander'
                'ethnicity_hispanic_or_latino', 'ethnicity_not_hispanic_or_latino',# 'ethnicity_other', 
                # 'date_of_earliest_covid_diagnosis',
                '2021Q2', '2021Q1','2020Q4','2020Q3', '2020Q2', '2020Q1', 
                # 'os_day1', 
                'os_day1_1.0','os_day1_3.0', 'os_day1_5.0', 'os_day1_7.0', 'os_day1_9.0', 
                # 'CCI_INDEX',
                # 'cci3p','cci3m',
                # 'CCI_INDEX_0','CCI_INDEX_1','CCI_INDEX_2','CCI_INDEX_3','CCI_INDEX_4','CCI_INDEX_5','CCI_INDEX_6','CCI_INDEX_7','CCI_INDEX_8','CCI_INDEX_9','CCI_INDEX_10','CCI_INDEX_11','CCI_INDEX_12',
                 'age', # age_bucket
                # 'age65p', 'age65m',
                ]  # + age_bucket 
                #'time_to_outcome','days_to_max_level', 'days_max_to_outcome', 'deter_speed' ] + treat_unique #+ treat_beforeAfter 
                # + age_bucket # max 149
                # + max_level_treatments 
    pts_col = pts_col_no_treat +   treat_unique
    print(pts_col)
    print(pts_col_no_treat)
    print("len columns: os_col ",len(os_col),' pts_col ->uniq ',len(pts_col),len(list(set(pts_col))) , ' treat_beforeAfter ',len(treat_beforeAfter) )

    # impORstayedORdeter  98,95,60 >>>  impORnot,  96, 96  >>> impORdeter   98,50 >>>
    print("\nset outcomes")
    pts.loc[(pts['os_day1']  >  (pts['os_day28']) ), '3_code'] = 0 # improved
    pts.loc[(pts['os_day1']  == (pts['os_day28']) ), '3_code'] =  1   # stayed
    pts.loc[(pts['os_day1']  <  (pts['os_day28']) ), '3_code'] = 2  # deteriorated

    pts.loc[(pts['os_day28'] == 1 ), '2_code'] = 1  # discharged
    pts.loc[(pts['os_day28'] == 11), '2_code'] = 0  # death

    for col in ['3_code','2_code']:
        onehot = pd.get_dummies(pts[col], drop_first=False,prefix=str(col))
        print(onehot.sum(axis=0))

    ## set control
    print('\nset control')
    if model_type == 'regression':
        pts =  pts[   (pts['os_day28']== 1)  ]
    print('max level filtering')
    print(pts.shape)
    if max_level == False:
        print("\nno max filtering")
    else:
        pts=pts[pts['max_level']==max_level]
    print(pts.shape)
    demo_shape = len(pts_col) # num features
    final_model = False  
    
    ### SPLIT
    print('\nsplit')
    x_train, x_test, y_train, y_test = train_test_split(pts[pts_col ], pts[outcome], test_size=0.30, random_state=7,shuffle=True,stratify=pts[outcome]) # + drug_col
    
    y_train = pd.DataFrame(data=y_train,columns=outcome)
    x_train = pd.DataFrame(data=x_train,columns=pts_col)

    print("after Train-test, shape train: ",x_train.shape,y_train.shape) 
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0]))
    print('onehot y_train')
    print(onehot.sum(axis=0))
  
    ### Class weight
    print("\nClass weight")
    num_classes = len(pts[outcome[0]].unique())
    if model_type == 'binary' or model_type == 'regression':
        num_classes = 1
    onehot = pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(col))
    class_counts = onehot.sum(axis=0).values
    total_count = sum(class_counts)
    class_rate = [ (total_count/(num_classes*x)) for x in class_counts ]
    class_weights = dict(enumerate(class_rate))
    print("num_classes: ", num_classes,  "class_counts: ", class_counts, "total_count: ", total_count, "class_weights: ", class_weights)

    print("\nscale")
    scale= [ 'bmi'] +  treat_unique # + treat_beforeAfter #  + ['days_to_max_level','time_to_outcome', 'days_max_to_outcome', 'deter_speed' ] , 'CCI_INDEX' [ 'os_day1']  + # 'CCI_INDEX', 'os_day1','date_of_earliest_covid_diagnosis','age'
    #  'age',
    print(scale) 
    scale_x = preprocessing.MinMaxScaler().fit(x_train[scale]) 
    x_train.loc[:,scale] = scale_x.transform(x_train[scale])
    x_test.loc[:,scale] = scale_x.transform(x_test[scale])                 
    
    original_x_train = x_train.copy()
    original_y_train = y_train.copy()
    original_x_test = x_test.copy()
    original_y_test = y_test.copy()
    ### SMOTE
    if resample:
        print('\nSMOTE')
        # sm = RandomOverSampler(random_state = 7) 
        # x_train, y_train = sm.fit_sample(x_train, y_train)
        sm = RandomOverSampler(random_state = 7) 
        x_train, y_train = sm.fit_resample(x_train, y_train) # sm.fit_sample(x_train, y_train)
        print("after sm.fit, shape train: ",x_train.shape,y_train.shape) 
        x_train = pd.DataFrame(data=x_train,columns=x_test.columns)
        y_train = pd.DataFrame(data=y_train,columns=y_test.columns) 
        for c in range(num_classes):
            class_weights[c]=1
        print('class weights reseted: ',class_weights)
    
    ### split learn/val
    print('\nsplit learn/val')
    x_learn, x_val, y_learn, y_val = train_test_split(x_train, y_train, test_size=0.30, random_state=7,shuffle=True,stratify=y_train)
    print("x_learn, x_val, y_learn, y_val",x_learn.shape, x_val.shape, y_learn.shape, y_val.shape)

    print('\nonehot train/test/learn/val')
    if model_type == 'classification' :
        y_train = pd.concat([ y_train , pd.get_dummies(y_train[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_train.sum(axis=0))
        y_test = pd.concat([ y_test , pd.get_dummies(y_test[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_test.sum(axis=0))
        y_learn = pd.concat([ y_learn , pd.get_dummies(y_learn[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_learn.sum(axis=0))
        y_val = pd.concat([ y_val , pd.get_dummies(y_val[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
        print(y_val.sum(axis=0))

    ### Hyper Param 
    ######################################
    param_grid = []
    if optimize == True:
        neuron_comb = hp.choice( 'neuron_comb',[100,200,300,400,600,800,900]) # ,600,900
        layers_comb = hp.choice( 'layers_comb',[1,2,3,4]) # 1,2,
        drop_rate = hp.choice( 'drop_rate',[0.3,0.4,0.5,0.6,0.7]) 
        batchNorm = hp.choice( 'batchNorm',[True, False] )
        activation_out =  hp.choice( 'activation_out',['softmax', 'sigmoid'] )
        learning_ratio = hp.choice( 'learning_ratio',[0.001,0.0001,0.00001] )
        epochs = hp.choice( 'epochs',[10,20,40,60] )# 20,40
        batch_size= hp.choice( 'batch_size',[16,32,64,128] )# 4,16,32,64 #

        param_grid = dict(neuron_comb=neuron_comb, layers_comb=layers_comb,
                drop_rate=drop_rate, batchNorm =batchNorm,activation_out=activation_out,
                learning_ratio=learning_ratio, # compile
                epochs=epochs, batch_size=batch_size ) # fit           

        if model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
 
    ### Functions ### 
    ######################################
    print("Model_name full")
    def create_model_full(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        print('TEST__ param_grid:', param_grid)

        # Model
        neuron_comb =  param_grid['neuron_comb']
        layers_comb = param_grid['layers_comb']
        drop_rate =  param_grid['drop_rate'] 
        batchNorm = param_grid['batchNorm'] 
        activation_out = param_grid['activation_out']
        # compile
        learning_ratio = param_grid['learning_ratio']
        # fit
        epochs = param_grid['epochs']
        batch_size= param_grid['batch_size']
    
        ## model__fn
        inputA = Input(shape=(demo_shape,))
        if final_model:
            print('\nFinal_model inputA.shape',inputA.shape)
 
        attention_probs_A = Dense(demo_shape, activation='softmax', name='attention_vec_A')(inputA) 
        inputAA = tf.keras.layers.Multiply(name='attention_mul_A')([inputA, attention_probs_A] ) 
        
        fully = Dense(neuron_comb, activation="relu",name="Fully-1")(inputAA)
        fully = Dropout(drop_rate,name="Drop-1")(fully)
        if batchNorm:
            fully = BatchNormalization(axis=1,name=("BatchNorm-1"))(fully)

        for l in range(2,layers_comb+1,1):
                fully = Dense(neuron_comb, activation="relu",name="Fully-"+str(l))(fully)
                fully = Dropout(drop_rate,name="Drop-"+str(l))(fully)
                if batchNorm:
                    fully = BatchNormalization(axis=1,name=("BatchNorm-"+str(l)))(fully)

        out= []
        if model_type != 'regression':
            out = Dense(num_classes, activation=activation_out, name="Out-sigmoid")(fully)  
            # sigmoid softmax tanh relu PReLU Leaky ReLU 
        else:
            out = Dense(num_classes,activation=None,name="Out-noActivation")(fully) 
            # linear' bias_initializer="zeros",
            # kernel_regularizer=None,
            # bias_regularizer=None,
            # activity_regularizer=None,
        model = Model(inputs=inputA, outputs=out)

        if final_model:
            model.summary()

        loss = 'categorical_crossentropy' # 'binary_crossentropy' # hinge # squared_hinge (-1 1 binary)  # categorical_crossentropy  # sparse_categorical_crossentropy # kullback_leibler_divergence (distribution based - auto encoder)
        metrics=[tf.keras.metrics.AUC(name = 'auc')] # 'accuracy', 'binary_accuracy', 'categorical_accuracy' # MeanSquaredLogarithmicError # MeanSquaredError # MeanAbsoluteError # MeanAbsolutePercentageError #'crossentropy' #  'mse'
        if model_type == 'binary':
            loss = 'binary_crossentropy'
        if model_type == 'regression':
            loss =  tf.keras.losses.MeanSquaredError(reduction="auto", name="mse_loss") 
            metrics=tf.keras.metrics.MeanAbsolutePercentageError(name="mape_metric", dtype=None) 
        model.compile(loss = loss,
                    metrics=metrics,
                    optimizer=Adam(lr=learning_ratio) ) # RMSprop(lr=0.001) # 'adam' # SGD(lr=learning_ratio, momentum=momentu, lr=0.01, momentum=0.9) #

        es = EarlyStopping(monitor='val_auc', mode='max', verbose=verbose, patience=patience, min_delta=0)
        mc = ModelCheckpoint('best_model.h5', monitor='val_auc', mode='max', verbose=verbose, save_best_only=True)  
        if model_type == 'regression':
            es = EarlyStopping(monitor='val_mape_metric', mode='min', verbose=verbose, patience=patience, min_delta=0)
            mc = ModelCheckpoint('best_model.h5', monitor='val_mape_metric', mode='min', verbose=verbose, save_best_only=True)

        history = []
        if model_type == 'classification':
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values), 
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc],
                                class_weight=class_weights
                                )
            model = load_model('best_model.h5')
        else:
            history = model.fit(x=x_learn[pts_col].values,  
                                y=y_learn.values,
                                validation_data=(x_val[pts_col].values,  y_val.values),
                                epochs= epochs, 
                                batch_size= batch_size,                            
                                verbose=verbose_model, 
                                callbacks=[es,mc]
                            )
            model = load_model('best_model.h5')

        pred_prob = model.predict( x_val[pts_col].values)
        if np.isnan(pred_prob).any():
            print("ERROR np.nan_to_num(pred_prob)")
            pred_prob = np.nan_to_num(pred_prob)
        predictions = pred_prob.argmax(axis=1)
        if model_type == 'binary':
            fpr, tpr, thresholds = roc_curve(y_val, pred_prob)
            gmeans = np.sqrt(tpr * (1-fpr))
            ix = np.argmax(gmeans)
            print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))
            predictions = (pred_prob >thresholds[ix]).astype(np.int) 
        if model_type == 'regression':
            predictions = model.predict( x_val[pts_col].values)
        y_val_binary = y_val.copy()
        if model_type == 'classification':
            y_val = y_val.values.argmax(axis=1)
      
        ### for ploting send out
        print('history.history.keys()',history.history.keys())
        train_acc=[]
        val_acc=[]
        if model_type == 'classification' or model_type == 'binary':
            train_acc = history.history['auc'] 
            val_acc = history.history['val_auc']
        elif model_type == 'regression':
            train_acc = history.history['mape_metric']
            val_acc = history.history['val_mape_metric']
        else:
            print("Error model type")
            return(0)
        train_loss = history.history['loss']
        val_loss = history.history['val_loss']
        epochs = range(len(train_acc))
    
        if model_type == 'regression' :
            y_val = y_val_binary
            actual, pred = np.array(y_val), np.array(predictions)
            mape_error =  np.mean(np.abs((actual - pred) / actual)) * 100
            mape_acc = 100 - mape_error 
            results = { 'loss': (-mape_acc),# 'status': STATUS_OK,
                        'accuracy_score_val': mape_acc, 
                        'train_acc':train_acc, 'val_acc':val_acc, 
                        'train_loss':train_loss, 'val_loss':val_loss, 'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                    }
            print("TEST__ accuracy_score:mape_acc ", mape_acc)
            print("datetime.now()",datetime.now())
            return (results)

        if model_type == 'binary' or model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
    ######################################
    print("Model_name rf")
    def create_model_rf(param_grid,y_val=y_val,x_val=x_val,y_learn=y_learn,x_learn=x_learn, class_weights=class_weights,num_classes=num_classes,final_model=final_model):
        
        ## model__rf
        # model = RandomForestClassifier(random_state=7)
        model = sk.ensemble.GradientBoostingClassifier(n_estimators=1000)
        # model = sk.neighbors.KNeighborsClassifier()
        # model = sk.svm.SVC()

        model.fit(x_learn[pts_col].values,y_learn.values.argmax(axis=1))
        predictions = model.predict(x_val[pts_col].values)
        pred_prob = model.predict_proba(x_val[pts_col].values)
        y_val_binary = y_val
        y_val = y_val.values.argmax(axis=1)
         
        ### for ploting send out
        train_acc=0
        val_acc=0
        train_loss = 0
        val_loss = 0
        epochs = 0
    
        if model_type =='classification':
            accuracy_score_val =round(accuracy_score(y_val,predictions),2)
            f1_score_val = f1_score(y_val, predictions, average=score_average)
            precision_score_val = precision_score(y_val,predictions, average=score_average)
            recall_score_val = recall_score(y_val, predictions, average=score_average)
            Roc_auc_score_val = roc_auc_score(y_val_binary, pred_prob) 
            roc_curve_val = roc_curve(y_val, pred_prob[:, 1])
            print("TEST__ accuracy_score_val:",accuracy_score_val," f1_score_val:" ,f1_score_val, " precision_score_val ", precision_score_val, " recall_score_val ", recall_score_val, " Roc_auc_score_val ",Roc_auc_score_val)
        
            confusion_= confusion_matrix(y_val_binary.values.argmax(axis=1),predictions)
            if model_type == 'binary':
                confusion_= confusion_matrix(y_val_binary.values,predictions)
            for class_ in range(len(confusion_)):
                class_accuracy = confusion_[class_][class_]/float(sum(confusion_[class_]))
                class_str ="class_"+str(class_)+"_accuracy"
                print('TEST__ confusion:',class_str,class_accuracy)
 

            results = { 'loss': (-Roc_auc_score_val), #'status': STATUS_OK,
                        'accuracy_score_val': accuracy_score_val, 'f1_score_val': f1_score_val, 
                        'precision_score_val': precision_score_val, 'recall_score_val': recall_score_val,
                        'Roc_auc_score_val' : Roc_auc_score_val,'roc_curve_val':roc_curve_val,
                        'val_acc':val_acc, 'val_loss':val_loss,
                        'train_acc':train_acc, 'train_loss':train_loss,
                        'epochs':epochs,
                        'y_val_binary':y_val_binary,'y_val':y_val,
                        'pred_prob':pred_prob, 'predictions':predictions,
                        'model': model
                        }
            print("datetime.now()",datetime.now())
            return (results)
        else:
            print("\nwrong model type")
            return(0)
    ######################################
    print("Results #############################")
    ### trials 
    # tpe_trials = Trials()
    tpe_best = []
    if optimize:
        if model_name == 'full':
            tpe_best = fmin(fn=create_model_full, space=param_grid, algo=tpe.suggest,  max_evals=max_eval, trials=tpe_trials, rstate= np.random.RandomState(14))
        elif model_name == 'rf':
            print("*** Turn optimize off for rf")
            return()
        else:
            print("Error optimize: model not found")
            return(0)
        param_grid= space_eval(param_grid, tpe_best)
        print("Best: ", getXfromBestModelfromTrials(tpe_trials, 'loss'), param_grid)
        print("datetime.now(): ", datetime.now())
    else: # __param
        if model_name == 'full':
            param_grid = {'activation_out': 'softmax', 'batchNorm': False, 'batch_size': 64, 'drop_rate': 0.5, 'epochs': 60, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 400}
            if max_level == 3:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'} #80 
            elif max_level == 5:
                param_grid = {'batch_size': 128, 'drop_rate': 0.5, 'epochs': 10, 'layers_comb': 2, 'learning_ratio': 0.001, 'neuron_comb': 300, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 5:
                param_grid = {'batch_size': 32, 'drop_rate': 0.5, 'epochs': 80, 'layers_comb': 3, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}
            elif max_level == 9:
                param_grid = {'batch_size': 128, 'drop_rate': 0.4, 'epochs': 80, 'layers_comb': 4, 'learning_ratio': 0.001, 'neuron_comb': 100, 'batchNorm':False, 'activation_out':'sigmoid'}

            if model_type == 'binary':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
            if model_type == 'regression':
                param_grid = {'batch_size': 128, 'drop_rate': 0.3, 'epochs': 60, 'layers_comb': 2, 'learning_ratio': 0.0001, 'neuron_comb': 600, 'batchNorm':False, 'activation_out':'sigmoid'}
        elif model_name == 'rf':
            print("rf not optimize")
        else:
            print("Error optimize: model not found")
            return(0)

    print("\nFinal Model")
    final_results = []
    final_model =  True
    # __call
    if model_name == 'full':
        final_results = create_model_full(param_grid,y_test,x_test,y_train,x_train)
    if model_name == 'rf':
        final_results = create_model_rf(param_grid,y_test,x_test,y_train,x_train)

    # model = Model(*model stages)
    # model.save(out_model)
    
    ## __plot
    if True: 
        # fig = plt.figure(figsize=(15,15)) 
        # ax0 = plt.subplot2grid((3,1),(0,0),rowspan = 1,colspan = 1)
        # ax1 = plt.subplot2grid((3,1),(1,0), rowspan = 1,colspan = 1)
        # ax2 = plt.subplot2grid((3,1),(2,0), rowspan = 1,colspan = 1)

        # ax0.plot(final_results['epochs'],final_results['train_acc'], color='red', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['acc_v'], legend='brief', label="accuracy")
        # ax0.plot(final_results['epochs'],final_results['val_acc'], color='green', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_acc'], legend='brief', label="val_accuracy")
        # ax0.plot(final_results['epochs'],final_results['train_loss'], color='black', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['loss_v'], legend='brief', label="loss")
        # ax0.plot(final_results['epochs'],final_results['val_loss'], color='blue', marker='o', linestyle='dashed') # sns.lineplot(ax=ax0,x=final_results['epochs'],y=final_results['val_loss'], legend='brief', label="val_loss")
        # ax0.set_title('Learning Curve: Training and validation accuracy/loss') #, y=1.05, size=15)
        # ax0.legend(['train_acc','val_acc','train_loss','val_loss'])

        # fpr, tpr, threshold = final_results['roc_curve_val']
        # roc_auc = auc(fpr, tpr)
        # ax1.set_title('Receiver Operating Characteristic')
        # ax1.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)
        # ax1.legend(loc = 'lower right')
        # ax1.plot([0, 1], [0, 1],'r--')
        # ax1.set_xlim([0, 1])
        # ax1.set_ylim([0, 1])
        # ax1.set_ylabel('True Positive Rate')
        # ax1.set_xlabel('False Positive Rate')
       
        print('\nInterpret model')
        ## __shap __importance __inter
        if model_importance: 

            x_overal = original_x_train[pts_col].append(original_x_test[pts_col], ignore_index = True) # no train oversampling
            y_overal = original_y_train[outcome].append(original_y_test[outcome], ignore_index = True) # no onehot encoded # 1 discharge
            xy_overal = pd.concat([ x_overal , y_overal  ], axis=1) # [['2_code_1.0']]
            print('y_overal',y_overal.sum(axis=0))
            print('xy_overal.shape: ',xy_overal.shape)
            print("\nMissigness")
            for col in xy_overal.columns:
                missigness = xy_overal[col].isna().astype(int).sum() 
                if missigness/float(xy_overal.shape[0])!=0:
                    print(col, ": " , missigness/float(xy_overal.shape[0]), missigness,xy_overal.shape[0])
            print("Before Dropna pts:",xy_overal.shape[0])
            xy_overal.dropna(inplace=True) # 
            print("After Dropna pts:",xy_overal.shape[0],"\n")
            # return(xy_overal)
      
            # propensity = LogisticRegression()
            # propensity = propensity.fit(xy_overal[pts_col_no_treat], xy_overal[['2_code']])
            # pscore = propensity.predict_proba(xy_overal[pts_col_no_treat])[:,1] # The predicted propensities by the model
            # print ('pscore[:5]',pscore[:5])
            # xy_overal['Propensity'] = pscore

            # print("\nMatchFrame")
            # g_discharge, g_death = xy_overal[xy_overal['2_code']==1],xy_overal[xy_overal['2_code']==0]
            # matched_frame = pd.DataFrame()
            # counter =0
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # for index, row in g_death.iterrows():
            #     g_discharge['prop_diff'] =  abs(g_discharge['Propensity'] - row['Propensity'])
            #     closest = g_discharge[g_discharge['prop_diff']==g_discharge['prop_diff'].min()]
            #     matched_frame = matched_frame.append(row, ignore_index = True)
            #     matched_frame = matched_frame.append(closest, ignore_index = True)
            #     g_discharge = g_discharge[g_discharge['prop_diff']!=g_discharge['prop_diff'].min()]
            #     # if counter ==1000:
            #     #     print(matched_frame[['Propensity','prop_diff']].head(10))
            #     #     break
            #     # counter +=1
            # print('g_discharge, g_death ',g_discharge.shape, g_death.shape )
            # print('matched_frame.shape',matched_frame.shape)
            # print(matched_frame.head(4))

            # x_overal = original_x_train[pts_col]
            # y_overal = original_y_train[outcome]
            # xy_overal = pd.concat([ x_overal , y_overal  ],
            print('\nSMOTE')
            sm = RandomOverSampler(random_state = 7)
            print("before sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0)) 
            x_overal, y_overal = sm.fit_resample(x_overal[pts_col], y_overal[outcome])
            print("after sm.fit, shape test: ",x_overal.shape,y_overal.shape,y_overal.sum(axis=0))
            if model_type == 'classification' :
                y_overal = pd.concat([ y_overal , pd.get_dummies(y_overal[outcome[0]], drop_first=False,prefix=str(outcome[0])) ], axis=1).drop(outcome[0], axis=1)
                print('y_overal.sum(axis=0)',y_overal.sum(axis=0))

            if model_name == 'full':
                final_results = create_model_full(param_grid,y_overal,x_overal,y_overal,x_overal)
            if model_name == 'rf':
                final_results = create_model_rf(param_grid,y_overal,x_overal,y_overal,x_overal)
            model = final_results['model']

            print("\nKmean start datetime.now(): ", datetime.now())
            kmeans_1 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_1.0']==1,pts_col])
            kmeans_0 = sk.cluster.KMeans(n_clusters=50, random_state=0).fit(x_overal.loc[y_overal['2_code_0.0']==1,pts_col])
            print('kmeans_1.shape',kmeans_1.cluster_centers_.shape)
            print('kmeans_0.shape',kmeans_0.cluster_centers_.shape)
            centroids= np.append(kmeans_1.cluster_centers_, kmeans_0.cluster_centers_, axis=0)
            print('centroids',centroids.shape)
            
            training = centroids           
            testing = shap_testing_for_explainer # matched_frame[pts_col] 
            
            print("\nShap Start datetime.now(): ", datetime.now())
            explainer = []
            if model_name == 'rf':
                explainer = shap.KernelExplainer(model.predict_proba, training)
            else:
                explainer = shap.KernelExplainer(model.predict, training)
            shap_values = all_treat_new # explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            shap_values.columns = shap_values.columns.str.replace('dementia', 'Dementia').str.replace('gender_female', 'Female').str.replace('_', ' ').str.strip().str.replace(' ',' & ').str.replace('os & day1 & ','Diagnosis OS ').str.replace('bmi','BMI').str.replace('mets','MetS').str.replace('age','Age').str.replace('renal','Renal').str.replace('Q',' Q')
            ## training = centroids   
            # explainer = shap.KernelExplainer(model.predict_proba, training)
            # explainer = shap.Explainer(model)
            
            ## testing = shap_testing_for_explainer
            ## shap_values = all_treat_new
            # shap_values = explainer.shap_values(testing.sample(n=int(testing.shape[0]//shap_precentage_test), random_state=1).values)
            # shap_values = explainer(X)

            # explainer = shap.Explainer(model)
            # shap_values = explainer(X)

              # In v0.20 force_plot now requires the base value as the first parameter
            # https://github.com/slundberg/shap
            # return(pd.DataFrame({'A' : [np.nan]}))
            # shap.plots.waterfall(all_treat_new.values[0])
            fig = plt.figure(figsize=(10,10))
            print("Plot Shap force")
            # https://towardsdatascience.com/tutorial-on-displaying-shap-force-plots-in-python-html-4883aeb0ee7c
            patient_num_Shap = 3660
            shap.plots.force(explainer.expected_value[0],all_treat_new.values[ patient_num_Shap],matplotlib=True,feature_names=all_treat_new.columns,plot_cmap=['#77dd77', '#f99191'])
            plt.show()
            return(pd.DataFrame({'A' : [np.nan]}))

            shap.plots.waterfall(all_treat_new.values[0])

            # explainer = shap.KernelExplainer(svm.predict_proba, X_train, link="logit")
            # shap_values = explainer.shap_values(X_test, nsamples=100)
            # shap.force_plot(explainer.expected_value[0], shap_values[0][0,:], X_test.iloc[0,:], link="logit")
            shap.plots.force(all_treat_new.values[0])
            plt.show()
            
            return(pd.DataFrame())
   

    

